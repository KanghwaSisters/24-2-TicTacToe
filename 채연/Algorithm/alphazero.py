# -*- coding: utf-8 -*-
"""AlphaZero_최종.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKee3uUfP-5Xtz95h5moUa1oyJbUTg1O

https://github.com/Jpub/AlphaZero/blob/master/6_7_tictactoe/evaluate_network.py

# State
"""

import numpy as np

class State:
    def __init__(self, board_size=3, pieces=None, enemy_pieces=None):
        self.board_size = board_size
        self.pieces = np.zeros(board_size * board_size, dtype=int) if pieces is None else np.array(pieces)
        self.enemy_pieces = np.zeros(board_size * board_size, dtype=int) if enemy_pieces is None else np.array(enemy_pieces)

    def piece_count(self, pieces):
        return np.sum(pieces)

    def is_lose(self):
        board = self.enemy_pieces.reshape(self.board_size, self.board_size)

        return any(
            np.all(line == 1) for line in
            np.vstack([
                board,  # 행
                board.T,  # 열
                np.diag(board),  # 대각선
                np.diag(np.fliplr(board))  # 반대 대각선
            ])
        )

    def is_draw(self):
        return self.piece_count(self.pieces) + self.piece_count(self.enemy_pieces) == self.board_size * self.board_size

    def is_done(self):
        # 종료 조건: 상대방이 승리 or 무승부
        return self.is_lose() or self.is_draw()

    def next(self, action):
        # 현재 상태에서 주어진 action(칸)에 말을 놓은 후 다음 상태 반환
        pieces = self.pieces.copy()
        pieces[action] = 1
        return State(self.board_size, self.enemy_pieces, pieces)

    def legal_actions(self):
        # 가능한 행동(빈 칸의 위치)을 반환
        return np.where((self.pieces + self.enemy_pieces) == 0)[0]

    def to_feature(self):
        # 상태를 신경망 입력 형태로 변환 (2, board_size, board_size)
        return np.stack([
            self.pieces.reshape(self.board_size, self.board_size),
            self.enemy_pieces.reshape(self.board_size, self.board_size)
        ], axis=0)

    def __str__(self):
        ox = ('o', 'x') if np.sum(self.pieces) == np.sum(self.enemy_pieces) else ('x', 'o')
        board = np.full(self.board_size * self.board_size, '-')
        board[self.pieces == 1] = ox[0]
        board[self.enemy_pieces == 1] = ox[1]
        return '\n'.join([
            ''.join(board[i:i + self.board_size]) for i in range(0, self.board_size * self.board_size, self.board_size)
        ])

    def get_total_state(self):
    # """ 현재 보드 상태를 반환 (플레이어 + AI 말 포함)
        board = np.zeros(self.board_size * self.board_size, dtype=int)
        board[self.pieces == 1] = 1   # 플레이어 돌
        board[self.enemy_pieces == 1] = -1  # AI 돌 (반대 방향)
        return board.reshape(self.board_size, self.board_size)

"""# Dual Network"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    def __init__(self, n_filters=128):
        super().__init__()
        self.conv1 = nn.Conv2d(n_filters, n_filters, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(n_filters)
        self.conv2 = nn.Conv2d(n_filters, n_filters, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(n_filters)

    def forward(self, x):
        shortcut = x # 잔차
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        return F.relu(x + shortcut)

class DualNetwork(nn.Module):
    def __init__(self, board_size, input_channels=2, n_filters=128, n_res_blocks=16):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, n_filters, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(n_filters)
        self.res_blocks = nn.Sequential(
            *[ResidualBlock(n_filters) for _ in range(n_res_blocks)]
        )
        self.global_pool = nn.AdaptiveAvgPool2d(1)

        # policy
        self.policy_head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(n_filters, board_size * board_size),
        )
        # value
        self.value_head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(n_filters, 1),
            nn.Tanh()
        )

    def forward(self, x):
        # input x : (배치 크기, 채널, 보드 크기, 보드 크기)
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.res_blocks(x)
        x = self.global_pool(x)
        policy = self.policy_head(x)
        value = self.value_head(x)
        return policy, value

"""# MCTS"""

C_PCUT = 1.0 # 탐험과 탐욕적 행동 선택 균형
PV_EVALUATE_COUNT = 100

from math import sqrt
import numpy as np
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 추론
def predict(model, state, board_size):
    # 입력 데이터 정규화
    x = np.array([state.pieces, state.enemy_pieces], dtype=np.float32) / 1.0 # np.array로 해놓고 torch.float32으로 함..
    x = x.reshape(2, board_size, board_size).transpose(0, 1, 2).reshape(1, 2, board_size, board_size)
    x = torch.tensor(x, dtype=torch.float32).to(device)

    with torch.no_grad():
        policy, value = model(x) # 신경망으로 정책 및 가치 예측
        policy = policy.cpu().numpy().flatten()
        value = value.cpu().numpy().flatten()

    legal_actions = state.legal_actions()
    policies = policy[legal_actions]
    policy_sum = policies.sum()
    if policy_sum > 0:
        policies /= policy_sum
    else:
        policies = np.ones_like(policies) / len(policies)
    return policies, value[0]

class MCTSNode:
    def __init__(self, state, p, c_puct=C_PCUT):
        self.state = state
        self.p = p # 신경망에서 예측한 정책 확률
        self.w = 0 # 노드 누적 보상
        self.n = 0 # 노드 방문 횟수
        self.c_puct = c_puct
        self.child_nodes = None

    def evaluate(self, model):
        # 1
        if self.state.is_done(): # 종료 상태인지 확인
            value = -1 if self.state.is_lose() else 0 # 패배: -1, 무승부: 0
            # 보상 누계와 시행 횟수 갱신
            self.w += value
            self.n += 1
            return value
        # 2
        # 자식 노드가 존재하지 않는 경우
        if not self.child_nodes:
            # 신경망으로 정책 및 가치 예측
            policies, value = predict(model, self.state, self.state.board_size)
            # 보상 누계와 시행 횟수 갱신
            self.w += value
            self.n += 1
            # 자식 노드 전개(확장)
            self.child_nodes = []
            for action, policy in zip(self.state.legal_actions(), policies):
              self.child_nodes.append(MCTSNode(self.state.next(action), policy, c_puct = self.c_puct))
            return value
        # 3
        else:
            # UCB1이 가장 큰 자식 노드를 평가해 가치 얻기
            # 탐색-활용(Exploration-Exploitation) 균형 전략 중 하나
            value = -self.next_child_node().evaluate(model)
            # 보상 누계와 시행 횟수 갱신
            self.w += value
            self.n += 1
            return value

    # Upper Confidence Bound1
    def next_child_node(self):
        # 공식
        t = sum(child.n for child in self.child_nodes)
        pucb_values = []
        for child_node in self.child_nodes:
          pucb_values.append((-child_node.w/child_node.n if child_node.n else 0.0)+
                             self.c_puct * child_node.p * sqrt(t) / (1 + child_node.n))
        return self.child_nodes[np.argmax(pucb_values)] # 최대값 출력

# 노드 리스트를 시행 횟수 리스트로 변환
def nodes_to_scores(nodes):
    return [node.n for node in nodes]

# 볼츠만 분포 계산
def boltzman(xs, temperature):
    xs = [x ** (1 / temperature) for x in xs]
    return [x / sum(xs) for x in xs]

# 몬테카를로 트리 탐색
def pv_mcts_scores(model, state, temperature):
    # 현재 상태의 루트 노드 생성
    root_node = MCTSNode(state, 0)

    # 시뮬레이션 반복 실행
    for _ in range(PV_EVALUATE_COUNT):
        root_node.evaluate(model)

    # 자식 노드 방문 횟수를 점수로 변환
    scores = nodes_to_scores(root_node.child_nodes)

    if temperature == 0: # 최대값인 경우에만 1
        action = np.argmax(scores)
        policy = np.zeros(len(scores))
        policy[action] = 1
    else:
        policy = boltzman(scores, temperature) # 볼츠만 분포를 기반으로 분산 추가
    return policy


# 몬테카를로 트리 탐색을 활용한 행동 선택
def pv_mcts_action(model, temperature=0):
    def action_selector(state):
        scores = pv_mcts_scores(model, state, temperature) # 각 행동에 대한(상태에 기반) 확률분포 계산
        return np.random.choice(state.legal_actions(), p=scores) # 하나의 행동을 확률적으로 선택
    return action_selector

def mcts_simulation(root_node, model, simulations=100):
    for _ in range(simulations):
        root_node.evaluate(model)
    # 방문 횟수 기준 정책 계산
    child_visits = [child.n for child in root_node.child_nodes]
    policy = np.array(child_visits) / sum(child_visits)
    return root_node, policy # 현재 게임 상태, 정책

"""# self play"""

def self_play_game(model, board_size=3, simulations=100, temperature=0):
    state = State(board_size=board_size)
    game_data = []

    while not state.is_done():
        # 현재 상태에서 MCTS 기반 점수 계산
        scores = pv_mcts_scores(model, state, temperature)

        # 점수를 고정 크기 정책으로 변환
        ''' 이유
         정책 벡터가 보드의 크기와 맞춰지고, 유효한 행동만 확률로 유지'''

        full_policy = np.zeros(board_size * board_size, dtype=np.float32)
        legal_actions = state.legal_actions()  # 유효한 행동 리스트
        full_policy[legal_actions] = scores
        game_data.append((state.to_feature(), full_policy))

        # 확률적으로 행동 선택
        action = np.random.choice(state.legal_actions(), p=scores)
        state = state.next(action)

    # 게임 종료 후 보상 계산
    reward = 1 if not state.is_lose() else -1
    game_data = [(s, p, reward) for s, p in game_data]
    return game_data

"""# Train Model"""

def train_model(model, scheduler, optimizer, game_data, epochs=1, batch_size=64):
    model.train()
    loss_fn_policy = torch.nn.CrossEntropyLoss()
    loss_fn_value = torch.nn.MSELoss()

    # 데이터를 PyTorch 텐서로 변환
    states, policies, values = zip(*game_data)

    states = torch.tensor(states, dtype=torch.float32).to(device)
    policies = torch.tensor(policies, dtype=torch.float32).to(device)
    values = torch.tensor(values, dtype=torch.float32).unsqueeze(1).to(device)

    dataset = torch.utils.data.TensorDataset(states, policies, values)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    for epoch in range(epochs):
        total_loss, total_policy_loss, total_value_loss = 0, 0, 0

        for inputs, targets_policy, targets_value in data_loader:
            inputs = inputs.to(device)
            targets_policy = targets_policy.to(device)
            targets_value = targets_value.to(device)

            # 모델예측
            predicted_policies, predicted_values = model(inputs)

            loss_policy = loss_fn_policy(predicted_policies, targets_policy)
            loss_value = loss_fn_value(predicted_values, targets_value)
            loss = 0.5*loss_policy + loss_value # policy가 너무 크게 나와서 앞에 0.5 곱해줌 이래도 되나..

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_policy_loss += loss_policy.item()
            total_value_loss += loss_value.item()

        # 에포크별 평균 기록
        training_metrics["loss"].append(total_loss / len(data_loader))
        training_metrics["policy_loss"].append(total_policy_loss / len(data_loader))
        training_metrics["value_loss"].append(total_value_loss / len(data_loader))

        scheduler.step() # scheduler는 마지막에

"""# Evaluate model"""

def evaluate_models(new_model, old_model, games=20, board_size=3):
    new_wins, old_wins = 0, 0
    temperature = 1.0

    for _ in range(games):
        state = State(board_size=board_size)
        current_player = new_model

        while not state.is_done():
            scores = pv_mcts_scores(current_player, state, temperature)
            action = np.random.choice(state.legal_actions(), p=scores)
            state = state.next(action)

            # 플레이어 교체
            current_player = old_model if current_player == new_model else new_model

        if state.is_lose():
            if current_player == new_model:
                old_wins += 1
            else:
                new_wins += 1

    # 승리 결과 기록
    evaluation_metrics["new_model_wins"].append(new_wins)
    evaluation_metrics["old_model_wins"].append(old_wins)
    return new_wins, old_wins

"""# Final"""

import matplotlib.pyplot as plt

def plot_graph():
    # 그래프
    plt.figure(figsize=(15, 5))

    # 1. 정책 정확도 그래프
    plt.subplot(1, 3, 1)
    plt.plot(training_metrics["policy_loss"], label="Policy Loss", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Policy Loss Over Epochs")
    plt.legend()

    # 2. 손실 그래프 (전체 손실과 가치 손실)
    plt.subplot(1, 3, 2)
    plt.plot(training_metrics["loss"], label="Total Loss", marker="o")
    plt.plot(training_metrics["value_loss"], label="Value Loss", marker="x")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Loss Over Epochs")
    plt.legend()

    # 3. 승률 그래프 (새 모델 vs 기존 모델 승리 횟수)
    plt.subplot(1, 3, 3)
    plt.plot(evaluation_metrics["new_model_wins"], label="New Model Wins", marker="o")
    plt.plot(evaluation_metrics["old_model_wins"], label="Old Model Wins", marker="x")
    plt.xlabel("Iterations")
    plt.ylabel("Wins")
    plt.title("Model Evaluation Results")
    plt.legend()

    plt.tight_layout()
    plt.show()

BOARD_SIZE = 3
LEARNING_RATE = 2e-4
MOMENTUM = 0.9
WEIGHT_DECAY = 1e-4
SIMULATIONS = 120
GAMES_PER_ITERATION = 8
EVALUATION_GAMES = 140
ITERATIONS = 3

import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

model = DualNetwork(BOARD_SIZE)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = StepLR(optimizer, step_size=50, gamma=0.5)

old_model =  DualNetwork(BOARD_SIZE).to(device)
old_model.load_state_dict(model.state_dict())

training_metrics = {
    "loss": [],
    "policy_loss": [],
    "value_loss": []
}

evaluation_metrics = {
    "new_model_wins": [],
    "old_model_wins": []
}

for iteration in range(ITERATIONS):
    print(f"Iteration {iteration + 1}")
    game_data = []
    for _ in range(GAMES_PER_ITERATION):
        game_data = self_play_game(model, BOARD_SIZE, simulations=SIMULATIONS)
    train_model(model, scheduler, optimizer, game_data, epochs=10)

    new_wins, old_wins = evaluate_models(model, old_model, games=EVALUATION_GAMES)
    last_loss = training_metrics['loss'][-1] # 어떤 행동을 취해야 할지(정책)와 그 상태가 얼마나 좋은지(가치)를 학습
    last_value_loss = training_metrics['value_loss'][-1] # 정확한 승리 확률이나 기대 보상을 예측하도록 학습
    print(f"New Model Wins: {new_wins}, Old Model Wins: {old_wins}, Loss: {last_loss}, Value Loss: {last_value_loss}")

    # 승률 계산
    total_games = new_wins + old_wins
    win_rate = (new_wins / total_games * 100) if total_games > 0 else 0

    print(f"New Model Win Rate: {win_rate:.2f}%")

plot_graph()

from google.colab import drive
drive.mount('/content/drive')

torch.save(model.state_dict(), "/content/drive/MyDrive/kanghwa/trained_tic_tac_toe_model.pth")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = DualNetwork(BOARD_SIZE)
model.load_state_dict(torch.load("/content/drive/MyDrive/kanghwa/trained_tic_tac_toe_model.pth"))
model.to(device)  # 모델을 GPU로 이동
model.eval()

import numpy as np
import torch

def play_against_ai(model, board_size=3):
    state = State(board_size=board_size)

    # 선공 결정
    first_player = input("누가 먼저 시작할까요? (1: 플레이어, 2: AI): ")
    while first_player not in ['1', '2']:
        first_player = input("잘못된 입력입니다. 1 또는 2를 입력하세요: ")
    player_turn = first_player == '1'

    print("게임 시작! 당신은 'O', AI는 'X'입니다.")

    while not state.is_done():
        print(state)

        if player_turn:
            # 플레이어 차례
            action = None
            while action not in state.legal_actions():
                try:
                    action = int(input("당신의 차례입니다. 1~9 중 빈 칸을 선택하세요: ")) - 1
                except ValueError:
                    continue
            state = state.next(action)
        else:
            # AI 차례
            print("\nAI가 수를 두는 중...")
            ai_action = pv_mcts_action(model, temperature=0)(state)
            state = state.next(ai_action)

        if state.is_done():
            break

        player_turn = not player_turn  # 턴 변경

    print(state)

    if not player_turn and state.is_lose():
        print("AI가 승리했습니다!")
    elif player_turn and state.is_lose():
        print("축하합니다! 당신이 승리했습니다!")
    elif state.is_draw():
        print("무승부입니다!")

play_against_ai(model)

"""# 히트맵ver."""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_board_and_heatmap(state, policy_probs):
    """현재 보드 상태와 AI의 MCTS 히트맵을 출력"""
    board_size = state.board_size
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # 왼쪽: 보드 상태
    axes[0].set_facecolor('skyblue')
    for x in range(board_size):
        axes[0].plot([x, x], [0, board_size-1], color="black", linewidth=1, zorder=1)
        axes[0].plot([0, board_size-1], [x, x], color="black", linewidth=1, zorder=1)

    # 보드 상태 가져오기
    try:
        board = state.get_total_state()  # 기존 코드
    except AttributeError:
        board = state.pieces.reshape(board_size, board_size) - state.enemy_pieces.reshape(board_size, board_size)

    for i in range(board_size):
        for j in range(board_size):
            if board[i, j] == 1:
                circle = plt.Circle((j, board_size-1 - i), 0.4, color="black", zorder=2)
                axes[0].add_artist(circle)
            elif board[i, j] == -1:
                circle = plt.Circle((j, board_size-1 - i), 0.4, facecolor='white', edgecolor="black", linewidth=1.5, zorder=2)
                axes[0].add_artist(circle)

    axes[0].set_xlim(-0.5, board_size - 0.5)
    axes[0].set_ylim(-0.5, board_size - 0.5)
    axes[0].set_aspect('equal')
    axes[0].set_title("Current Board State")

    # 오른쪽: AI의 MCTS 정책 히트맵
    full_policy = np.zeros(board_size * board_size)  # 9칸짜리 배열
    legal_actions = state.legal_actions()  # 현재 가능한 행동들

    # 해결: policy_probs의 크기를 legal_actions 크기에 맞춤
    if len(policy_probs) > len(legal_actions):
        policy_probs = policy_probs[:len(legal_actions)]

    if len(policy_probs) != len(legal_actions):
        raise ValueError(f"크기 불일치 이슈 legal_actions: {len(legal_actions)}, policy_probs: {len(policy_probs)}")

    full_policy[legal_actions] = policy_probs  # 빈 칸에만 확률 넣기
    full_policy = full_policy.reshape(board_size, board_size)  # 3x3 변환

    sns.heatmap(full_policy, annot=True, fmt='.2f', cmap='YlGnBu', cbar=True, ax=axes[1])
    axes[1].set_title("MCTS Policy")

    # AI 예상 승률 계산
    ai_win_prob = np.max(policy_probs)  # 가장 높은 확률의 액션이 AI의 예상 승률
    player_win_prob = 1 - ai_win_prob   # 상대방(플레이어) 승률

    # 승률 출력
    print(f"AI 예상 승률: {ai_win_prob * 100:.2f}%")
    print(f"플레이어 예상 승률: {player_win_prob * 100:.2f}%")

    plt.show(block=False)
    plt.pause(1)
    plt.close()

import numpy as np
import torch
import seaborn as sns
import matplotlib.pyplot as plt

def play_against_ai(model, board_size=3):
    state = State(board_size=board_size)
    game_data = []  # 게임 진행 데이터 저장 리스트

    # 선공 결정
    first_player = input("누가 먼저 시작할까요? (1: 플레이어, 2: AI): ")
    while first_player not in ['1', '2']:
        first_player = input("잘못된 입력입니다. 1 또는 2를 입력하세요: ")
    player_turn = first_player == '1'

    print("게임 시작! 당신은 'O', AI는 'X'입니다.")

    while not state.is_done():
        print(state)

        if player_turn:
            action = None
            while action not in state.legal_actions():
                try:
                    action = int(input("당신의 차례입니다. 1~9 중 빈 칸을 선택하세요: ")) - 1
                except ValueError:
                    continue
            state = state.next(action)
        else:
            print("\nAI가 수를 두는 중...")

            # MCTS 실행하여 확률 계산
            root_node = MCTSNode(state, 1.0)  # 초기 루트 노드 설정
            root_node, policy_probs = mcts_simulation(root_node, model, simulations=100)  # MCTS 실행

            legal_actions = state.legal_actions()  # 현재 가능한 행동 가져오기
            ai_action = legal_actions[np.argmax(policy_probs)]

            game_data.append((state, policy_probs))  # 현재 상태와 AI의 정책 저장
            state = state.next(ai_action)

            # 시각화 실행 (보드 상태 + 히트맵)
            visualize_board_and_heatmap(state, policy_probs)

        if state.is_done():
            break

        player_turn = not player_turn

    print(state)

    if not player_turn and state.is_lose():
        print("AI가 승리했습니다!")
    elif player_turn and state.is_lose():
        print("축하합니다! 당신이 승리했습니다!")
    elif state.is_draw():
        print("무승부입니다!")

play_against_ai(model)