{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQtE2fb1lM68"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPshCCOo5clB",
        "outputId": "f6d3ce1a-6dda-4791-ff83-bb0b25c43555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOtuPNV2lPD7"
      },
      "outputs": [],
      "source": [
        "# useful library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANAqaZdpkDRe"
      },
      "source": [
        "# 00 Game Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6JDyoNwj_4l"
      },
      "outputs": [],
      "source": [
        "STATE_SIZE = (3,3)\n",
        "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
        "STATE_DIM = 3 # first player 정보 넣음\n",
        "BOARD_SHAPE = (STATE_DIM, 3, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPMnsPXdRcT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMmNpZtNkSZc"
      },
      "source": [
        "# 01 HYPER PARAMS\n",
        "#### PV_EVALUATE_COUNT: MCTS 성능을 향상\n",
        "#### EVAL_COUNT: 평가의 신뢰도 향상"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjzuKNOXkVuR"
      },
      "outputs": [],
      "source": [
        "# 신경망 구조 관련\n",
        "N_KERNEL = 64 # 합성곱 레이어(ConvLayer)의 필터 수 / 클수록 더 많은 패턴을 학습\n",
        "N_RESIDUAL_BLOCK = 16 # Residual Block 개수 / 클수록 더 깊은 네트워크를 만들 수 있음\n",
        "\n",
        "# 학습 관련\n",
        "BATCH_SIZE = 128 # 한 번의 학습에서 처리하는 데이터 샘플 개수\n",
        "EPOCHS = 50 # 전체 학습 반복 횟수 -> 늘려보자!\n",
        "LEARNING_RATE = 0.001 # 학습률 (뉴런 가중치 업데이트 속도)\n",
        "LEARN_EPOCH = 100 # 학습률 감소 주기\n",
        "\n",
        "# Self-play 관련 -> 한번 selfplay 돌린 후 학습\n",
        "SP_GAME_COUNT = 20  # self-play에서 생성할 게임 수\n",
        "SP_TEMPERATURE = 1.5  # self-play에서 탐색 다양성을 조절하는 온도 파라미터\n",
        "\n",
        "# 평가 관련\n",
        "EVAL_COUNT = 100  # 평가할 때 MCTS 탐색을 몇 번 수행할지 결정\n",
        "PV_EVALUATE_COUNT = 800 # MCTS가 얼마나 깊이 탐색할지를 결정 -> 400으로 수정\n",
        "EVAL_GAME_COUNT = 20  # 평가 1회 당 게임 수(오리지널: 400)\n",
        "EVAL_TEMPERATURE = 1.0  # 온도 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8brv3wrv5jb",
        "outputId": "e5b7aaf9-4fd2-47ed-95dc-ada5cdd5c00d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhcoruIWki_L"
      },
      "source": [
        "# Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5JyKGYYI7aL"
      },
      "outputs": [],
      "source": [
        "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
        "    def __init__(self):\n",
        "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
        "        self.num_actions = self.n ** 2\n",
        "        self.action_space = np.arange(self.num_actions)\n",
        "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
        "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
        "\n",
        "\n",
        "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
        "        \"\"\"\n",
        "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
        "        next_state, is_done, is_lose를 반환한다.\n",
        "        \"\"\"\n",
        "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
        "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
        "        next_state = present_state.next(action_idx)\n",
        "        is_done, is_lose = next_state.check_done()\n",
        "        #next_state에서 게임이 종료되었는지 확인\n",
        "        #현재 플레이어가 패배했는지 확인\n",
        "\n",
        "        return next_state, is_done, is_lose\n",
        "\n",
        "\n",
        "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
        "        \"\"\"\n",
        "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
        "        final_state: 게임이 종료된 state\n",
        "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
        "        \"\"\"\n",
        "        '''\n",
        "        장예원 수정!\n",
        "        '''\n",
        "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
        "\n",
        "        # 승리, 패배, 무승부 정확하게 판별\n",
        "        if not is_done:\n",
        "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
        "\n",
        "        # ✅ 먼저, 무승부인지 확인\n",
        "        if not is_lose and final_state.total_pieces_count() == self.num_actions:\n",
        "            return self.reward_dict['draw']  # 무승부 시 0\n",
        "\n",
        "        # ✅ 승패 보상을 반영하되, 선공/후공이 동일한 기준을 갖도록 설정\n",
        "        reward = self.reward_dict['lose'] if is_lose else self.reward_dict['win']\n",
        "\n",
        "        # ✅ 현재 state의 플레이어가 선공인지 후공인지에 따라 보상 반전\n",
        "        return reward if final_state.check_first_player() else -reward\n",
        "\n",
        "\n",
        "    def render(self, state): #게임 상태 출력\n",
        "        '''\n",
        "        입력받은 state를 문자열로 출력한다.\n",
        "        X: first_player, O: second_player\n",
        "        '''\n",
        "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
        "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
        "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
        "\n",
        "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
        "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
        "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
        "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
        "        #보기 좋게 줄바꿈하여 출력\n",
        "        return formatted_board  #print() 대신 문자열 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0NSd4xbJA_Z"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpUJCSU2I9rQ"
      },
      "outputs": [],
      "source": [
        "class State(Environment):\n",
        "    def __init__(self, state=None, enemy_state=None, move_count=0):\n",
        "        super().__init__()\n",
        "        self.state = np.array(state if state is not None else np.zeros(STATE_SIZE, dtype=int))\n",
        "        self.enemy_state = np.array(enemy_state if enemy_state is not None else np.zeros(STATE_SIZE, dtype=int))\n",
        "        self.move_count = move_count  # ✅ move_count를 인자로 받아 명확하게 설정\n",
        "\n",
        "\n",
        "    def total_pieces_count(self):\n",
        "        '''\n",
        "        이 state의 전체 돌의 개수를 반환한다.\n",
        "        '''\n",
        "        total_state = self.state + self.enemy_state\n",
        "        return np.sum(total_state)\n",
        "\n",
        "\n",
        "    def get_legal_actions(self):\n",
        "        '''\n",
        "        이 state에서 가능한 action을\n",
        "        one-hot encoding 형식의 array로 반환한다.\n",
        "        '''\n",
        "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
        "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
        "        return legal_actions\n",
        "\n",
        "\n",
        "    def check_done(self):\n",
        "        '''\n",
        "        이 state의 done, lose 여부를 반환한다.\n",
        "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
        "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
        "        '''\n",
        "        is_done, is_lose = False, False\n",
        "\n",
        "        # Check lose\n",
        "        lose_condition = np.concatenate([\n",
        "            self.enemy_state.sum(axis=0),\n",
        "            self.enemy_state.sum(axis=1),\n",
        "            [np.trace(self.enemy_state)],  # 왼쪽 위 → 오른쪽 아래 대각선 검사\n",
        "            [np.trace(np.fliplr(self.enemy_state))]  # 오른쪽 위 → 왼쪽 아래 대각선 검사\n",
        "        ])\n",
        "\n",
        "        if np.any(lose_condition == self.n):  # 확실히 3이 있는지 체크\n",
        "            is_done, is_lose = True, True\n",
        "\n",
        "        # Check draw (승리 조건이 없을 경우만 무승부 확인)\n",
        "        if not is_lose and self.total_pieces_count() == self.n ** 2:\n",
        "            is_done, is_lose = True, False\n",
        "\n",
        "        # 디버깅 출력\n",
        "        #print(f\"\\n[DEBUG] check_done() 호출됨\")\n",
        "        #print(f\"[DEBUG] Current Board:\\n{self.state - self.enemy_state}\")  # 현재 보드 상태 출력\n",
        "        #print(f\"[DEBUG] Lose condition: {lose_condition}\")  # lose_condition 값 출력\n",
        "        #print(f\"[DEBUG] is_done: {is_done}, is_lose: {is_lose}\")  # 종료 여부 출력\n",
        "\n",
        "        return is_done, is_lose\n",
        "\n",
        "\n",
        "    def next(self, action_idx):\n",
        "        '''\n",
        "        주어진 action에 따라 다음 state를 생성한다.\n",
        "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
        "        '''\n",
        "        x, y =np.divmod(action_idx, self.n)\n",
        "        state = self.state.copy()\n",
        "        state[x, y] = 1\n",
        "        enemy_state = self.enemy_state.copy()\n",
        "\n",
        "        # ✅ move_count를 정확히 1 증가\n",
        "        new_move_count = self.move_count + 1\n",
        "\n",
        "        next_state = State(enemy_state, state, new_move_count)\n",
        "        return next_state\n",
        "\n",
        "\n",
        "    def check_first_player(self):\n",
        "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
        "        return self.move_count % 2 == 0\n",
        "\n",
        "\n",
        "    def get_random_action(self):\n",
        "        '''\n",
        "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
        "        '''\n",
        "        legal_actions = self.get_legal_actions()\n",
        "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
        "        action = np.random.choice(legal_action_idxs)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return Environment().render(self)  # state를 인자로 명확히 전달"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp7bZbRSlxJD"
      },
      "source": [
        "# 03 Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLSbySd2x6NG"
      },
      "source": [
        "## Dual Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISfWS6galzyW"
      },
      "source": [
        "### ConvLayer (보드 공간 정보를 유지하면서 승리 / 방어 조건을 나타내는 패턴을 자동으로 감지)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVpPy60ylzH8"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, state_dim, n_kernel, kernel_size=3, padding=1, dropout_rate=0.0):\n",
        "        #입력 채널 수\n",
        "        #출력 채널 수\n",
        "        #필터 크기 (3X3)\n",
        "        #패딩: 입력과 출력 크기 동일하게 유지\n",
        "        #바이어스 항 생략 (배치 정규화 사용 시 바이어스 항을 생략하는 것이 일반적)\n",
        "        #드롭아웃 비율\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, n_kernel, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(n_kernel) #배치 정규화 사용할 채널 수\n",
        "        self.relu = nn.ReLU(inplace=True)  # in-place 연산으로 메모리 효율성 증가\n",
        "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x) #입력 데이터에 합성곱 연산을 적용하여 특징맵 생성\n",
        "        x = self.bn(x) #배치 정규화 적용\n",
        "        x = self.relu(x) #비선형 활성화 함수 적용\n",
        "        if self.dropout:  # 드롭아웃 적용 (필요시)\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84tuL-xVow67"
      },
      "source": [
        "### Residual Block\n",
        "##### 1. 배치 정규화 (학습 안정성)와 ReLU 활성화 함수 (비선형성)를 사용 -> 일반 CNN 동일\n",
        "##### 2. 잔차 연결을 추가하여 기존 CNN 구조를 확장 -> 출력값이 아닌 잔차를 학습\n",
        "##### 3. 깊은 신경망에서도 기울기 소실 문제를 완화 -> 일반 CNN 문제점 완화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsVhd2s0oyeb"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, n_kernel, kernel_size=3, padding=1, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(n_kernel, n_kernel, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        #합성곱 연산 수행\n",
        "        #residual block은 입력과 출력 채널이 동일해야 한다는 제약을 따른다~\n",
        "        self.bn1 = nn.BatchNorm2d(n_kernel) #배치 정규화 적용\n",
        "        self.conv2 = nn.Conv2d(n_kernel, n_kernel, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(n_kernel)\n",
        "        self.relu = nn.ReLU(inplace=True)  # 활성화 함수 객체화\n",
        "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x #입력을 잔차로 저장\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x) # double check\n",
        "        if self.dropout:  # 드롭아웃 적용\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x += residual\n",
        "        #모델이 잔차를 학습하도록 하여 기울기 소실 문제를 완화\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFvraY4io1cD"
      },
      "source": [
        "### Network -> Alphazero 스타일의 딥러닝 모델\n",
        "#### 정책(Policy)과 값(Value)을 동시에 출력하는 신경망을 사용!\n",
        "##### 1. 정책망: 현재 상태에서 가능한 행동들에 대해 확률 분포를 반환\n",
        "##### 2. 가치망: v=1 내가 반드시 승리할 상태 / v=-1 상대방이 반드시 승리할 상태"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRvhnlFso2PO"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, n_residual_block, n_kernel, input_shape, n_actions, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.n_residual_block = n_residual_block #residual block를 쌓아 네트워크 깊이를 조정\n",
        "        self.n_kernel = n_kernel #합성곱 레이어의 필터 수\n",
        "        self.input_shape = input_shape #입력 데이터의 크기 ex)틱택토 (3X3X3)\n",
        "        self.n_actions = n_actions #길이 계산 -> 가능한 행동의 총 개수를 저장 ex)틱택토 9개\n",
        "        self.n_fc_nodes = n_kernel  # Match to the number of kernels\n",
        "\n",
        "        # nn\n",
        "        self.conv_layer = ConvLayer(self.input_shape[0], self.n_kernel) # input_shape[0] = 3\n",
        "        self.residual_blocks = nn.Sequential() #모듈을 차례로 실행\n",
        "        for i in range(self.n_residual_block):\n",
        "            self.residual_blocks.add_module(f\"residual_block_{i+1}\", ResidualBlock(self.n_kernel))\n",
        "        #각 residual block에 이름을 부여하여 추가\n",
        "        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        #공간 차원을 축소해 각 채널별로 하나의 평균값을 반환\n",
        "        # heads and softmax ftn for policy\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(self.n_fc_nodes, self.n_actions),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        #fully connected layer를 통해 행동별 확률 분포를 계산 / 이 행동을 선택했을 때 유리할 가능성\n",
        "        #self.n_fc_nodes: 각 출력 노드 값 (특정 행동 선택 확률) / self.n_actions: 보드 칸 수 (9)\n",
        "        #p = [0.1, 0.3, 0.2, ..., 0.4]\n",
        "        #policy head의 출력값을 확률 분포로 변환\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(self.n_fc_nodes, 1),\n",
        "            nn.Tanh(),\n",
        "            )\n",
        "        #fully connected layer를 통해 현재 상태의 가치를 평가 / 현재 상태의 승리 가능성\n",
        "        #v = 0.8\n",
        "        #value head의 출력값을 [-1,1]로 제한\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = self.residual_blocks(x)\n",
        "        x = self.global_pooling(x) #conv -> residual blocks -> global pooling을 거쳐 1D로 변환\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)  # Optional Dropout\n",
        "\n",
        "        p = self.policy_head(x)  # 행동별 확률 분포\n",
        "        v = self.value_head(x)  # 현재 상태의 가치 평가\n",
        "\n",
        "        return p, v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPngnDtjs3pn"
      },
      "source": [
        "## Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HSDV7fds4xx",
        "outputId": "f9a924ad-36a4-4578-c863-a3fefc2b5d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정책의 출력 크기: (2, 9)\n",
            "가치의 출력 크기: (2, 1)\n",
            "정책 출력:\n",
            " [[0.12674648 0.08209809 0.49866658 0.06866132 0.08782072 0.0182383\n",
            "  0.03088977 0.03405626 0.05282244]\n",
            " [0.03800518 0.04073203 0.5444247  0.20422314 0.08512492 0.01014396\n",
            "  0.0198465  0.03819559 0.01930401]]\n",
            "가치 출력:\n",
            " [[ 0.030595 ]\n",
            " [-0.6824331]]\n"
          ]
        }
      ],
      "source": [
        "N_RESIDUAL_BLOCK = 16\n",
        "N_KERNEL = 64\n",
        "N_ACTIONS = 9\n",
        "INPUT_SHAPE = (3, 3, 3)  # (채널, 높이, 너비)\n",
        "\n",
        "model = Network(N_RESIDUAL_BLOCK, N_KERNEL, INPUT_SHAPE, N_ACTIONS, dropout_rate=0.1) #배치 크기, 채널 수, 높이, 너비\n",
        "#알파제로의 정책 가치 네트워크\n",
        "\n",
        "# example\n",
        "x = torch.randn((2, STATE_DIM, 3, 3), requires_grad=True)\n",
        "\n",
        "# policy and value\n",
        "p, v = model(x) #입력 텐서를 Network 모델에 통과시켜 정책 p와 가치 v를 계산\n",
        "p, v = p.detach().numpy(), v.detach().numpy()\n",
        "\n",
        "print(f\"정책의 출력 크기: {p.shape}\")  # 예상 출력: (2, 9)\n",
        "print(f\"가치의 출력 크기: {v.shape}\")  # 예상 출력: (2, 1)\n",
        "print(\"정책 출력:\\n\", p)\n",
        "print(\"가치 출력:\\n\", v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB8N8XJZpp6x"
      },
      "source": [
        "# 04 MCTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfvQQXN4yLW"
      },
      "source": [
        "### Node (MCTS에서 하나의 상태를 표현하는 노드)\n",
        "#### PUCT 알고리즘: Upper Confidence Bound for Trees\n",
        "1. 게임의 현재 상태 state\n",
        "2. 해당 상태에서 가능한 행동 child_nodes,\n",
        "3. 각 행동의 평가 정보 n,w,p로 구성된다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6oEfruRpsXB"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, state, p):\n",
        "        self.state = state #현재 상태\n",
        "        self.p = p # prior prob / residual 네트워크에서 예측된 행동 확률\n",
        "        self.n = 0 # n_visit / 노드 방문 횟수\n",
        "        self.w = 0 # cum weight / 노드 평가 가치를 누적한 값\n",
        "        self.child_nodes = [] #현재 가능한 모든 행동을 통해 생성된 자식 노드\n",
        "\n",
        "\n",
        "    def evaluate_value(self, model, C_PUCT): #model 이용하여 현재 노드 가치 평가 / 자식 노드를 확장 or 선택\n",
        "        #model -> 정책 및 가치 평가를 수행할 모델\n",
        "        #return -> 현재 노드의 평가 가치\n",
        "        is_done, is_lose = self.state.check_done()\n",
        "\n",
        "        if is_done: #게임이 끝난 상태인지 확인 / 끝난 상태라면 가치 계산\n",
        "            # judge current value / Use the reward system from the common environment\n",
        "            value = self.state.get_reward(self.state)\n",
        "            self.n += 1 #노드 업데이트\n",
        "            self.w += value\n",
        "            #print(f\"[DEBUG] MCTS - Terminal State Reached. Value: {value}\")\n",
        "            return value\n",
        "\n",
        "        # When child node does not exist 자식 노드가 없는 경우\n",
        "        if len(self.child_nodes) == 0:\n",
        "            policy, value = predict(model, self.state) # by using nn\n",
        "            #residual 네트워크를 호출하여 정책 및 가치 예측\n",
        "\n",
        "            # update 평가 결과를 부모 노드로 역전파하여 업데이트\n",
        "            self.n += 1 #탐색 후 방문 횟수 업데이트\n",
        "            self.w += value #탐색 후 누적 가치를 업데이트\n",
        "            #print(f\"[DEBUG] MCTS - Expanding Node. Policy: {policy}, Value: {value}\")\n",
        "\n",
        "            # expand 현재 상태에서 가능한 행동을 기반으로 자식 노드 확장\n",
        "            legal_actions = np.where(self.state.get_legal_actions() != 0)[0]\n",
        "            policy = np.maximum(policy, 1e-3)  # 정책 값이 0이 되지 않도록 최소값 설정\n",
        "            policy /= np.sum(policy)  # 정규화\n",
        "\n",
        "            for action, p in zip(legal_actions, policy):\n",
        "                self.child_nodes.append(Node(self.state.next(action), p))\n",
        "                #가능한 행동과, 이에 대응하는 확률을 사용하여 새로운 자식 노드 생성\n",
        "\n",
        "            return value\n",
        "\n",
        "        # When child node exist 자식 노드가 있는 경우\n",
        "        else:\n",
        "            value = - self.select_next_child_node(C_PUCT).evaluate_value(model, C_PUCT)\n",
        "            #PUCT 알고리즘을 사용해 최적의 자식 노드 선택 / 반환된 값을 반전\n",
        "\n",
        "            # update\n",
        "            self.n += 1\n",
        "            self.w += value\n",
        "\n",
        "            return value\n",
        "\n",
        "\n",
        "    def select_next_child_node(self, C_PUCT): #PUCT 알고리즘을 사용하여 최적의 자식 노드 선택\n",
        "        total_visit = sum(get_n_child(self.child_nodes)) #모든 자식 노드의 총 방문 횟수\n",
        "        values = []\n",
        "        for node in self.child_nodes: #각 자식 노드에 대해~\n",
        "            q_value = -(node.w / node.n) if node.n != 0 else 0 #현재 노드의 평균 가치\n",
        "            #방문하지 않은 노드는 0\n",
        "            node_value = q_value + C_PUCT * node.p * sqrt(total_visit) / (1 + node.n) # PUCT 알고리즘\n",
        "            #탐험 용어: 방문 횟수가 적고, 사전 확률 p가 높은 자식 노드를 더 많이 탐험\n",
        "            values.append(node_value) #PUCT 점수: q_value + node_value를 계산\n",
        "\n",
        "        return self.child_nodes[np.argmax(values)]\n",
        "        #가장 높은 PUCT 점수를 가진 자식 노드를 선택하여 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic-LLHKc4yLX"
      },
      "source": [
        "### def for MCTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UTq-_ov4yLX"
      },
      "source": [
        "### get_n_child"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-qaDSqM4yLX"
      },
      "outputs": [],
      "source": [
        "def get_n_child(child_nodes): #자식 노드 리스트에서 각 노드의 방문 횟수를 추출\n",
        "\n",
        "    child_n = [] #각 노드의 방문 횟수를 담은 리스트\n",
        "\n",
        "    for node in child_nodes:\n",
        "        child_n.append(node.n)\n",
        "\n",
        "    return child_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haM5xlic4yLX"
      },
      "source": [
        "### predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37B-ylZG4yLX"
      },
      "outputs": [],
      "source": [
        "def predict(model, state): #주어진 상태에서 정책 확률과 가치를 계산\n",
        "    # 디바이스 설정\n",
        "    device = next(model.parameters()).device\n",
        "    # NumPy 배열로 변환\n",
        "    input_array = np.array([state.state, state.enemy_state, state.state - state.enemy_state], dtype=np.float32)\n",
        "    input_tensor = torch.tensor(input_array).unsqueeze(0).to(device)  # (1, 3, 3, 3)\n",
        "\n",
        "    model.eval() #모델 평가 모드로 설정\n",
        "    with torch.no_grad():\n",
        "        policy, value = model(input_tensor)\n",
        "        policy = policy.cpu().numpy().reshape(-1)\n",
        "        value = value.cpu().numpy().item()\n",
        "\n",
        "    # Value 정규화 (승패를 더욱 명확하게 구분)\n",
        "    value = np.tanh(value * 25)  # 기존 8에서 15으로 증가하여 승패 차이를 더 극대화\n",
        "\n",
        "    # Debugging policy values\n",
        "    #print(f\"[DEBUG] Predict() - Raw Policy: {policy}, Value: {value}\")\n",
        "\n",
        "    # ✅ 정책 값 정규화 및 NaN 방지\n",
        "    if np.isnan(policy).any() or np.isinf(policy).any() or np.sum(policy) == 0:\n",
        "        print(f\"[WARNING] NaN detected in predict(). Resetting to uniform policy.\")\n",
        "        policy = np.ones_like(policy) / len(policy)\n",
        "\n",
        "    # take legal policy\n",
        "    legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "\n",
        "    # ✅ 정책 크기 정합성 확인 및 보정\n",
        "    if policy.shape[0] != N_ACTIONS:\n",
        "        print(f\"[WARNING] Policy shape mismatch in predict(). Expected {N_ACTIONS}, got {policy.shape[0]}. Adjusting...\")\n",
        "        policy = np.ones(N_ACTIONS) / N_ACTIONS # 기본 균일 분포 설정\n",
        "\n",
        "    policy = policy[legal_actions] if len(legal_actions) > 0 else np.ones_like(policy) / len(policy)\n",
        "\n",
        "    # ✅ 최종 확률 정규화\n",
        "    policy /= np.sum(policy)\n",
        "\n",
        "    #모델에서 반환된 정책 확률 중 현재 상태에서 가능한 행동의 확률만 포함된 리스트\n",
        "\n",
        "    # Debugging normalized policy\n",
        "    #print(f\"[DEBUG] Predict() - Legal Policy: {legal_policy}\")\n",
        "\n",
        "    return policy, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU5ngbmI4yLX"
      },
      "source": [
        "## MCTS (최적의 행동을 선택)\n",
        "#### 각 루트 노드에서 시뮬레이션을 실행하여 다음 액션에 대한 확률을 계산\n",
        "#### 값을 높이면 MCTS가 더 깊이 탐색하므로 더 정확한 정책/가치 평가를 제공"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2_tfGcvprQG"
      },
      "outputs": [],
      "source": [
        "class MCTS:\n",
        "    def __init__(self, initial_pv_evaluate_count=800, max_pv_evaluate_count=1500, growth_rate=1.02, initial_c_puct=2.0, min_c_puct=1.5, decay_rate=0.99):\n",
        "        self.policy = None #MCTS를 통해 계산된 행동 확률 분포를 저장\n",
        "        self.pv_evaluate_count = initial_pv_evaluate_count #MCTS가 현재 상태를 탐색하는 횟수를 정의\n",
        "        self.MAX_PV_EVALUATE_COUNT = max_pv_evaluate_count  # 최대 탐색 횟수\n",
        "        self.GROWTH_RATE = growth_rate  # 증가율\n",
        "        self.C_PUCT = initial_c_puct  # 초기 C_PUCT 값\n",
        "        self.MIN_C_PUCT = min_c_puct  # 최소 C_PUCT 값\n",
        "        self.DECAY_RATE = decay_rate  # 감소율\n",
        "\n",
        "\n",
        "    def update_c_puct(self):\n",
        "        \"\"\" 학습이 진행될수록 C_PUCT 값을 점진적으로 감소 \"\"\"\n",
        "        self.C_PUCT = max(self.MIN_C_PUCT, self.C_PUCT * self.DECAY_RATE)\n",
        "        print(f\"[DEBUG] C_PUCT updated to {self.C_PUCT:.3f}\")\n",
        "\n",
        "\n",
        "    def update_pv_evaluate_count(self):\n",
        "        \"\"\" 학습이 진행될수록 MCTS 시뮬레이션 횟수를 점진적으로 증가 \"\"\"\n",
        "        self.pv_evaluate_count = min(self.MAX_PV_EVALUATE_COUNT, int(self.pv_evaluate_count * self.GROWTH_RATE))\n",
        "        print(f\"[DEBUG] Updated pv_evaluate_count: {self.pv_evaluate_count}\")\n",
        "\n",
        "\n",
        "    def get_policy(self, state, model, temp): #현재 상태에서 최적의 행동 확률 분포 계산\n",
        "        original_temp = temp  # 기존 temp 값 저장\n",
        "\n",
        "        if state.move_count == 0:\n",
        "            temp = max(2.5, original_temp)  # Move 0에서는 온도를 높여 더 균등한 분포 유지\n",
        "\n",
        "        root_node = Node(state, p=1.0)\n",
        "\n",
        "        # ✅ 후공(Player 2)의 탐색을 보다 깊게 수행\n",
        "        adjust_c_puct = self.C_PUCT * (1.3 if not state.check_first_player() else 1.0)\n",
        "        evaluate_count = self.pv_evaluate_count + (100 if not state.check_first_player() else 50)\n",
        "\n",
        "        for _ in range(evaluate_count):\n",
        "            root_node.evaluate_value(model, adjust_c_puct)\n",
        "\n",
        "        childs_n = get_n_child(root_node.child_nodes)\n",
        "        legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "\n",
        "        # ✅ 정책 값 shape 정합성 체크 및 보정\n",
        "        if len(childs_n) != len(legal_actions):\n",
        "            print(f\"[WARNING] Mismatch detected in get_policy(): legal_actions={len(legal_actions)}, childs_n={len(childs_n)}.\")\n",
        "\n",
        "            # Fix: `childs_n` 크기를 `legal_actions` 크기로 조정\n",
        "            if len(legal_actions) > 0:\n",
        "                childs_n = np.ones(len(legal_actions))  # 균일한 확률로 보정\n",
        "            else:\n",
        "                childs_n = np.ones(env.num_actions)  # 기본 크기(9)로 설정\n",
        "\n",
        "        policy = self.boltzmann_dist(childs_n, temp)\n",
        "\n",
        "        # ✅ NaN 값 방지\n",
        "        if np.isnan(policy).any() or np.isinf(policy).any() or np.sum(policy) == 0:\n",
        "            print(f\"[WARNING] Invalid policy detected in get_policy(). Resetting to uniform distribution.\")\n",
        "            policy = np.ones(len(legal_actions)) / len(legal_actions)\n",
        "        else:\n",
        "            policy /= np.sum(policy)  # 정규화\n",
        "\n",
        "        temp = original_temp  # Move 0 후에는 원래 temp 값으로 복구\n",
        "\n",
        "        return policy\n",
        "\n",
        "\n",
        "    def get_actions_of(self, model, temp): #MCTS 통해 계산된 정책을 기반으로 상태에서 행동을 선택\n",
        "        def get_action(state):\n",
        "            self.policy = self.get_policy(state, model, temp)\n",
        "            #get_policy를 호출하여 현재 상태에서 탐색된 행동 확률 분포를 계산하고 저장\n",
        "            legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "            action = np.random.choice(legal_actions, p=self.policy)\n",
        "            #가능한 행동 중 하나를 선택 / 선택 확률은 self.policy에 따라 결정\n",
        "            #ex) 가능한 행동 [0,1,2] / 행동 확률 [0.2, 0.5, 0.3]\n",
        "            return action\n",
        "        return get_action\n",
        "\n",
        "\n",
        "    def boltzmann_dist(self, x_lst, temp):\n",
        "        x_lst = np.array(x_lst, dtype=np.float32) + 1e-8  # 작은 값 추가하여 0으로 나누는 것을 방지\n",
        "\n",
        "        # ✅ NaN 방지: x_lst 값이 너무 크거나 작으면 클리핑 적용\n",
        "        x_lst = np.clip(x_lst, -20, 20)  # log scale에서 안정적인 값 유지\n",
        "\n",
        "        x_lst = np.exp(x_lst / temp)  # 지수 함수 적용\n",
        "\n",
        "        # ✅ 정책 정규화 (합이 1이 되도록 보정)\n",
        "        x_lst /= np.sum(x_lst) if np.sum(x_lst) > 0 else len(x_lst)\n",
        "\n",
        "        # ✅ NaN 확인 및 대체\n",
        "        if np.isnan(x_lst).any() or np.isinf(x_lst).any():\n",
        "            print(f\"[WARNING] NaN detected in boltzmann_dist, replacing with uniform distribution.\")\n",
        "            x_lst = np.ones_like(x_lst) / len(x_lst)\n",
        "\n",
        "        return x_lst.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzW3bIIo-0N"
      },
      "source": [
        "# 05 SelfPlay *환경 적용에 따라 수정한 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgeL5sBSyBE-"
      },
      "source": [
        "## SelfPlay (두 에이전트가 대전하여 데이터 생성)\n",
        "### MCTS와 정책-가치 네트워크를 활용\n",
        "#### 랜덤 행동 비율을 추가하여 초반 데이터 다양성을 증가!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUrcYuCLCS-K"
      },
      "outputs": [],
      "source": [
        "# Self-play 데이터 크기 제한 (최신 N개의 게임만 유지)\n",
        "MAX_HISTORY_SIZE = 5000  # 최근 5000개의 게임 기록만 유지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2mdF10T4yLY"
      },
      "outputs": [],
      "source": [
        "class SelfPlay:\n",
        "    def __init__(self, model, temp, sp_game_count, pv_evaluate_count, temp_discount=0.999995):\n",
        "        # model\n",
        "        self.model = model #정책-가치 네트워크 모델\n",
        "\n",
        "        # params\n",
        "        self.sp_game_count = sp_game_count #self-play 게임 횟수\n",
        "\n",
        "        # about temps\n",
        "        self.temp = temp #초기 볼츠만 온도 파라미터\n",
        "        self.temp_discount = 0.999995 #온도 감소율\n",
        "\n",
        "        self.history = [] # selfPlay's yield / self-play 결과를 저장 (게임 상태, 정책, 가치)\n",
        "\n",
        "        self.pv_evaluate_count = pv_evaluate_count\n",
        "        self.mcts = MCTS(self.pv_evaluate_count) #MCTS에 전달\n",
        "\n",
        "        self.env = Environment()\n",
        "\n",
        "        self.FIRST_PLAYER_BIAS = 0.6  # Player 1이 선공할 확률을 60%로 설정\n",
        "\n",
        "\n",
        "    def get_first_player_value(self, ended_state): #게임 종료 상태에서 첫번째 플레이어의 승패를 평가\n",
        "        # Use the common environment's reward system\n",
        "        reward = self.env.get_reward(ended_state)\n",
        "        return reward if ended_state.check_first_player() else -reward\n",
        "\n",
        "\n",
        "    def _single_play(self, i): #하나의 self-play 게임을 수행하여 데이터를 생성\n",
        "        history = [] #게임 중 저장된 데이터\n",
        "        # Player 1이 선공할 확률을 FIRST_PLAYER_BIAS로 조정\n",
        "        first_player = random.random() < self.FIRST_PLAYER_BIAS\n",
        "        # 초기 상태 설정\n",
        "        #state = State() if first_player else State(state=np.zeros((self.env.n, self.env.n)),\n",
        "                                               #enemy_state=np.zeros((self.env.n, self.env.n)))\n",
        "\n",
        "        # ✅ State 객체 생성 시 move_count 명확히 설정\n",
        "        if first_player:\n",
        "            state = State(move_count=0)  # 선공 (Player 1)\n",
        "        else:\n",
        "            state = State(state=np.zeros(STATE_SIZE, dtype=int),\n",
        "                          enemy_state=np.zeros(STATE_SIZE, dtype=int),\n",
        "                          move_count=2)  # 후공 (Player 2), move_count=2로 설정\n",
        "\n",
        "        legal_actions = np.where(state.get_legal_actions() != 0)[0] # 현재 상태에서 가능한 행동들을 가져옴\n",
        "\n",
        "        # 첫 번째 수를 MCTS 기반으로 선택\n",
        "        if len(legal_actions) > 0:\n",
        "            policy = self.mcts.get_policy(state, self.model, self.temp)  # MCTS로 정책 계산\n",
        "\n",
        "            # ✅ 정책 값에서 NaN 확인 및 정규화\n",
        "            if np.isnan(policy).any() or np.sum(policy) == 0:\n",
        "                print(f\"[WARNING] NaN detected in policy at Game {i + 1}. Replacing with uniform distribution.\")\n",
        "                policy = np.ones_like(legal_actions) / len(legal_actions)\n",
        "\n",
        "            action = np.random.choice(legal_actions, p=policy)  # MCTS 정책 기반 선택\n",
        "            state = state.next(action)\n",
        "            # 선공에 따라 move_count를 올바르게 설정\n",
        "            #state.move_count = 0 if first_player else 1 # 첫 수를 둔 후 move_count를 조정하여 불일치 방지\n",
        "\n",
        "            print(f\"Game {i + 1}: {'Player 1' if first_player else 'Player 2'} starts, first move at {action}\")\n",
        "\n",
        "        # 디버깅 메시지 추가\n",
        "        print(f\"[DEBUG] Game {i + 1}: first_player={first_player}\")# check_first_player()={state.check_first_player()}\")\n",
        "\n",
        "        move_count = 0\n",
        "\n",
        "        # 게임 진행 (MCTS를 사용하여 수를 선택)\n",
        "        while not state.check_done()[0]: #여기서는 state.is_done()이 true일 때 루프 종료\n",
        "            move_count += 1\n",
        "            policy = self.mcts.get_policy(state, self.model, self.temp)\n",
        "\n",
        "            # ✅ 정책 값에서 NaN 확인 및 정규화\n",
        "            if np.isnan(policy).any() or np.sum(policy) == 0:\n",
        "                print(f\"[WARNING] NaN detected in policy during move {move_count} of Game {i + 1}.\")\n",
        "                policy = np.ones_like(legal_actions) / len(legal_actions)\n",
        "\n",
        "            legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "\n",
        "            # Check if legal_actions is empty before random move selection\n",
        "            if len(legal_actions) == 0:\n",
        "                print(f\"Game {i + 1}: No legal actions available, ending game.\")\n",
        "                break  # or handle it differently\n",
        "\n",
        "            final_policy = np.zeros([self.env.num_actions]) # init value / 행동 확률 초기화\n",
        "            final_policy[legal_actions] = policy\n",
        "            #현재 상태에서 가능한 행동에 대해서만 확률을 채우기\n",
        "            #전체 행동 공간에 대한 정책이 완성!\n",
        "            #불가능한 행동의 확률은 0으로 유지, 가능한 행동의 확률은 MCTS 결과에 기반~\n",
        "            history.append([state, final_policy, 0]) #데이터 저장 (보드 상태, 정책, 가치)\n",
        "            #state_n.board: 게임의 n번째 상태\n",
        "            #policy_n: 해당 상태에서의 정책(확률 분포)\n",
        "            #None: 가치(아직 업데이트되지 않음)\n",
        "\n",
        "            action = np.random.choice(legal_actions, p=policy)\n",
        "            state = state.next(action)\n",
        "\n",
        "        # 게임 종료 후 보상 계산\n",
        "        final_reward = self.env.get_reward(state)\n",
        "        player1_reward = final_reward if first_player else -final_reward\n",
        "        player2_reward = -player1_reward\n",
        "\n",
        "        # 게임 결과 출력\n",
        "        game_result = \"Draw\" if final_reward == 0 else \"Player 1 Wins\" if player1_reward > 0 else \"Player 2 Wins\"\n",
        "        print(f\"Game {i + 1} Result: {game_result} (P1: {player1_reward}, P2: {player2_reward})\")\n",
        "\n",
        "        # 구분선 추가 (게임 종료 후)\n",
        "        print(\"-\" * 20)  # 가독성을 위한 구분선\n",
        "\n",
        "        # 히스토리의 가치 업데이트 (각 턴마다 반전)\n",
        "        value = player1_reward\n",
        "        for j in range(len(history)): #가치를 게임 히스토리에 업데이트\n",
        "            history[j][-1] = value #각 상태 history[i]의 마지막 요소에 가치를 기록\n",
        "            #state.board, final_policy, None -> state_i.board, policy_i, value\n",
        "            value = -value #각 턴마다 가치를 반전하여 저장\n",
        "            #첫번째 플레이어의 가치와 두번째 플레이어의 가치는 반대이다!\n",
        "            #즉 한 플레이어가 유리한 상태일수록 상대방은 불리한 상태가 된다~\n",
        "\n",
        "        # discount temp / 온도 감소\n",
        "        if i >= 30:\n",
        "            self.temp = max(0.75, self.temp * self.temp_discount)\n",
        "        #초기 30턴까지는 온도를 유지, 이후에는 temp_discount를 적용하여 점진적으로 감소\n",
        "\n",
        "        return history #종료 상태의 가치(점수)를 히스토리에 기록\n",
        "\n",
        "\n",
        "    def _self_play(self): #지정된 횟수만큼 self-play를 반복 수행\n",
        "        # sp_game_count 번 만큼 single play 시행\n",
        "        for i in range(self.sp_game_count): #지정된 게임 횟수만큼 반복\n",
        "            single_history = self._single_play(i) #한 번의 게임 실행\n",
        "            self.history.extend(single_history) #게임 결과를 self.history 전체 히스토리에 추가\n",
        "\n",
        "            # 메모리 사용량을 줄이기 위해 오래된 데이터 삭제\n",
        "            if len(self.history) > MAX_HISTORY_SIZE:\n",
        "                self.history = self.history[-MAX_HISTORY_SIZE:]  # 최근 기록만 유지\n",
        "\n",
        "            if (i+1) % (self.sp_game_count // 10) == 0: #게임 진행 상태를 10% 단위로 출력\n",
        "                print(f\"self play {i+1} / {self.sp_game_count}\")\n",
        "\n",
        "\n",
        "    def __call__(self, execute_self_play=False):\n",
        "        \"\"\"\n",
        "        SelfPlay 객체를 호출 시 self-play 실행 여부를 제어.\n",
        "        - execute_self_play: True이면 self-play 실행, False이면 실행하지 않음.\n",
        "        \"\"\"\n",
        "        if execute_self_play:\n",
        "            print(\"Self-play 실행 중...\")\n",
        "            self._self_play()\n",
        "        else:\n",
        "            print(\"Self-play 실행이 비활성화되었습니다.\")\n",
        "\n",
        "\n",
        "    def reset_history(self):\n",
        "        \"\"\"\n",
        "        Self-play 데이터를 초기화합니다.\n",
        "        \"\"\"\n",
        "        self.history = []\n",
        "        print(\"Self-play 데이터가 초기화되었습니다.\")\n",
        "\n",
        "\n",
        "    def update_model(self, model):  # 모델 업데이트 메서드\n",
        "        self.model.load_state_dict(model.module.state_dict())  # 주어진 모델의 가중치를 현재 모델에 복사"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6b3bSYgGyZnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Cj0nAg0qxx"
      },
      "source": [
        "# 06 TrainNetwork"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTAqJbv-x-fY"
      },
      "source": [
        "## TrainNetwork\n",
        "### 알파제로 알고리즘의 정책-가치 네트워크를 학습\n",
        "#### self-play로 생성된 데이터를 사용하여 정책-가치 네트워크를 업데이트\n",
        "1. 샘플링된 데이터를 준비\n",
        "2. 손실 함수 계산 (정책 및 가치 손실)\n",
        "3. 역전파를 통해 네트워크 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPVHDQaF7uPl"
      },
      "outputs": [],
      "source": [
        "class TrainNetwork:\n",
        "    def __init__(self, model, batch_size, learning_rate, learn_epoch, save_loss=True):\n",
        "        # define\n",
        "        self.losses = [] # elements : ( p_loss, v_loss, total_loss )\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # model & device\n",
        "        self.model = model\n",
        "        self.device = next(model.parameters()).device #네트워크가 실행될 디바이스 (GPU/CPU)\n",
        "\n",
        "        # learning rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.learn_decay = 0.1\n",
        "        self.learn_epoch = learn_epoch #학습률 감소 주기\n",
        "\n",
        "        # define loss ftn / 손실 함수\n",
        "        self.mse_loss = F.mse_loss #가치 손실 계산\n",
        "        #네트워크가 예측한 가치와 실제 가치 간의 차이를 최소화\n",
        "        #가치는 연속형 데이터로 [-1,1] 범위의 실수 (회귀 문제)\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss() #정책 손실 계산\n",
        "        #네트워크가 예측한 정책과 실제 정책 간의 차이를 최소화\n",
        "        #정책은 확률 분포로 클래스 확률 값, 즉 확률의 합이 1 (분류 문제)\n",
        "\n",
        "        # Optimizer & Scheduler\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, eps=1e-4, weight_decay=1e-4) # L2 정규화 포함\n",
        "        #가중치를 학습시키기 위해 사용\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=self.learn_epoch, gamma = self.learn_decay)\n",
        "        #학습률을 learn_epoch 주기로 감소시켜 학습\n",
        "\n",
        "        self.save_loss = save_loss  # 손실을 저장할지 여부\n",
        "        self.loss_file = \"train_losses.csv\"\n",
        "\n",
        "        # CSV 파일이 없으면 헤더 추가\n",
        "        if self.save_loss and not os.path.exists(self.loss_file):\n",
        "            with open(self.loss_file, \"w\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"epoch\", \"p_loss\", \"v_loss\", \"total_loss\"])\n",
        "\n",
        "\n",
        "    def _train(self, history, epoch): #self-play에서 생성된 데이터를 사용해 네트워크를 학습\n",
        "        # 샘플링\n",
        "        # history에 배치 샘플링을 위한 충분한 요소가 있는지 확인\n",
        "        num_samples = len(history) # history의 실제 크기 가져오기\n",
        "        batch_size = min(self.batch_size, num_samples)  # history가 더 짧으면 더 작은 배치를 사용\n",
        "        sample = random.choices(history, k=batch_size) #history에서 데이터를 무작위로 샘플링\n",
        "\n",
        "        states, target_policies, target_values = zip(*sample)\n",
        "        #샘플 데이터는 상태, 목표 정책, 목표 가치로 분리\n",
        "\n",
        "        # `reward` 값이 비정상적으로 -1로만 채워졌는지 확인\n",
        "        # print(f\"[DEBUG] Target values before normalization: {target_values}\")\n",
        "\n",
        "        # NumPy 배열로 변환\n",
        "        expanded_states = np.array([\n",
        "            np.array([s.state, s.enemy_state, s.state - s.enemy_state], dtype=np.float32)\n",
        "            for s in states\n",
        "        ])  # (BATCH, 3, 3, 3)\n",
        "\n",
        "        #샘플 데이터를 파이토치 텐서로 변환\n",
        "        states = torch.tensor(expanded_states, dtype=torch.float32)  # (BATCH, STATE_DIM, 3, 3)\n",
        "        target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).view(batch_size, -1)  # (BATCH, 9)\n",
        "        target_values = torch.tensor(np.array(target_values), dtype=torch.float32).view(batch_size, -1)  # (BATCH, 1)\n",
        "\n",
        "        target_values = (target_values - target_values.mean()) / (target_values.std() + 1e-6)  # 정규화\n",
        "\n",
        "        # print(f\"[DEBUG] Target values after normalization: {target_values}\")\n",
        "\n",
        "        # device에 올리기 / 데이터를 디바이스 (GPU/CPU)에 전송\n",
        "        states, target_policies, target_values = states.to(self.device), target_policies.to(self.device), target_values.to(self.device)\n",
        "\n",
        "        # 학습 실행\n",
        "        self.model.train()\n",
        "        policy, value = self.model(states) #정책-가치 네트워크 실행\n",
        "\n",
        "        # 손실 계산\n",
        "        p_loss = self.cross_entropy_loss(policy, target_policies) #정책 손실\n",
        "        v_loss = 0.25 * self.mse_loss(value, target_values) #가치 손실\n",
        "        total_loss = p_loss + 0.5 * v_loss #총 손실 / 기존보다 Value Loss 기여도를 낮춤\n",
        "\n",
        "        # Logging losses / 손실 기록\n",
        "        self.losses.append((p_loss.item(), v_loss.item(), total_loss.item()))\n",
        "\n",
        "        # 10 에포크마다 CSV 파일에 저장\n",
        "        if epoch % 10 == 0 and self.save_loss:\n",
        "            with open(self.loss_file, \"a\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerows([(epoch, p, v, t) for p, v, t in self.losses])\n",
        "            self.losses.clear()  # 저장 후 메모리에서 삭제하여 불필요한 메모리 사용 방지\n",
        "\n",
        "        # Backpropagation / 역전파 및 업데이트\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step() #가중치 업데이트\n",
        "        self.scheduler.step() #학습률 감소\n",
        "\n",
        "\n",
        "    def __call__(self, history, epoch):\n",
        "        self._train(history, epoch) #호출 시 내부적으로 _train 매서드 실행\n",
        "        #train_network._train(history) 직접 호출 -> train_network(history) 함수처럼 호출\n",
        "\n",
        "\n",
        "    def update_model(self, model): #self-play에 사용되는 네트워크 모델을 업데이트 / 최신 학습된 모델로 교체\n",
        "        self.model.load_state_dict(model.module.state_dict())\n",
        "        #현재 클래스가 사용하는 모델을 외부에서 제공된 새로운 모델의 가중치로 업데이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vESf5R59qVh5"
      },
      "source": [
        "# 07 EvalNetwork\n",
        "### 학습된 최근 모델을 현재 최고 모델과 비교 평가하는 역할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBUrMkZBEnLs"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpu96qOGDj7T"
      },
      "outputs": [],
      "source": [
        "save_path = '/content/drive/My Drive/3-2/강화'\n",
        "os.makedirs(save_path, exist_ok=True)  # 폴더 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V3tv0d9qacD"
      },
      "outputs": [],
      "source": [
        "class EvalNetwork: #모델 성능 평가를 담당\n",
        "    def __init__(self, best_model, eval_game_count, eval_temperature, eval_count, env):\n",
        "        # eval info\n",
        "        self.eval_game_count = eval_game_count #평가 시 실행할 게임 수\n",
        "        self.eval_temperature = eval_temperature #볼츠만 분포 온도 파라미터\n",
        "\n",
        "        # models\n",
        "        self.best_model = best_model #현재 최고 모델\n",
        "        self.recent_model = None #비교할 최신 모델 / 초기화 시에는 None\n",
        "        #평가 후, 최신 모델이 더 성능이 좋으면 최고 모델로 업데이트~\n",
        "\n",
        "        # MCTS\n",
        "        self.mcts = MCTS(eval_count) #평가에 사용할 MCTS 탐색 개체\n",
        "\n",
        "        # update / win\n",
        "        self.updated = False #최근 모델이 최고 모델로 업데이트되었는지 여부\n",
        "        self.win_rate = 0.0 #최근 모델의 평균 승률\n",
        "\n",
        "        self.env = env  # AlphaZeroAgent에서 전달받도록 수정\n",
        "\n",
        "    def _first_player_point(self, ended_state):\n",
        "        reward = self.env.get_reward(ended_state)\n",
        "        # 첫 번째 플레이어일 경우 그대로 반환, 두 번째 플레이어일 경우 반전\n",
        "        return reward if ended_state.check_first_player() else -reward\n",
        "\n",
        "\n",
        "    def _evaluate_network(self, recent_model): #최신 모델을 현재 최고 모델과 평가\n",
        "        self.updated = False #초기화\n",
        "        self.recent_model = recent_model\n",
        "\n",
        "        # MCTS를 이용\n",
        "        next_actions_recent = self.mcts.get_actions_of(self.recent_model, self.eval_temperature)\n",
        "        #최신 모델의 행동 생성 함수 / 각 모델의 행동 확률 생성\n",
        "        next_actions_best = self.mcts.get_actions_of(self.best_model, self.eval_temperature)\n",
        "        #최고 모델의 행동 생성 함수\n",
        "        next_actions = (next_actions_recent, next_actions_best)\n",
        "\n",
        "        total_point = 0\n",
        "\n",
        "        for i in range(self.eval_game_count): #지정된 게임 수만큼 평가 진행\n",
        "            if i % 2 == 0: #짝수 게임: 최신 모델이 선공\n",
        "                total_point += self._single_play(next_actions)\n",
        "            else: #홀수 게임: 최신 모델이 후공\n",
        "                total_point += 1 - self._single_play(list(reversed(next_actions)))\n",
        "\n",
        "        average_point = total_point / self.eval_game_count #각 게임 결과를 누적하여 평균 점수 계산\n",
        "        print('Average Point of Latest Model', average_point)\n",
        "\n",
        "        if average_point >= 0.5:\n",
        "            self._update_best_model() #평균 점수가 0.5 이상이면 최고 모델 업데이트\n",
        "\n",
        "\n",
        "    def _single_play(self, next_actions): #단일 게임을 실행하여 첫번째 플레이어의 점수 반환\n",
        "        state = State() #초기 상태 생성\n",
        "        while not state.check_done()[0]:\n",
        "            next_action = next_actions[0] if state.check_first_player() else next_actions[1]\n",
        "            #행동 생성 함수를 호출하여 행동 선택\n",
        "            #next_actions[0]: next_actions_recent\n",
        "            #next_actions[1]: next_actions_best\n",
        "            action = next_action(state)\n",
        "            state = state.next(action) #선택된 행동에 따라 다음 상태로 이동\n",
        "        return self._first_player_point(state) #첫번째 플레이어의 점수 반환\n",
        "\n",
        "\n",
        "    def _update_best_model(self): #최신 모델을 현재 최고 모델로 업데이트\n",
        "        #DataParallel로 감싸져 있는지 확인하고, 상태에 따라 적절히 state_dict를 로드\n",
        "        if isinstance(self.recent_model, torch.nn.DataParallel):\n",
        "            recent_model_state_dict = self.recent_model.module.state_dict()  # DataParallel 제거\n",
        "        else:\n",
        "            recent_model_state_dict = self.recent_model.state_dict()\n",
        "\n",
        "        if isinstance(self.best_model, torch.nn.DataParallel):\n",
        "            self.best_model.module.load_state_dict(recent_model_state_dict)  # DataParallel 제거 후 로드\n",
        "        else:\n",
        "            self.best_model.load_state_dict(recent_model_state_dict)\n",
        "\n",
        "        self.updated = True\n",
        "        print(\"Best Model is Updated.\")\n",
        "\n",
        "\n",
        "    #현재 최고 모델의 가중치를 지정된 경로에 저장\n",
        "    def save_model(self, file_name=\"best_model_env_weights.pth\"):\n",
        "        save_file_path = os.path.join(save_path, file_name)\n",
        "        if isinstance(self.best_model, torch.nn.DataParallel):\n",
        "            torch.save(self.best_model.module.state_dict(), save_file_path)\n",
        "        else:\n",
        "            torch.save(self.best_model.state_dict(), save_file_path)\n",
        "        print(f\"Model saved to {save_file_path}\")\n",
        "\n",
        "\n",
        "    def __call__(self, recent_model):\n",
        "        self._evaluate_network(recent_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kgQRjCvvz0t"
      },
      "source": [
        "# 08. AlphaZeroAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SozNqLGz9o8Z"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP0MaM1-58au"
      },
      "outputs": [],
      "source": [
        "# Updated main code with optimizations / device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZZwb5U-y80Q"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroAgent:\n",
        "    def __init__(self, model_path, sp_game_count, eval_game_count, learning_rate, batch_size, epochs, learn_epoch, pv_evaluate_count, env):\n",
        "        \"\"\"\n",
        "        AlphaZeroAgent 초기화 메서드\n",
        "        \"\"\"\n",
        "        # 모델 초기화 및 가중치 로드\n",
        "        self.model = Network(N_RESIDUAL_BLOCK, N_KERNEL, STATE_SIZE, N_ACTIONS)\n",
        "        self.model = nn.DataParallel(self.model).to(device) #Multi-GPU 활용\n",
        "\n",
        "        # 가중치 로드\n",
        "        if model_path:\n",
        "            params = torch.load(model_path, map_location=device)  # map_location 추가\n",
        "            self.model.load_state_dict(params, strict=False)\n",
        "\n",
        "        # SelfPlay, TrainNetwork, EvalNetwork 초기화\n",
        "        self.selfplay = SelfPlay(self.model, SP_TEMPERATURE, sp_game_count, pv_evaluate_count)\n",
        "        self.train_network = TrainNetwork(self.model, batch_size, learning_rate, LEARN_EPOCH)\n",
        "        self.eval_network = EvalNetwork(self.model, eval_game_count, EVAL_TEMPERATURE, EVAL_COUNT, env)\n",
        "\n",
        "        # 학습 관련 설정\n",
        "        self.epochs = epochs\n",
        "        self.sp_game_count = sp_game_count\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        MCTS를 이용해 현재 상태에서 최적의 행동을 반환합니다.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'selfplay') or not hasattr(self.selfplay, 'mcts'):\n",
        "            raise AttributeError(\"MCTS 객체가 초기화되지 않았습니다.\")\n",
        "\n",
        "        # MCTS를 사용하여 행동 확률을 계산\n",
        "        action_probs = self.selfplay.mcts.get_policy(state, self.model, SP_TEMPERATURE)\n",
        "        # 확률에 따라 행동을 선택\n",
        "        legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "        return np.random.choice(legal_actions, p=action_probs)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.epochs):\n",
        "            print(f'\\n[Epoch {i + 1}/{self.epochs}] --------------------------------')\n",
        "\n",
        "\n",
        "            for j in range(self.sp_game_count):  # self-play 한 판 → 학습 한 판 반복\n",
        "                print(f'\\n[Game {j + 1}/{self.sp_game_count} in Epoch {i + 1}]')\n",
        "\n",
        "\n",
        "                # Self-play 수행 (한 판만 진행)\n",
        "                start_time = time.time()\n",
        "                with torch.no_grad():\n",
        "                    self.selfplay._single_play(j)  # 한 판만 수행\n",
        "                print(f\"Single Self-play 완료. 소요 시간: {time.time() - start_time:.2f}초\")\n",
        "\n",
        "                # Training 수행 (바로 학습)\n",
        "                start_time = time.time()\n",
        "                self.train_network(self.selfplay.history, i+1)  # 손실 반환\n",
        "                print(f\"Training 완료. 소요 시간: {time.time() - start_time:.2f}초\")\n",
        "\n",
        "                # Loss 출력\n",
        "                if len(self.train_network.losses) > 0:\n",
        "                    latest_p_loss, latest_v_loss, latest_total_loss = self.train_network.losses[-1]\n",
        "                    print(f\"🔹 Game {j + 1}: Policy Loss = {latest_p_loss:.4f}, Value Loss = {latest_v_loss:.4f}, Total Loss = {latest_total_loss:.4f}\")\n",
        "\n",
        "                # Model evaluation (한 epoch 단위로 평가)\n",
        "                start_time = time.time()\n",
        "                self.eval_network(self.train_network.model)\n",
        "                print(f\"Evaluation 완료. 소요 시간: {time.time() - start_time:.2f}초\")\n",
        "\n",
        "                # C_PUCT 및 MCTS 시뮬레이션 횟수 업데이트 적용\n",
        "                self.selfplay.mcts.update_c_puct()\n",
        "                self.selfplay.mcts.update_pv_evaluate_count()\n",
        "\n",
        "                # 최신 모델이 최고 모델보다 성능이 좋다면 업데이트\n",
        "                if self.eval_network.updated:\n",
        "                    print(f\"Saving best model at epoch {i + 1}...\")\n",
        "                    self.eval_network.save_model(f\"model_env_checkpoint_epoch_{i + 1}.pth\")  # 에포크별 파일 저장\n",
        "                    self.eval_network.save_model()  # 기본 파일 이름(best_model_env_weights.pth)으로 저장\n",
        "                    print(\"Model saved.\")\n",
        "\n",
        "                    print(\"최신 모델이 최고 모델로 업데이트되었습니다.\")\n",
        "                    self.eval_network.updated = False  # 업데이트 상태 초기화\n",
        "\n",
        "        print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMedV3oeqfeA"
      },
      "source": [
        "# 09. Main\n",
        "#### self-play, 모델 학습, 평가, 최고 모델 업데이트 -> 모델 성능 점진적으로 개선"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXCBd648DhzG"
      },
      "outputs": [],
      "source": [
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ItLuIud3_qnx",
        "outputId": "10ff6e77-6f44-4bc5-af54-d46cfacc86f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.73초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 20: Policy Loss = 2.0406, Value Loss = 0.0613, Total Loss = 2.0713\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 66.15초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Epoch 3/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 3]\n",
            "Game 1: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.58초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "🔹 Game 1: Policy Loss = 2.0435, Value Loss = 0.0519, Total Loss = 2.0695\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 64.12초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 3]\n",
            "Game 2: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.97초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 2: Policy Loss = 2.0399, Value Loss = 0.0157, Total Loss = 2.0478\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 78.68초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 3/20 in Epoch 3]\n",
            "Game 3: Player 2 starts, first move at 7\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.87초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 3: Policy Loss = 2.0260, Value Loss = 0.0320, Total Loss = 2.0420\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.61초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 3]\n",
            "Game 4: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.66초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 4: Policy Loss = 2.0403, Value Loss = 0.0343, Total Loss = 2.0574\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 76.75초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 3]\n",
            "Game 5: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.83초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 5: Policy Loss = 2.0421, Value Loss = 0.0360, Total Loss = 2.0601\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.53초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 3]\n",
            "Game 6: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 6: first_player=False\n",
            "Game 6 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.49초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 6: Policy Loss = 2.0446, Value Loss = 0.0219, Total Loss = 2.0556\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 79.10초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 3]\n",
            "Game 7: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.34초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0378, Value Loss = 0.0494, Total Loss = 2.0625\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.01초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 3]\n",
            "Game 8: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.80초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 8: Policy Loss = 1.9957, Value Loss = 0.0225, Total Loss = 2.0069\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 71.68초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 9/20 in Epoch 3]\n",
            "Game 9: Player 1 starts, first move at 7\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.59초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 9: Policy Loss = 2.0146, Value Loss = 0.0382, Total Loss = 2.0337\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.54초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 3]\n",
            "Game 10: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.35초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 10: Policy Loss = 2.0257, Value Loss = 0.0314, Total Loss = 2.0414\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 74.41초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 3]\n",
            "Game 11: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.56초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0525, Value Loss = 0.0784, Total Loss = 2.0917\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.63초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 3]\n",
            "Game 12: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.02초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 12: Policy Loss = 2.0315, Value Loss = 0.0586, Total Loss = 2.0608\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.76초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 3]\n",
            "Game 13: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.95초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 13: Policy Loss = 2.0271, Value Loss = 0.0321, Total Loss = 2.0432\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 63.83초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 14/20 in Epoch 3]\n",
            "Game 14: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.04초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 14: Policy Loss = 2.0104, Value Loss = 0.0345, Total Loss = 2.0277\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 61.73초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 15/20 in Epoch 3]\n",
            "Game 15: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=False\n",
            "Game 15 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.14초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 15: Policy Loss = 2.0292, Value Loss = 0.0268, Total Loss = 2.0426\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 62.68초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 3]\n",
            "Game 16: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.55초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 16: Policy Loss = 2.0501, Value Loss = 0.0217, Total Loss = 2.0610\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 60.88초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 17/20 in Epoch 3]\n",
            "Game 17: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.16초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 17: Policy Loss = 2.0545, Value Loss = 0.0385, Total Loss = 2.0737\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 62.55초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 18/20 in Epoch 3]\n",
            "Game 18: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 18: first_player=False\n",
            "Game 18 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 8.52초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 18: Policy Loss = 2.0124, Value Loss = 0.0137, Total Loss = 2.0193\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 58.56초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 3]\n",
            "Game 19: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.27초\n",
            "Training 완료. 소요 시간: 0.45초\n",
            "🔹 Game 19: Policy Loss = 2.0423, Value Loss = 0.0574, Total Loss = 2.0710\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 64.23초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 3...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_3.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 3]\n",
            "Game 20: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.14초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 20: Policy Loss = 2.0104, Value Loss = 0.0383, Total Loss = 2.0295\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 65.28초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Epoch 4/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 4]\n",
            "Game 1: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.00초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 1: Policy Loss = 2.0062, Value Loss = 0.0472, Total Loss = 2.0298\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 68.04초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 4]\n",
            "Game 2: Player 2 starts, first move at 7\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.03초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 2: Policy Loss = 2.0191, Value Loss = 0.0212, Total Loss = 2.0297\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 67.46초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 4]\n",
            "Game 3: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 3: first_player=True\n",
            "Game 3 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.47초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "🔹 Game 3: Policy Loss = 2.0649, Value Loss = 0.0108, Total Loss = 2.0703\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.61초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 4]\n",
            "Game 4: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.08초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 4: Policy Loss = 2.0457, Value Loss = 0.0224, Total Loss = 2.0569\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 68.25초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 4]\n",
            "Game 5: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 5: first_player=False\n",
            "Game 5 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.08초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "🔹 Game 5: Policy Loss = 2.0561, Value Loss = 0.0304, Total Loss = 2.0713\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.92초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 4]\n",
            "Game 6: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.74초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 6: Policy Loss = 2.0490, Value Loss = 0.0330, Total Loss = 2.0654\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.39초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 4]\n",
            "Game 7: Player 1 starts, first move at 7\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.25초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 7: Policy Loss = 2.0329, Value Loss = 0.0248, Total Loss = 2.0453\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.62초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 4]\n",
            "Game 8: Player 1 starts, first move at 7\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 17.05초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 8: Policy Loss = 2.0296, Value Loss = 0.0200, Total Loss = 2.0396\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.57초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 4]\n",
            "Game 9: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.16초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 9: Policy Loss = 2.0387, Value Loss = 0.0264, Total Loss = 2.0519\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 70.33초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 10/20 in Epoch 4]\n",
            "Game 10: Player 2 starts, first move at 7\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 18.32초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 10: Policy Loss = 2.0366, Value Loss = 0.0472, Total Loss = 2.0602\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 69.83초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 4]\n",
            "Game 11: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.45초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 11: Policy Loss = 2.0302, Value Loss = 0.0395, Total Loss = 2.0499\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.39초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 4]\n",
            "Game 12: Player 1 starts, first move at 7\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.19초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 12: Policy Loss = 2.0441, Value Loss = 0.0195, Total Loss = 2.0539\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.05초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 13/20 in Epoch 4]\n",
            "Game 13: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 13: first_player=False\n",
            "Game 13 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.74초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 13: Policy Loss = 2.0554, Value Loss = 0.0467, Total Loss = 2.0788\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 4]\n",
            "Game 14: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.31초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 14: Policy Loss = 2.0239, Value Loss = 0.0444, Total Loss = 2.0460\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.73초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 4]\n",
            "Game 15: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.08초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 15: Policy Loss = 2.0420, Value Loss = 0.0393, Total Loss = 2.0616\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.89초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 4]\n",
            "Game 16: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.68초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 16: Policy Loss = 2.0164, Value Loss = 0.0182, Total Loss = 2.0255\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 67.66초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 17/20 in Epoch 4]\n",
            "Game 17: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.97초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 17: Policy Loss = 2.0554, Value Loss = 0.0142, Total Loss = 2.0625\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 66.08초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 4]\n",
            "Game 18: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.31초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 18: Policy Loss = 2.0426, Value Loss = 0.0295, Total Loss = 2.0573\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.21초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 4]\n",
            "Game 19: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.05초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 19: Policy Loss = 2.0081, Value Loss = 0.0383, Total Loss = 2.0273\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 61.43초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 4]\n",
            "Game 20: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 20: first_player=False\n",
            "Game 20 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.62초\n",
            "Training 완료. 소요 시간: 0.45초\n",
            "🔹 Game 20: Policy Loss = 2.0569, Value Loss = 0.0225, Total Loss = 2.0682\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 60.88초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 4...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_4.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 5/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 5]\n",
            "Game 1: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.58초\n",
            "Training 완료. 소요 시간: 0.30초\n",
            "🔹 Game 1: Policy Loss = 1.9920, Value Loss = 0.0451, Total Loss = 2.0146\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 63.01초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 2/20 in Epoch 5]\n",
            "Game 2: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.33초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 2: Policy Loss = 2.0281, Value Loss = 0.0440, Total Loss = 2.0501\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 68.77초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 5]\n",
            "Game 3: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 3: first_player=True\n",
            "Game 3 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.90초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 3: Policy Loss = 2.0291, Value Loss = 0.0327, Total Loss = 2.0455\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 69.42초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 5]\n",
            "Game 4: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.29초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 2.0584, Value Loss = 0.0166, Total Loss = 2.0667\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 68.51초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 5]\n",
            "Game 5: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=False\n",
            "Game 5 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.30초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 5: Policy Loss = 2.0287, Value Loss = 0.0379, Total Loss = 2.0476\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.55초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 5]\n",
            "Game 6: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.99초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 6: Policy Loss = 2.0160, Value Loss = 0.0183, Total Loss = 2.0251\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.75초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 5]\n",
            "Game 7: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.81초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 7: Policy Loss = 2.0295, Value Loss = 0.0314, Total Loss = 2.0451\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.14초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 5]\n",
            "Game 8: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.77초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 8: Policy Loss = 2.0438, Value Loss = 0.0277, Total Loss = 2.0576\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 73.17초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 9/20 in Epoch 5]\n",
            "Game 9: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 9: first_player=False\n",
            "Game 9 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.85초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 9: Policy Loss = 2.0415, Value Loss = 0.0411, Total Loss = 2.0620\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.41초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 5]\n",
            "Game 10: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.46초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 10: Policy Loss = 2.0004, Value Loss = 0.0283, Total Loss = 2.0146\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 60.33초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 5]\n",
            "Game 11: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.76초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 11: Policy Loss = 2.0223, Value Loss = 0.0284, Total Loss = 2.0365\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 60.79초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 5]\n",
            "Game 12: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.57초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 12: Policy Loss = 1.9850, Value Loss = 0.0195, Total Loss = 1.9948\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 56.86초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 5]\n",
            "Game 13: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 13: first_player=False\n",
            "Game 13 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.44초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 13: Policy Loss = 2.0735, Value Loss = 0.0207, Total Loss = 2.0839\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 56.49초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 14/20 in Epoch 5]\n",
            "Game 14: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.45초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 14: Policy Loss = 2.0226, Value Loss = 0.0284, Total Loss = 2.0368\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 63.30초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 5]\n",
            "Game 15: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 8.59초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 15: Policy Loss = 2.0335, Value Loss = 0.0316, Total Loss = 2.0493\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 57.46초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 16/20 in Epoch 5]\n",
            "Game 16: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 16: first_player=False\n",
            "Game 16 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.25초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 16: Policy Loss = 2.0464, Value Loss = 0.0158, Total Loss = 2.0543\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 72.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 5]\n",
            "Game 17: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.86초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 17: Policy Loss = 2.0420, Value Loss = 0.0335, Total Loss = 2.0588\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 81.07초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 5]\n",
            "Game 18: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.60초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 18: Policy Loss = 2.0499, Value Loss = 0.0418, Total Loss = 2.0708\n",
            "Average Point of Latest Model 0.3\n",
            "Evaluation 완료. 소요 시간: 76.10초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 19/20 in Epoch 5]\n",
            "Game 19: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.44초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 19: Policy Loss = 2.0574, Value Loss = 0.0325, Total Loss = 2.0737\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.84초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 5...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_5.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 5]\n",
            "Game 20: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.56초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 20: Policy Loss = 2.0340, Value Loss = 0.0188, Total Loss = 2.0434\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 72.64초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Epoch 6/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 6]\n",
            "Game 1: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.67초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 1: Policy Loss = 2.0140, Value Loss = 0.0431, Total Loss = 2.0355\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 6]\n",
            "Game 2: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.15초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 2: Policy Loss = 1.9928, Value Loss = 0.0197, Total Loss = 2.0027\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.59초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 6]\n",
            "Game 3: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 3: first_player=True\n",
            "Game 3 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.47초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 3: Policy Loss = 1.9942, Value Loss = 0.0247, Total Loss = 2.0066\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 72.42초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 6]\n",
            "Game 4: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.17초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 4: Policy Loss = 2.0168, Value Loss = 0.0135, Total Loss = 2.0236\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.70초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 6]\n",
            "Game 5: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=False\n",
            "Game 5 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.52초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 5: Policy Loss = 2.0011, Value Loss = 0.0213, Total Loss = 2.0118\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.00초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 6]\n",
            "Game 6: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 6: first_player=False\n",
            "Game 6 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.74초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 6: Policy Loss = 2.0093, Value Loss = 0.0367, Total Loss = 2.0276\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.46초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 6]\n",
            "Game 7: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.91초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 7: Policy Loss = 2.0574, Value Loss = 0.0356, Total Loss = 2.0752\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.10초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 6]\n",
            "Game 8: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.38초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 8: Policy Loss = 2.0520, Value Loss = 0.0202, Total Loss = 2.0620\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 67.19초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 6]\n",
            "Game 9: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.21초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 9: Policy Loss = 2.0365, Value Loss = 0.0084, Total Loss = 2.0407\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 65.96초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 6]\n",
            "Game 10: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.09초\n",
            "Training 완료. 소요 시간: 0.38초\n",
            "🔹 Game 10: Policy Loss = 2.0612, Value Loss = 0.0307, Total Loss = 2.0766\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.26초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 6]\n",
            "Game 11: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.78초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0596, Value Loss = 0.0460, Total Loss = 2.0826\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.28초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 6]\n",
            "Game 12: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.08초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 12: Policy Loss = 2.0264, Value Loss = 0.0194, Total Loss = 2.0360\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 70.66초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 13/20 in Epoch 6]\n",
            "Game 13: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 13: first_player=False\n",
            "Game 13 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.03초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 13: Policy Loss = 2.0134, Value Loss = 0.0159, Total Loss = 2.0214\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.87초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 6]\n",
            "Game 14: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.18초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0100, Value Loss = 0.0312, Total Loss = 2.0256\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 68.23초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 6]\n",
            "Game 15: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 15: first_player=False\n",
            "Game 15 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.48초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 15: Policy Loss = 2.0135, Value Loss = 0.0191, Total Loss = 2.0231\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 66.77초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 6]\n",
            "Game 16: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.62초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 16: Policy Loss = 2.0255, Value Loss = 0.0240, Total Loss = 2.0375\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 70.36초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 6]\n",
            "Game 17: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.92초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "🔹 Game 17: Policy Loss = 2.0252, Value Loss = 0.0348, Total Loss = 2.0425\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.16초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 6]\n",
            "Game 18: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 18: first_player=False\n",
            "Game 18 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.16초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 18: Policy Loss = 1.9916, Value Loss = 0.0210, Total Loss = 2.0021\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 66.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 6]\n",
            "Game 19: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.55초\n",
            "Training 완료. 소요 시간: 0.38초\n",
            "🔹 Game 19: Policy Loss = 2.0154, Value Loss = 0.0167, Total Loss = 2.0237\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.50초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 6]\n",
            "Game 20: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 20: first_player=False\n",
            "Game 20 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.66초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 20: Policy Loss = 2.0375, Value Loss = 0.0201, Total Loss = 2.0475\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.08초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 6...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_6.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 7/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 7]\n",
            "Game 1: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 1: first_player=False\n",
            "Game 1 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.85초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 1: Policy Loss = 2.0342, Value Loss = 0.0169, Total Loss = 2.0427\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.25초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 7]\n",
            "Game 2: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.70초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 2: Policy Loss = 2.0543, Value Loss = 0.0369, Total Loss = 2.0727\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.84초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 7]\n",
            "Game 3: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.08초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 3: Policy Loss = 2.0241, Value Loss = 0.0138, Total Loss = 2.0310\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.82초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 7]\n",
            "Game 4: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.39초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 4: Policy Loss = 2.0508, Value Loss = 0.0261, Total Loss = 2.0638\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 72.95초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 7]\n",
            "Game 5: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.86초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 5: Policy Loss = 2.0334, Value Loss = 0.0209, Total Loss = 2.0438\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 73.11초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 6/20 in Epoch 7]\n",
            "Game 6: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.04초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 6: Policy Loss = 2.0552, Value Loss = 0.0320, Total Loss = 2.0712\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.45초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 7]\n",
            "Game 7: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.73초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0637, Value Loss = 0.0263, Total Loss = 2.0768\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.19초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 8/20 in Epoch 7]\n",
            "Game 8: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.11초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 8: Policy Loss = 2.0469, Value Loss = 0.0265, Total Loss = 2.0602\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 70.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 9/20 in Epoch 7]\n",
            "Game 9: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.21초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 9: Policy Loss = 2.0197, Value Loss = 0.0112, Total Loss = 2.0253\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.28초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 7]\n",
            "Game 10: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.31초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 10: Policy Loss = 2.0164, Value Loss = 0.0283, Total Loss = 2.0305\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.74초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 7]\n",
            "Game 11: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.20초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0677, Value Loss = 0.0611, Total Loss = 2.0983\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.82초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 7]\n",
            "Game 12: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=False\n",
            "Game 12 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.84초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 12: Policy Loss = 2.0123, Value Loss = 0.0362, Total Loss = 2.0304\n",
            "Average Point of Latest Model 0.3\n",
            "Evaluation 완료. 소요 시간: 71.39초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 13/20 in Epoch 7]\n",
            "Game 13: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 13: first_player=False\n",
            "Game 13 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.07초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "🔹 Game 13: Policy Loss = 2.0296, Value Loss = 0.0245, Total Loss = 2.0418\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 74.46초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 14/20 in Epoch 7]\n",
            "Game 14: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.20초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0637, Value Loss = 0.0341, Total Loss = 2.0808\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.47초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 7]\n",
            "Game 15: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.84초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 15: Policy Loss = 1.9923, Value Loss = 0.0316, Total Loss = 2.0081\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 71.97초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 16/20 in Epoch 7]\n",
            "Game 16: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.38초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 16: Policy Loss = 2.0463, Value Loss = 0.0140, Total Loss = 2.0533\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 70.31초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 7]\n",
            "Game 17: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.92초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 17: Policy Loss = 2.0258, Value Loss = 0.0235, Total Loss = 2.0376\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.45초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 7]\n",
            "Game 18: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 18: first_player=False\n",
            "Game 18 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.20초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 18: Policy Loss = 1.9985, Value Loss = 0.0178, Total Loss = 2.0074\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 73.14초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 19/20 in Epoch 7]\n",
            "Game 19: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.74초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 19: Policy Loss = 2.0703, Value Loss = 0.0252, Total Loss = 2.0829\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.04초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 7]\n",
            "Game 20: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 20: first_player=False\n",
            "Game 20 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.29초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 20: Policy Loss = 2.0282, Value Loss = 0.0299, Total Loss = 2.0431\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.13초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 7...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_7.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 8/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 8]\n",
            "Game 1: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.40초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 1: Policy Loss = 2.0247, Value Loss = 0.0378, Total Loss = 2.0436\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.35초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 8]\n",
            "Game 2: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.95초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 2: Policy Loss = 2.0003, Value Loss = 0.0220, Total Loss = 2.0113\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.97초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 8]\n",
            "Game 3: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 3: first_player=True\n",
            "Game 3 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.02초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "🔹 Game 3: Policy Loss = 2.0551, Value Loss = 0.0167, Total Loss = 2.0635\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 67.54초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 8]\n",
            "Game 4: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.33초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 4: Policy Loss = 2.0260, Value Loss = 0.0405, Total Loss = 2.0463\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 67.48초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 8]\n",
            "Game 5: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.45초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 5: Policy Loss = 1.9975, Value Loss = 0.0377, Total Loss = 2.0163\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 68.31초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 6/20 in Epoch 8]\n",
            "Game 6: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 6: first_player=False\n",
            "Game 6 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.94초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 6: Policy Loss = 2.0241, Value Loss = 0.0426, Total Loss = 2.0454\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 69.66초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 7/20 in Epoch 8]\n",
            "Game 7: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.20초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0250, Value Loss = 0.0283, Total Loss = 2.0391\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.37초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 8]\n",
            "Game 8: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.57초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 8: Policy Loss = 2.0257, Value Loss = 0.0201, Total Loss = 2.0358\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.63초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 8]\n",
            "Game 9: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.60초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 9: Policy Loss = 2.0347, Value Loss = 0.0264, Total Loss = 2.0479\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.24초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 8]\n",
            "Game 10: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 10: first_player=True\n",
            "Game 10 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.53초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 10: Policy Loss = 2.0182, Value Loss = 0.0124, Total Loss = 2.0244\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 74.42초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 8]\n",
            "Game 11: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.97초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 11: Policy Loss = 1.9970, Value Loss = 0.0251, Total Loss = 2.0095\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.68초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 8]\n",
            "Game 12: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.52초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 12: Policy Loss = 1.9903, Value Loss = 0.0285, Total Loss = 2.0045\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.79초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 8]\n",
            "Game 13: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.72초\n",
            "Training 완료. 소요 시간: 0.98초\n",
            "🔹 Game 13: Policy Loss = 2.0049, Value Loss = 0.0237, Total Loss = 2.0168\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.63초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 8]\n",
            "Game 14: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.29초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 14: Policy Loss = 2.0251, Value Loss = 0.0250, Total Loss = 2.0375\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.37초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 8]\n",
            "Game 15: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 15: first_player=False\n",
            "Game 15 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.00초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 15: Policy Loss = 2.0098, Value Loss = 0.0210, Total Loss = 2.0202\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 8]\n",
            "Game 16: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 16: first_player=False\n",
            "Game 16 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.34초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 16: Policy Loss = 2.0570, Value Loss = 0.0277, Total Loss = 2.0709\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 78.24초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 8]\n",
            "Game 17: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.14초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 17: Policy Loss = 2.0601, Value Loss = 0.0319, Total Loss = 2.0760\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.63초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 8]\n",
            "Game 18: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 18: first_player=False\n",
            "Game 18 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.64초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 18: Policy Loss = 2.0560, Value Loss = 0.0267, Total Loss = 2.0694\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.93초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 8]\n",
            "Game 19: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.92초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 19: Policy Loss = 2.0224, Value Loss = 0.0248, Total Loss = 2.0348\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.43초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 8]\n",
            "Game 20: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.58초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 20: Policy Loss = 2.0358, Value Loss = 0.0233, Total Loss = 2.0475\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.00초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 8...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_8.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 9/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 9]\n",
            "Game 1: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.99초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 1: Policy Loss = 2.0525, Value Loss = 0.0246, Total Loss = 2.0648\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 75.07초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 2/20 in Epoch 9]\n",
            "Game 2: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.62초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 2: Policy Loss = 2.0415, Value Loss = 0.0270, Total Loss = 2.0550\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.96초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 9]\n",
            "Game 3: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.08초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 3: Policy Loss = 2.0280, Value Loss = 0.0266, Total Loss = 2.0413\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 71.33초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 9]\n",
            "Game 4: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.65초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 2.0335, Value Loss = 0.0300, Total Loss = 2.0485\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 74.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 9]\n",
            "Game 5: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.52초\n",
            "Training 완료. 소요 시간: 0.39초\n",
            "🔹 Game 5: Policy Loss = 2.0656, Value Loss = 0.0328, Total Loss = 2.0820\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.90초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 9]\n",
            "Game 6: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.17초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 6: Policy Loss = 2.0473, Value Loss = 0.0265, Total Loss = 2.0605\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 9]\n",
            "Game 7: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.14초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0053, Value Loss = 0.0268, Total Loss = 2.0187\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.70초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 9]\n",
            "Game 8: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.25초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 8: Policy Loss = 2.0322, Value Loss = 0.0219, Total Loss = 2.0431\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.10초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 9]\n",
            "Game 9: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.23초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 9: Policy Loss = 2.0356, Value Loss = 0.0244, Total Loss = 2.0478\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.34초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 9]\n",
            "Game 10: Player 1 starts, first move at 4\n",
            "[DEBUG] Game 10: first_player=True\n",
            "Game 10 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.19초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 10: Policy Loss = 2.0039, Value Loss = 0.0215, Total Loss = 2.0147\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 68.65초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 9]\n",
            "Game 11: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.81초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0377, Value Loss = 0.0273, Total Loss = 2.0514\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.73초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 9]\n",
            "Game 12: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.67초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 12: Policy Loss = 2.0343, Value Loss = 0.0176, Total Loss = 2.0431\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.10초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 9]\n",
            "Game 13: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.28초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 13: Policy Loss = 2.0405, Value Loss = 0.0168, Total Loss = 2.0489\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 66.78초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 9]\n",
            "Game 14: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.81초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 14: Policy Loss = 2.0152, Value Loss = 0.0195, Total Loss = 2.0249\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.51초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 9]\n",
            "Game 15: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.82초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 15: Policy Loss = 2.0273, Value Loss = 0.0297, Total Loss = 2.0422\n",
            "Average Point of Latest Model 0.25\n",
            "Evaluation 완료. 소요 시간: 71.79초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 16/20 in Epoch 9]\n",
            "Game 16: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.42초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 16: Policy Loss = 2.0464, Value Loss = 0.0410, Total Loss = 2.0669\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.35초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 17/20 in Epoch 9]\n",
            "Game 17: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.75초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 17: Policy Loss = 2.0154, Value Loss = 0.0284, Total Loss = 2.0296\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.72초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 9]\n",
            "Game 18: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.09초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 18: Policy Loss = 2.0171, Value Loss = 0.0284, Total Loss = 2.0313\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.31초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 9]\n",
            "Game 19: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.64초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 19: Policy Loss = 2.0349, Value Loss = 0.0265, Total Loss = 2.0482\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 71.14초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 20/20 in Epoch 9]\n",
            "Game 20: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.81초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 20: Policy Loss = 2.0477, Value Loss = 0.0215, Total Loss = 2.0585\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.90초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 9...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_9.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 10/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 10]\n",
            "Game 1: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.64초\n",
            "Training 완료. 소요 시간: 0.51초\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.87초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 10]\n",
            "Game 2: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.10초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 69.80초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 10]\n",
            "Game 3: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.98초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 71.68초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 10]\n",
            "Game 4: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.68초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 68.70초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 10]\n",
            "Game 5: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.78초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.47초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 10]\n",
            "Game 6: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 6: first_player=False\n",
            "Game 6 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.16초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.60초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 10]\n",
            "Game 7: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.48초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.59초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 10]\n",
            "Game 8: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.68초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.71초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 10]\n",
            "Game 9: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.17초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 70.90초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 10/20 in Epoch 10]\n",
            "Game 10: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 10: first_player=True\n",
            "Game 10 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.59초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.34초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 10]\n",
            "Game 11: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.76초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 10]\n",
            "Game 12: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=False\n",
            "Game 12 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.71초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.56초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 10]\n",
            "Game 13: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.62초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.7\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.88초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 10]\n",
            "Game 14: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.62초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.89초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 10]\n",
            "Game 15: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.87초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.26초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 10]\n",
            "Game 16: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.72초\n",
            "Training 완료. 소요 시간: 0.31초\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 75.83초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 10]\n",
            "Game 17: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.85초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 78.48초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 18/20 in Epoch 10]\n",
            "Game 18: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.15초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.92초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 10]\n",
            "Game 19: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.02초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.78초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 10]\n",
            "Game 20: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.38초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.19초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 10...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_10.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 11/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 11]\n",
            "Game 1: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 1: first_player=False\n",
            "Game 1 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.03초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 1: Policy Loss = 2.0622, Value Loss = 0.0223, Total Loss = 2.0734\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.34초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 11]\n",
            "Game 2: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.67초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 2: Policy Loss = 2.0489, Value Loss = 0.0264, Total Loss = 2.0621\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 75.48초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 3/20 in Epoch 11]\n",
            "Game 3: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.08초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 3: Policy Loss = 2.0471, Value Loss = 0.0290, Total Loss = 2.0617\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 75.96초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 11]\n",
            "Game 4: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.25초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 2.0364, Value Loss = 0.0201, Total Loss = 2.0464\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 77.78초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 11]\n",
            "Game 5: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.47초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "🔹 Game 5: Policy Loss = 2.0388, Value Loss = 0.0237, Total Loss = 2.0507\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.96초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 11]\n",
            "Game 6: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.84초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 6: Policy Loss = 2.0254, Value Loss = 0.0235, Total Loss = 2.0371\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 79.59초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 11]\n",
            "Game 7: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.36초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 7: Policy Loss = 2.0252, Value Loss = 0.0202, Total Loss = 2.0353\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.63초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 11]\n",
            "Game 8: Player 1 starts, first move at 3\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.58초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 8: Policy Loss = 2.0445, Value Loss = 0.0232, Total Loss = 2.0561\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.19초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 11]\n",
            "Game 9: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=False\n",
            "Game 9 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.06초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 9: Policy Loss = 2.0276, Value Loss = 0.0308, Total Loss = 2.0430\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 11]\n",
            "Game 10: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.47초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 10: Policy Loss = 2.0580, Value Loss = 0.0260, Total Loss = 2.0710\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.06초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 11]\n",
            "Game 11: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.55초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "🔹 Game 11: Policy Loss = 2.0191, Value Loss = 0.0296, Total Loss = 2.0339\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.99초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 11]\n",
            "Game 12: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 12: first_player=False\n",
            "Game 12 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.47초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 12: Policy Loss = 2.0306, Value Loss = 0.0248, Total Loss = 2.0430\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.13초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 11]\n",
            "Game 13: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.56초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 13: Policy Loss = 2.0453, Value Loss = 0.0225, Total Loss = 2.0566\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.97초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 11]\n",
            "Game 14: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.59초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0420, Value Loss = 0.0154, Total Loss = 2.0497\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.88초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 11]\n",
            "Game 15: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.36초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 15: Policy Loss = 2.0555, Value Loss = 0.0184, Total Loss = 2.0647\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.58초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 16/20 in Epoch 11]\n",
            "Game 16: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 16: first_player=False\n",
            "Game 16 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.90초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 16: Policy Loss = 2.0163, Value Loss = 0.0259, Total Loss = 2.0292\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 70.30초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 11]\n",
            "Game 17: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.91초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 17: Policy Loss = 2.0191, Value Loss = 0.0331, Total Loss = 2.0357\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.65초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 11]\n",
            "Game 18: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.26초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 18: Policy Loss = 2.0442, Value Loss = 0.0274, Total Loss = 2.0579\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.64초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 11]\n",
            "Game 19: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.45초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 19: Policy Loss = 2.0319, Value Loss = 0.0245, Total Loss = 2.0441\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 11...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_11.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 11]\n",
            "Game 20: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.96초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 20: Policy Loss = 2.0460, Value Loss = 0.0296, Total Loss = 2.0608\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 71.75초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Epoch 12/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 12]\n",
            "Game 1: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.90초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 1: Policy Loss = 2.0191, Value Loss = 0.0289, Total Loss = 2.0336\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.78초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 12]\n",
            "Game 2: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.22초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 2: Policy Loss = 2.0124, Value Loss = 0.0174, Total Loss = 2.0211\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 12]\n",
            "Game 3: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.50초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 3: Policy Loss = 2.0448, Value Loss = 0.0362, Total Loss = 2.0628\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 12]\n",
            "Game 4: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.13초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 2.0374, Value Loss = 0.0394, Total Loss = 2.0571\n",
            "Average Point of Latest Model 0.8\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.63초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 12]\n",
            "Game 5: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 5: first_player=False\n",
            "Game 5 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.38초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 5: Policy Loss = 2.0397, Value Loss = 0.0222, Total Loss = 2.0508\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.40초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 6/20 in Epoch 12]\n",
            "Game 6: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 6: first_player=False\n",
            "Game 6 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.14초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 6: Policy Loss = 2.0324, Value Loss = 0.0128, Total Loss = 2.0388\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.22초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 12]\n",
            "Game 7: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.99초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 7: Policy Loss = 2.0074, Value Loss = 0.0201, Total Loss = 2.0174\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.90초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 12]\n",
            "Game 8: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.06초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 8: Policy Loss = 2.0542, Value Loss = 0.0236, Total Loss = 2.0660\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.27초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 9/20 in Epoch 12]\n",
            "Game 9: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=False\n",
            "Game 9 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.93초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 9: Policy Loss = 2.0617, Value Loss = 0.0246, Total Loss = 2.0740\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.85초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 12]\n",
            "Game 10: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 16.16초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 10: Policy Loss = 2.0233, Value Loss = 0.0166, Total Loss = 2.0316\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.75초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 12]\n",
            "Game 11: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.03초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0415, Value Loss = 0.0136, Total Loss = 2.0483\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.97초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 12]\n",
            "Game 12: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 12: first_player=False\n",
            "Game 12 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.62초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 12: Policy Loss = 2.0689, Value Loss = 0.0345, Total Loss = 2.0862\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 72.73초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 13/20 in Epoch 12]\n",
            "Game 13: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 13: first_player=False\n",
            "Game 13 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.66초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 13: Policy Loss = 2.0064, Value Loss = 0.0267, Total Loss = 2.0198\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.59초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 12]\n",
            "Game 14: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.18초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0192, Value Loss = 0.0190, Total Loss = 2.0287\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.42초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 12]\n",
            "Game 15: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.44초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 15: Policy Loss = 2.0280, Value Loss = 0.0156, Total Loss = 2.0358\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.58초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 12]\n",
            "Game 16: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.38초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 16: Policy Loss = 2.0273, Value Loss = 0.0307, Total Loss = 2.0426\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 78.42초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 17/20 in Epoch 12]\n",
            "Game 17: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.94초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 17: Policy Loss = 2.0334, Value Loss = 0.0202, Total Loss = 2.0435\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.78초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 12]\n",
            "Game 18: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.09초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 18: Policy Loss = 2.0486, Value Loss = 0.0386, Total Loss = 2.0679\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 78.45초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 12]\n",
            "Game 19: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.07초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 19: Policy Loss = 2.0393, Value Loss = 0.0312, Total Loss = 2.0549\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.86초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 12]\n",
            "Game 20: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.01초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 20: Policy Loss = 2.0154, Value Loss = 0.0278, Total Loss = 2.0293\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.30초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 12...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_12.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 13/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 13]\n",
            "Game 1: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.27초\n",
            "Training 완료. 소요 시간: 0.43초\n",
            "🔹 Game 1: Policy Loss = 1.9916, Value Loss = 0.0140, Total Loss = 1.9986\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 13]\n",
            "Game 2: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.69초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 2: Policy Loss = 2.0138, Value Loss = 0.0117, Total Loss = 2.0196\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 76.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 3/20 in Epoch 13]\n",
            "Game 3: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.00초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 3: Policy Loss = 2.0355, Value Loss = 0.0288, Total Loss = 2.0499\n",
            "Average Point of Latest Model 0.7\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.18초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 13]\n",
            "Game 4: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.60초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 4: Policy Loss = 2.0524, Value Loss = 0.0285, Total Loss = 2.0666\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.08초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 13]\n",
            "Game 5: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.28초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 5: Policy Loss = 2.0166, Value Loss = 0.0238, Total Loss = 2.0285\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.65초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 13]\n",
            "Game 6: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 6: first_player=False\n",
            "Game 6 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.68초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 6: Policy Loss = 2.0014, Value Loss = 0.0180, Total Loss = 2.0104\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 78.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 7/20 in Epoch 13]\n",
            "Game 7: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.58초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0617, Value Loss = 0.0312, Total Loss = 2.0773\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.11초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 13]\n",
            "Game 8: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.92초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 8: Policy Loss = 2.0581, Value Loss = 0.0165, Total Loss = 2.0664\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 74.93초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 9/20 in Epoch 13]\n",
            "Game 9: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.26초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 9: Policy Loss = 2.0443, Value Loss = 0.0254, Total Loss = 2.0570\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.09초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 13]\n",
            "Game 10: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.21초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 10: Policy Loss = 2.0609, Value Loss = 0.0295, Total Loss = 2.0757\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.26초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 13]\n",
            "Game 11: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.00초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0604, Value Loss = 0.0217, Total Loss = 2.0713\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 75.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 13]\n",
            "Game 12: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.84초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 12: Policy Loss = 1.9897, Value Loss = 0.0168, Total Loss = 1.9981\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 13]\n",
            "Game 13: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.20초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 13: Policy Loss = 2.0611, Value Loss = 0.0133, Total Loss = 2.0677\n",
            "Average Point of Latest Model 0.7\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 13]\n",
            "Game 14: Player 1 starts, first move at 7\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.63초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 14: Policy Loss = 2.0644, Value Loss = 0.0200, Total Loss = 2.0744\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.56초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 13]\n",
            "Game 15: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 15: first_player=False\n",
            "Game 15 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.06초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 15: Policy Loss = 2.0005, Value Loss = 0.0071, Total Loss = 2.0040\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 75.34초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 16/20 in Epoch 13]\n",
            "Game 16: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 16: first_player=True\n",
            "Game 16 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.60초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 16: Policy Loss = 2.0205, Value Loss = 0.0157, Total Loss = 2.0283\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 79.61초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 17/20 in Epoch 13]\n",
            "Game 17: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.85초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 17: Policy Loss = 2.0425, Value Loss = 0.0255, Total Loss = 2.0553\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 75.80초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 18/20 in Epoch 13]\n",
            "Game 18: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 18: first_player=False\n",
            "Game 18 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.54초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 18: Policy Loss = 2.0574, Value Loss = 0.0174, Total Loss = 2.0661\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 78.25초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 13]\n",
            "Game 19: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.92초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 19: Policy Loss = 2.0114, Value Loss = 0.0116, Total Loss = 2.0172\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 78.36초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 13...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_13.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 13]\n",
            "Game 20: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 16.64초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 20: Policy Loss = 2.0405, Value Loss = 0.0190, Total Loss = 2.0500\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 77.08초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Epoch 14/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 14]\n",
            "Game 1: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 1: first_player=False\n",
            "Game 1 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.92초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 1: Policy Loss = 2.0196, Value Loss = 0.0160, Total Loss = 2.0276\n",
            "Average Point of Latest Model 0.3\n",
            "Evaluation 완료. 소요 시간: 72.82초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 2/20 in Epoch 14]\n",
            "Game 2: Player 1 starts, first move at 7\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.20초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 2: Policy Loss = 2.0429, Value Loss = 0.0060, Total Loss = 2.0459\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.84초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 14]\n",
            "Game 3: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.94초\n",
            "Training 완료. 소요 시간: 0.30초\n",
            "🔹 Game 3: Policy Loss = 1.9995, Value Loss = 0.0241, Total Loss = 2.0115\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.47초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 14]\n",
            "Game 4: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.52초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 2.0666, Value Loss = 0.0371, Total Loss = 2.0852\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.03초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 14]\n",
            "Game 5: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.91초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 5: Policy Loss = 2.0073, Value Loss = 0.0325, Total Loss = 2.0236\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 78.28초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 6/20 in Epoch 14]\n",
            "Game 6: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.86초\n",
            "Training 완료. 소요 시간: 0.45초\n",
            "🔹 Game 6: Policy Loss = 1.9958, Value Loss = 0.0302, Total Loss = 2.0109\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.57초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 14]\n",
            "Game 7: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.97초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0203, Value Loss = 0.0212, Total Loss = 2.0309\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 72.57초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 8/20 in Epoch 14]\n",
            "Game 8: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.31초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 8: Policy Loss = 2.0161, Value Loss = 0.0133, Total Loss = 2.0227\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.82초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 14]\n",
            "Game 9: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 9: first_player=True\n",
            "Game 9 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.80초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 9: Policy Loss = 2.0417, Value Loss = 0.0240, Total Loss = 2.0537\n",
            "Average Point of Latest Model 0.7\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 66.72초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 14]\n",
            "Game 10: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 10: first_player=False\n",
            "Game 10 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.56초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 10: Policy Loss = 2.0183, Value Loss = 0.0233, Total Loss = 2.0299\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 73.57초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 14]\n",
            "Game 11: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.08초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 11: Policy Loss = 2.0200, Value Loss = 0.0206, Total Loss = 2.0303\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 68.95초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 14]\n",
            "Game 12: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.12초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 12: Policy Loss = 2.0373, Value Loss = 0.0205, Total Loss = 2.0475\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.00초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 14]\n",
            "Game 13: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.16초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 13: Policy Loss = 2.0299, Value Loss = 0.0169, Total Loss = 2.0384\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.53초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 14]\n",
            "Game 14: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.50초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0019, Value Loss = 0.0145, Total Loss = 2.0092\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 14]\n",
            "Game 15: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 15: first_player=False\n",
            "Game 15 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.28초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 15: Policy Loss = 2.0262, Value Loss = 0.0214, Total Loss = 2.0369\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.58초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 14]\n",
            "Game 16: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 16: first_player=False\n",
            "Game 16 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.69초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 16: Policy Loss = 2.0103, Value Loss = 0.0108, Total Loss = 2.0157\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 71.50초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 14]\n",
            "Game 17: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.35초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 17: Policy Loss = 2.0444, Value Loss = 0.0137, Total Loss = 2.0512\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.83초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 14]\n",
            "Game 18: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 18: first_player=False\n",
            "Game 18 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.32초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 18: Policy Loss = 2.0277, Value Loss = 0.0147, Total Loss = 2.0350\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 71.02초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 14]\n",
            "Game 19: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.90초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 19: Policy Loss = 2.0461, Value Loss = 0.0193, Total Loss = 2.0558\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 73.80초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 20/20 in Epoch 14]\n",
            "Game 20: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 20: first_player=False\n",
            "Game 20 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.33초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 20: Policy Loss = 2.0015, Value Loss = 0.0289, Total Loss = 2.0159\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.08초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 14...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_14.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 15/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 15]\n",
            "Game 1: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.37초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 1: Policy Loss = 2.0432, Value Loss = 0.0208, Total Loss = 2.0536\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 69.45초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 2/20 in Epoch 15]\n",
            "Game 2: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.83초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 2: Policy Loss = 2.0329, Value Loss = 0.0323, Total Loss = 2.0491\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 68.94초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 15]\n",
            "Game 3: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 3: first_player=False\n",
            "Game 3 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.42초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 3: Policy Loss = 2.0383, Value Loss = 0.0169, Total Loss = 2.0468\n",
            "Average Point of Latest Model 0.25\n",
            "Evaluation 완료. 소요 시간: 73.01초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 15]\n",
            "Game 4: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 4: first_player=False\n",
            "Game 4 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.18초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 1.9816, Value Loss = 0.0240, Total Loss = 1.9936\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 15]\n",
            "Game 5: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.89초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 5: Policy Loss = 1.9868, Value Loss = 0.0213, Total Loss = 1.9975\n",
            "Average Point of Latest Model 0.3\n",
            "Evaluation 완료. 소요 시간: 73.65초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 6/20 in Epoch 15]\n",
            "Game 6: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.17초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 6: Policy Loss = 2.0409, Value Loss = 0.0218, Total Loss = 2.0518\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.36초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 7/20 in Epoch 15]\n",
            "Game 7: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 7: first_player=True\n",
            "Game 7 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.04초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 7: Policy Loss = 2.0234, Value Loss = 0.0174, Total Loss = 2.0322\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 76.65초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 8/20 in Epoch 15]\n",
            "Game 8: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.15초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 8: Policy Loss = 2.0193, Value Loss = 0.0318, Total Loss = 2.0352\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.65초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 15]\n",
            "Game 9: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 9: first_player=False\n",
            "Game 9 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.03초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 9: Policy Loss = 2.0442, Value Loss = 0.0291, Total Loss = 2.0587\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.86초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 15]\n",
            "Game 10: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 10: first_player=True\n",
            "Game 10 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.43초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 10: Policy Loss = 2.0176, Value Loss = 0.0103, Total Loss = 2.0227\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.86초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 15]\n",
            "Game 11: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 11: first_player=True\n",
            "Game 11 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.35초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 11: Policy Loss = 2.0303, Value Loss = 0.0244, Total Loss = 2.0425\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 74.97초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 15]\n",
            "Game 12: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=False\n",
            "Game 12 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.65초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 12: Policy Loss = 2.0384, Value Loss = 0.0194, Total Loss = 2.0481\n",
            "Average Point of Latest Model 0.3\n",
            "Evaluation 완료. 소요 시간: 74.29초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 13/20 in Epoch 15]\n",
            "Game 13: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 13: first_player=False\n",
            "Game 13 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.42초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 13: Policy Loss = 2.0239, Value Loss = 0.0207, Total Loss = 2.0342\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.61초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 15]\n",
            "Game 14: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.55초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0014, Value Loss = 0.0103, Total Loss = 2.0065\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.90초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 15/20 in Epoch 15]\n",
            "Game 15: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.65초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 15: Policy Loss = 2.0022, Value Loss = 0.0188, Total Loss = 2.0116\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.20초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 15]\n",
            "Game 16: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 16: first_player=False\n",
            "Game 16 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.09초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 16: Policy Loss = 1.9928, Value Loss = 0.0203, Total Loss = 2.0029\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 77.15초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 17/20 in Epoch 15]\n",
            "Game 17: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 17: first_player=False\n",
            "Game 17 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.83초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 17: Policy Loss = 2.0028, Value Loss = 0.0098, Total Loss = 2.0076\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 18/20 in Epoch 15]\n",
            "Game 18: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.52초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 18: Policy Loss = 2.0518, Value Loss = 0.0159, Total Loss = 2.0597\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 76.44초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 19/20 in Epoch 15]\n",
            "Game 19: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=False\n",
            "Game 19 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.72초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 19: Policy Loss = 2.0494, Value Loss = 0.0302, Total Loss = 2.0645\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 78.02초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 20/20 in Epoch 15]\n",
            "Game 20: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.34초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "🔹 Game 20: Policy Loss = 2.0480, Value Loss = 0.0251, Total Loss = 2.0606\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 15...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_15.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Epoch 16/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 16]\n",
            "Game 1: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 1: first_player=True\n",
            "Game 1 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.42초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 1: Policy Loss = 2.0450, Value Loss = 0.0322, Total Loss = 2.0611\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 75.18초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 2/20 in Epoch 16]\n",
            "Game 2: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 2: first_player=True\n",
            "Game 2 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.38초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 2: Policy Loss = 2.0337, Value Loss = 0.0134, Total Loss = 2.0404\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.67초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 3/20 in Epoch 16]\n",
            "Game 3: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 3: first_player=True\n",
            "Game 3 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.69초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 3: Policy Loss = 2.0496, Value Loss = 0.0253, Total Loss = 2.0622\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.68초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 4/20 in Epoch 16]\n",
            "Game 4: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 8.99초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 4: Policy Loss = 2.0329, Value Loss = 0.0075, Total Loss = 2.0367\n",
            "Average Point of Latest Model 0.65\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.07초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 5/20 in Epoch 16]\n",
            "Game 5: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.22초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 5: Policy Loss = 2.0147, Value Loss = 0.0132, Total Loss = 2.0213\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.91초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 16]\n",
            "Game 6: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.51초\n",
            "Training 완료. 소요 시간: 0.29초\n",
            "🔹 Game 6: Policy Loss = 2.0369, Value Loss = 0.0176, Total Loss = 2.0457\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 75.74초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 7/20 in Epoch 16]\n",
            "Game 7: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.74초\n",
            "Training 완료. 소요 시간: 0.40초\n",
            "🔹 Game 7: Policy Loss = 2.0054, Value Loss = 0.0236, Total Loss = 2.0172\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.56초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 8/20 in Epoch 16]\n",
            "Game 8: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 8: first_player=True\n",
            "Game 8 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.20초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 8: Policy Loss = 2.0495, Value Loss = 0.0337, Total Loss = 2.0663\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 76.27초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 9/20 in Epoch 16]\n",
            "Game 9: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 9: first_player=False\n",
            "Game 9 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.07초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 9: Policy Loss = 2.0428, Value Loss = 0.0183, Total Loss = 2.0519\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.41초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 10/20 in Epoch 16]\n",
            "Game 10: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 10: first_player=True\n",
            "Game 10 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.93초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 10: Policy Loss = 2.0233, Value Loss = 0.0229, Total Loss = 2.0348\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.66초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 11/20 in Epoch 16]\n",
            "Game 11: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.58초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 11: Policy Loss = 2.0365, Value Loss = 0.0349, Total Loss = 2.0540\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 75.03초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 12/20 in Epoch 16]\n",
            "Game 12: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=False\n",
            "Game 12 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 10.08초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 12: Policy Loss = 2.0366, Value Loss = 0.0223, Total Loss = 2.0477\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 76.03초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 13/20 in Epoch 16]\n",
            "Game 13: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.91초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 13: Policy Loss = 2.0085, Value Loss = 0.0241, Total Loss = 2.0206\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 74.26초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 16]\n",
            "Game 14: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 14: first_player=True\n",
            "Game 14 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.48초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 14: Policy Loss = 2.0300, Value Loss = 0.0337, Total Loss = 2.0469\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 72.09초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 15/20 in Epoch 16]\n",
            "Game 15: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 15: first_player=True\n",
            "Game 15 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.17초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 15: Policy Loss = 2.0051, Value Loss = 0.0238, Total Loss = 2.0170\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.02초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 16/20 in Epoch 16]\n",
            "Game 16: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 16: first_player=False\n",
            "Game 16 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.59초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 16: Policy Loss = 2.0274, Value Loss = 0.0219, Total Loss = 2.0383\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 70.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 17/20 in Epoch 16]\n",
            "Game 17: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 17: first_player=True\n",
            "Game 17 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.34초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 17: Policy Loss = 2.0253, Value Loss = 0.0222, Total Loss = 2.0364\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 71.62초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 18/20 in Epoch 16]\n",
            "Game 18: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 18: first_player=True\n",
            "Game 18 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.37초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 18: Policy Loss = 2.0196, Value Loss = 0.0130, Total Loss = 2.0261\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 73.54초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 16...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_16.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 19/20 in Epoch 16]\n",
            "Game 19: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 19: first_player=True\n",
            "Game 19 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.19초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 19: Policy Loss = 2.0331, Value Loss = 0.0216, Total Loss = 2.0439\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 70.55초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 20/20 in Epoch 16]\n",
            "Game 20: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 20: first_player=True\n",
            "Game 20 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.76초\n",
            "Training 완료. 소요 시간: 0.54초\n",
            "🔹 Game 20: Policy Loss = 2.0446, Value Loss = 0.0291, Total Loss = 2.0591\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 77.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Epoch 17/50] --------------------------------\n",
            "\n",
            "[Game 1/20 in Epoch 17]\n",
            "Game 1: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 1: first_player=False\n",
            "Game 1 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.11초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 1: Policy Loss = 2.0268, Value Loss = 0.0273, Total Loss = 2.0405\n",
            "Average Point of Latest Model 0.55\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.69초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 17...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_17.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 2/20 in Epoch 17]\n",
            "Game 2: Player 2 starts, first move at 2\n",
            "[DEBUG] Game 2: first_player=False\n",
            "Game 2 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 14.04초\n",
            "Training 완료. 소요 시간: 0.66초\n",
            "🔹 Game 2: Policy Loss = 2.0337, Value Loss = 0.0158, Total Loss = 2.0417\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 74.60초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 3/20 in Epoch 17]\n",
            "Game 3: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 3: first_player=True\n",
            "Game 3 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.60초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 3: Policy Loss = 2.0627, Value Loss = 0.0204, Total Loss = 2.0729\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 74.78초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 4/20 in Epoch 17]\n",
            "Game 4: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 4: first_player=True\n",
            "Game 4 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.77초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 4: Policy Loss = 2.0308, Value Loss = 0.0127, Total Loss = 2.0372\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 76.79초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 5/20 in Epoch 17]\n",
            "Game 5: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 5: first_player=True\n",
            "Game 5 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 11.78초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 5: Policy Loss = 2.0368, Value Loss = 0.0310, Total Loss = 2.0523\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 77.32초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 17...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_17.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 6/20 in Epoch 17]\n",
            "Game 6: Player 1 starts, first move at 2\n",
            "[DEBUG] Game 6: first_player=True\n",
            "Game 6 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 13.19초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 6: Policy Loss = 2.0317, Value Loss = 0.0202, Total Loss = 2.0418\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 72.22초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 7/20 in Epoch 17]\n",
            "Game 7: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 7: first_player=False\n",
            "Game 7 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.11초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 7: Policy Loss = 2.0575, Value Loss = 0.0319, Total Loss = 2.0734\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 76.21초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 8/20 in Epoch 17]\n",
            "Game 8: Player 2 starts, first move at 6\n",
            "[DEBUG] Game 8: first_player=False\n",
            "Game 8 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.57초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 8: Policy Loss = 2.0354, Value Loss = 0.0180, Total Loss = 2.0444\n",
            "Average Point of Latest Model 0.35\n",
            "Evaluation 완료. 소요 시간: 74.42초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 9/20 in Epoch 17]\n",
            "Game 9: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 9: first_player=False\n",
            "Game 9 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.30초\n",
            "Training 완료. 소요 시간: 0.28초\n",
            "🔹 Game 9: Policy Loss = 2.0437, Value Loss = 0.0286, Total Loss = 2.0580\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 72.96초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 10/20 in Epoch 17]\n",
            "Game 10: Player 1 starts, first move at 5\n",
            "[DEBUG] Game 10: first_player=True\n",
            "Game 10 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 15.65초\n",
            "Training 완료. 소요 시간: 0.27초\n",
            "🔹 Game 10: Policy Loss = 2.0252, Value Loss = 0.0167, Total Loss = 2.0336\n",
            "Average Point of Latest Model 0.4\n",
            "Evaluation 완료. 소요 시간: 77.15초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 11/20 in Epoch 17]\n",
            "Game 11: Player 2 starts, first move at 5\n",
            "[DEBUG] Game 11: first_player=False\n",
            "Game 11 Result: Player 2 Wins (P1: -1, P2: 1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.54초\n",
            "Training 완료. 소요 시간: 0.42초\n",
            "🔹 Game 11: Policy Loss = 2.0213, Value Loss = 0.0312, Total Loss = 2.0369\n",
            "Average Point of Latest Model 0.45\n",
            "Evaluation 완료. 소요 시간: 74.98초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "\n",
            "[Game 12/20 in Epoch 17]\n",
            "Game 12: Player 1 starts, first move at 6\n",
            "[DEBUG] Game 12: first_player=True\n",
            "Game 12 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 9.43초\n",
            "Training 완료. 소요 시간: 0.41초\n",
            "🔹 Game 12: Policy Loss = 2.0270, Value Loss = 0.0286, Total Loss = 2.0413\n",
            "Average Point of Latest Model 0.6\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.53초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 17...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_17.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 13/20 in Epoch 17]\n",
            "Game 13: Player 1 starts, first move at 1\n",
            "[DEBUG] Game 13: first_player=True\n",
            "Game 13 Result: Player 1 Wins (P1: 1, P2: -1)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.81초\n",
            "Training 완료. 소요 시간: 0.44초\n",
            "🔹 Game 13: Policy Loss = 2.0321, Value Loss = 0.0271, Total Loss = 2.0457\n",
            "Average Point of Latest Model 0.5\n",
            "Best Model is Updated.\n",
            "Evaluation 완료. 소요 시간: 72.30초\n",
            "[DEBUG] C_PUCT updated to 1.500\n",
            "[DEBUG] Updated pv_evaluate_count: 1500\n",
            "Saving best model at epoch 17...\n",
            "Model saved to /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_17.pth\n",
            "Model saved to /content/drive/My Drive/3-2/강화/best_model_env_weights.pth\n",
            "Model saved.\n",
            "최신 모델이 최고 모델로 업데이트되었습니다.\n",
            "\n",
            "[Game 14/20 in Epoch 17]\n",
            "Game 14: Player 2 starts, first move at 1\n",
            "[DEBUG] Game 14: first_player=False\n",
            "Game 14 Result: Draw (P1: 0, P2: 0)\n",
            "--------------------\n",
            "Single Self-play 완료. 소요 시간: 12.86초\n",
            "Training 완료. 소요 시간: 0.26초\n",
            "🔹 Game 14: Policy Loss = 2.0331, Value Loss = 0.0199, Total Loss = 2.0430\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-286-0dc5e9d41744>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# 학습 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0malphazero_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-284-b33d149e687a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Model evaluation (한 epoch 단위로 평가)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluation 완료. 소요 시간: {time.time() - start_time:.2f}초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-281-8dad0d48308d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, recent_model)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecent_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-281-8dad0d48308d>\u001b[0m in \u001b[0;36m_evaluate_network\u001b[0;34m(self, recent_model)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mtotal_point\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#홀수 게임: 최신 모델이 후공\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mtotal_point\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0maverage_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_point\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_game_count\u001b[0m \u001b[0;31m#각 게임 결과를 누적하여 평균 점수 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-281-8dad0d48308d>\u001b[0m in \u001b[0;36m_single_play\u001b[0;34m(self, next_actions)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#next_actions[0]: next_actions_recent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m#next_actions[1]: next_actions_best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#선택된 행동에 따라 다음 상태로 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_player_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#첫번째 플레이어의 점수 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-275-14df1d284524>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_actions_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#MCTS 통해 계산된 정책을 기반으로 상태에서 행동을 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;31m#get_policy를 호출하여 현재 상태에서 탐색된 행동 확률 분포를 계산하고 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mlegal_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-275-14df1d284524>\u001b[0m in \u001b[0;36mget_policy\u001b[0;34m(self, state, model, temp)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust_c_puct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mchilds_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_n_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-272-d2c2be04e6b0>\u001b[0m in \u001b[0;36mevaluate_value\u001b[0;34m(self, model, C_PUCT)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# When child node exist 자식 노드가 있는 경우\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_next_child_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_PUCT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_PUCT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m#PUCT 알고리즘을 사용해 최적의 자식 노드 선택 / 반환된 값을 반전\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-272-d2c2be04e6b0>\u001b[0m in \u001b[0;36mevaluate_value\u001b[0;34m(self, model, C_PUCT)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# When child node exist 자식 노드가 있는 경우\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_next_child_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_PUCT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_PUCT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m#PUCT 알고리즘을 사용해 최적의 자식 노드 선택 / 반환된 값을 반전\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-272-d2c2be04e6b0>\u001b[0m in \u001b[0;36mevaluate_value\u001b[0;34m(self, model, C_PUCT)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# When child node exist 자식 노드가 있는 경우\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_next_child_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_PUCT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_PUCT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m#PUCT 알고리즘을 사용해 최적의 자식 노드 선택 / 반환된 값을 반전\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-272-d2c2be04e6b0>\u001b[0m in \u001b[0;36mevaluate_value\u001b[0;34m(self, model, C_PUCT)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# When child node does not exist 자식 노드가 없는 경우\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# by using nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m#residual 네트워크를 호출하여 정책 및 가치 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-274-1d8ba929ea05>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, state)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#모델 평가 모드로 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataParallel.forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-270-e988b723856d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#conv -> residual blocks -> global pooling을 거쳐 1D로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-269-abc85e92c13b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# double check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 드롭아웃 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    import torch\n",
        "\n",
        "    # 필요한 상수 정의\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    N_KERNEL = 64\n",
        "    N_RESIDUAL_BLOCK = 16\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50  # 학습 에포크\n",
        "    LEARNING_RATE = 0.001\n",
        "    LEARN_EPOCH = 100\n",
        "\n",
        "    SP_GAME_COUNT = 20\n",
        "    SP_TEMPERATURE = 1.5 # 볼츠만 분포의 온도 파라미터\n",
        "\n",
        "    EVAL_COUNT = 100\n",
        "    PV_EVALUATE_COUNT = 800\n",
        "    EVAL_GAME_COUNT = 20\n",
        "    EVAL_TEMPERATURE = 1.0\n",
        "\n",
        "    # 이전 학습 가중치를 사용하지 않고 새 모델로 초기화\n",
        "    model_path = None  # None으로 설정하거나 관련 인수를 제거\n",
        "\n",
        "    # Environment 객체 생성\n",
        "    env = Environment()\n",
        "\n",
        "    # AlphaZeroAgent 초기화\n",
        "    alphazero_agent = AlphaZeroAgent(\n",
        "        model_path=None,  # 이전 가중치를 사용하지 않음\n",
        "        sp_game_count=SP_GAME_COUNT,\n",
        "        eval_game_count=EVAL_GAME_COUNT,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        learn_epoch=LEARN_EPOCH,\n",
        "        pv_evaluate_count=PV_EVALUATE_COUNT,  # MCTS 탐색 깊이 증가\n",
        "        env=env\n",
        "    )\n",
        "\n",
        "    # Self-play 실행을 명시적으로 제어\n",
        "    alphazero_agent.selfplay(execute_self_play=True)  # Self-play 비활성화\n",
        "\n",
        "    # 학습 실행\n",
        "    alphazero_agent.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4svRcB_eP4Lj"
      },
      "source": [
        "## Test Code\n",
        "### 알파제로 모델과 사람이 틱택토 게임을 플레이하는 인터페이스를 제공\n",
        "#### 모델 -> MCTS를 사용하여 행동을 선택\n",
        "#### 사람 -> 키보드 입력으로 행동을 선택"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YGB0_o9v_Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MP4pY8x9wDpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSFwDHOVbsh-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class ModelvsHuman: #알파제로와 사람이 대결하는 코드\n",
        "    def __init__(self, model):\n",
        "        self.model = model #알파제로 모델 저장\n",
        "        self.mcts = MCTS(EVAL_COUNT) #MCTS 객체 초기화하여 모델 기반의 행동 생성\n",
        "        self.next_actions = self.mcts.get_actions_of(model, EVAL_TEMPERATURE)\n",
        "        #MCTS가 생성한 행동 확률 분포를 사용하여 행동을 선택\n",
        "\n",
        "\n",
        "    def render(self, state): #게임 현재 상태를 사람이 이해할 수 있는 형태로 변환하여 출력\n",
        "        is_first_player = state.check_first_player()\n",
        "        board = (\n",
        "            state.state - state.enemy_state\n",
        "            if is_first_player\n",
        "            else state.enemy_state - state.state\n",
        "        )\n",
        "        #첫번째 플레이어와 두번째 플레이어의 보드를 합산하여 현재 보드 상태를 표현\n",
        "        #각 플레이어의 돌을 구별하기 위해 값을 1 (첫번째 플레이어)와 -1 (두번째 플레이어)로 변환\n",
        "        #첫번째 플레이어 차례일 때: state.board[0] + (-1 * state.board[1])\n",
        "        #board = [[ 1, -1,  0],\n",
        "        #         [ 0,  1,  0],\n",
        "        #         [ 0,  0, -1]]\n",
        "        #두번째 플레이어 차례일 때: state.board[1] + (-1 * state.board[0])\n",
        "        #board = [[-1,  1,  0],\n",
        "        #         [ 0, -1,  0],\n",
        "        #         [ 0,  0,  1]]\n",
        "        def pattern(x): #숫자 상태 (1,0,-1)을 'O','공백','X'로 변환\n",
        "            if x == 1:\n",
        "                return 'O' #플레이어 말 1 (첫번째 플레이어) -> 'O'\n",
        "            elif x == 0:\n",
        "                return ' ' #플레이어 말 0 -> '공백'\n",
        "            else:\n",
        "                return 'X' #플레이어 말 -1 (두번째 플레이어) -> 'X'\n",
        "\n",
        "        board_list = list(map(pattern, board.reshape(-1)))\n",
        "        # Fix: Convert sub-lists to strings before joining\n",
        "        board_string = '\\n'.join([' | '.join(board_list[i:i + 3]) for i in range(0, 9, 3)])\n",
        "        print(board_string)\n",
        "\n",
        "\n",
        "    def vs_human(self, with_policy=True): #알파제로 모델과 인간 간의 게임 진행\n",
        "        \"\"\"\n",
        "        사람과 AlphaZero가 대국하며, 사람이 후공을 선택할 경우 직접 입력할 수 있도록 변경.\n",
        "        \"\"\"\n",
        "        # 선공 / 후공 선택\n",
        "        player_choice = input(\"선공(O)으로 플레이하려면 '1', 후공(X)으로 플레이하려면 '2'를 입력하세요: \").strip()\n",
        "        while player_choice not in [\"1\", \"2\"]:\n",
        "            player_choice = input(\"잘못된 입력입니다. '1'(선공) 또는 '2'(후공)을 입력하세요: \").strip()\n",
        "\n",
        "        human_first = player_choice == \"1\"  # 사람이 선공인지 여부\n",
        "        state = State()\n",
        "        self.render(state)\n",
        "\n",
        "        while not state.check_done()[0]:\n",
        "           if state.check_first_player() == human_first:  # 사람이 둘 차례\n",
        "                 legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "                 my_action = int(input(\"Choose Your Action (0-8): \"))\n",
        "                 while my_action not in legal_actions:\n",
        "                     print(f\"Invalid action. Choose from {legal_actions}\")\n",
        "                     my_action = int(input(\"Choose Your Action (0-8): \"))\n",
        "                 state = state.next(my_action)\n",
        "                 print(f\"Human ({'O' if human_first else 'X'}) chose: {my_action}\")\n",
        "           else:  # 알파제로가 둘 차례\n",
        "                 action = self.next_actions(state)\n",
        "                 state = state.next(action)\n",
        "                 print(f\"AlphaZero ({'X' if human_first else 'O'}) chose: {action}\")\n",
        "\n",
        "           self.render(state)  # 현재 보드 상태 출력\n",
        "\n",
        "        print(\"Game Over!\")\n",
        "\n",
        "\n",
        "    def __call__(self, with_policy=True): #기본 설정으로 알파제로 정책을 시각화하게 시킴\n",
        "        self.vs_human(with_policy)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class ModelvsHuman:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.mcts = MCTS(EVAL_COUNT)\n",
        "        self.next_actions = self.mcts.get_actions_of(model, EVAL_TEMPERATURE)\n",
        "\n",
        "    def render(self, state):\n",
        "        board = state.state - state.enemy_state if state.check_first_player() else state.enemy_state - state.state\n",
        "        board_list = [['O' if cell == 1 else 'X' if cell == -1 else ' ' for cell in row] for row in board]\n",
        "        formatted_board = \"\\n\".join([\" | \".join(row) for row in board_list])\n",
        "        print(formatted_board)\n",
        "\n",
        "    def vs_human(self):\n",
        "        \"\"\"\n",
        "        AlphaZero와 사람이 번갈아가며 정확히 교대하며 두도록 수정된 코드.\n",
        "        \"\"\"\n",
        "        # 선공/후공 선택\n",
        "        player_choice = input(\"선공(O)으로 플레이하려면 '1', 후공(X)으로 플레이하려면 '2'를 입력하세요: \").strip()\n",
        "        while player_choice not in [\"1\", \"2\"]:\n",
        "            player_choice = input(\"잘못된 입력입니다. '1'(선공) 또는 '2'(후공)을 입력하세요: \").strip()\n",
        "\n",
        "        human_first = player_choice == \"1\"\n",
        "        state = State()\n",
        "        self.render(state)\n",
        "\n",
        "        while not state.check_done()[0]:  # 게임이 종료되지 않았을 때\n",
        "            if state.check_first_player() == human_first:  # 사람이 둘 차례\n",
        "                legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "                my_action = input(f\"Choose Your Action {tuple(legal_actions)}: \").strip()\n",
        "\n",
        "                # 유효한 입력값만 허용\n",
        "                while not my_action.isdigit() or int(my_action) not in legal_actions:\n",
        "                    my_action = input(f\"잘못된 입력입니다. {tuple(legal_actions)} 중에서 선택하세요: \").strip()\n",
        "\n",
        "                my_action = int(my_action)\n",
        "                state = state.next(my_action)\n",
        "                print(f\"Human ({'O' if human_first else 'X'}) chose: {my_action}\")\n",
        "\n",
        "            else:  # AlphaZero 차례\n",
        "                action = self.next_actions(state)\n",
        "                state = state.next(action)\n",
        "                print(f\"AlphaZero ({'X' if human_first else 'O'}) chose: {action}\")\n",
        "\n",
        "            self.render(state)  # 현재 보드 상태 출력\n",
        "\n",
        "        print(\"Game Over!\")  # 게임 종료 메시지 출력\n",
        "\n",
        "    def __call__(self):\n",
        "        self.vs_human()\n",
        "'''"
      ],
      "metadata": {
        "id": "i6hOGcmzsnkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelvsHuman:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.mcts = MCTS(EVAL_COUNT)\n",
        "        self.get_action = self.mcts.get_actions_of(model, EVAL_TEMPERATURE)\n",
        "\n",
        "    def render(self, state):\n",
        "        \"\"\"현재 게임 상태를 보기 좋게 출력\"\"\"\n",
        "        board = state.state - state.enemy_state if state.check_first_player() else state.enemy_state - state.state\n",
        "        board_list = [['O' if cell == 1 else 'X' if cell == -1 else ' ' for cell in row] for row in board]\n",
        "        formatted_board = \"\\n\".join([\" | \".join(row) for row in board_list])\n",
        "        print(formatted_board)\n",
        "        print(\"-\" * 11)  # 구분선 추가\n",
        "\n",
        "    def vs_human(self):\n",
        "        \"\"\"AlphaZero와 사람이 정확히 번갈아가며 플레이하도록 수정\"\"\"\n",
        "        # ✅ 선공/후공 선택\n",
        "        player_choice = input(\"선공(O)으로 플레이하려면 '1', 후공(X)으로 플레이하려면 '2'를 입력하세요: \").strip()\n",
        "        while player_choice not in [\"1\", \"2\"]:\n",
        "            player_choice = input(\"잘못된 입력입니다. '1'(선공) 또는 '2'(후공)을 입력하세요: \").strip()\n",
        "\n",
        "        human_first = player_choice == \"1\"\n",
        "        state = State()\n",
        "        self.render(state)\n",
        "\n",
        "        while not state.check_done()[0]:  # 게임이 종료되지 않았을 때\n",
        "            if state.check_first_player() == human_first:  # 사람이 둘 차례\n",
        "                legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "                my_action = input(f\"Choose Your Action {tuple(legal_actions)}: \").strip()\n",
        "\n",
        "                # ✅ 유효한 입력값만 허용\n",
        "                while not my_action.isdigit() or int(my_action) not in legal_actions:\n",
        "                    my_action = input(f\"잘못된 입력입니다. {tuple(legal_actions)} 중에서 선택하세요: \").strip()\n",
        "\n",
        "                my_action = int(my_action)\n",
        "                state = state.next(my_action)\n",
        "                print(f\"Human ({'O' if human_first else 'X'}) chose: {my_action}\")\n",
        "\n",
        "            else:  # ✅ AlphaZero 차례\n",
        "                action = self.get_action(state)\n",
        "                state = state.next(action)\n",
        "                print(f\"AlphaZero ({'X' if human_first else 'O'}) chose: {action}\")\n",
        "\n",
        "            # ✅ 턴이 바뀔 때마다 보드 상태를 한 번만 출력\n",
        "            self.render(state)\n",
        "\n",
        "        print(\"Game Over!\")  # 게임 종료 메시지 출력\n",
        "\n",
        "    def __call__(self):\n",
        "        self.vs_human()"
      ],
      "metadata": {
        "id": "u3SxMTZ4Q_p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMYFrBIIaJlT"
      },
      "outputs": [],
      "source": [
        "#정책 시각화와 현재 게임 상태를 그래프 형태로 표현\n",
        "def visualize_history(history): #정책 값의 크기를 강조하고, 가장 높은 확률의 행동을 표시\n",
        "\n",
        "    state, policy, _ = history #self-play 과정에서 기록된 하나의 상태 기록\n",
        "    policy = np.array(policy).reshape(3,3) #알페제로가 생성한 확률 분포 -> 이를 3X3으로 변환!\n",
        "\n",
        "    # Obtain is_first_player from the state object\n",
        "    is_first_player = state.check_first_player() # state에서 is_first_player 정보를 얻어옵니다.\n",
        "\n",
        "\n",
        "    def make_coord(state): #좌표 생성 함수 / 돌이 놓인 위치를 좌표로 변환하는 역할\n",
        "        mask = state.astype(bool).reshape(-1)  # 불리언 마스크 생성\n",
        "        indices = list(np.arange(len(mask))[mask])  # True인 인덱스 추출\n",
        "        #state에서 돌이 놓인 위치를 True로 변환 후, 해당 좌표를 가져옴\n",
        "\n",
        "        # 배열의 열 크기 (state는 2D 배열로 가정)\n",
        "        width = state.shape[1]\n",
        "\n",
        "        # divmod로 몫과 나머지를 계산\n",
        "        if len(indices) != 0: #돌이 놓인 위치가 있을 경우\n",
        "            quotients, remainders = zip(*[divmod(index, width) for index in indices])\n",
        "            #divmod: 1D 인덱스를 2D 좌표 (row, col)로 변환\n",
        "            coords = [quotients, remainders]\n",
        "\n",
        "        else: #돌이 놓인 위치가 아닐 경우\n",
        "            coords = None\n",
        "\n",
        "        return coords\n",
        "\n",
        "\n",
        "    # Plotting 현재 상태 시각화\n",
        "    fig, ax = plt.subplots()\n",
        "    cax = ax.matshow(policy, cmap='Greens', alpha=0.7)\n",
        "    #policy 배열을 색상으로 표현 / 정책 확률 값의 크기를 초록색 음영으로 표현 / 색상 투명도\n",
        "\n",
        "\n",
        "    # 플레이어의 차례 인식\n",
        "    if is_first_player:  #첫번째 플레이어 차례 (즉, 알파제로)\n",
        "        #각 플레이어의 돌 좌표 계산\n",
        "        o_coords = make_coord(state.state) #state[0]: 첫번째 플레이어의 돌 좌표\n",
        "        x_coords = make_coord(state.enemy_state) #state[1]: 두번째 플레이어의 돌 좌표\n",
        "\n",
        "        #돌을 시각적으로 표시\n",
        "        if o_coords is not None: #첫번째 플레이어 (O)의 돌이 보드에 있는 경우\n",
        "            plt.scatter(o_coords[1], o_coords[0], marker=\"o\", s=2000, c='black')  #큰 원 (검정색)\n",
        "            plt.scatter(o_coords[1], o_coords[0], marker=\"o\", s=1500, c='#F2F2F2') #작은 원 (흰색 내부)\n",
        "            #플레이어의 돌을 시각적으로 표시\n",
        "            #돌이 없을 때 불필요한 시각화 작업을 방지\n",
        "\n",
        "        if x_coords is not None: #두번째 플레이어 (X)의 돌이 보드에 있는 경우\n",
        "            plt.scatter(x_coords[1], x_coords[0], marker=\"x\", s=2000, c='black', linewidths=3.0)\n",
        "\n",
        "\n",
        "    else: #두번째 플레이어 차례 (즉, 인간)\n",
        "        o_coords = make_coord(state.enemy_state) #state[1]: 두번째 플레이어의 돌 좌표\n",
        "        x_coords = make_coord(state.state) #state[0]: 첫번째 플레이어의 돌 좌표\n",
        "\n",
        "        #돌을 시각적으로 표시\n",
        "        if o_coords is not None: #두번째 플레이어 (O)의 돌이 보드에 있는 경우\n",
        "            plt.scatter(o_coords[1], o_coords[0], marker=\"o\", s=2000, c='black')  # x, y 좌표로 매핑\n",
        "            plt.scatter(o_coords[1], o_coords[0], marker=\"o\", s=1500, c='#F2F2F2')\n",
        "\n",
        "        if x_coords is not None:  #첫번째 플레이어 (X)의 돌이 보드에 있는 경우\n",
        "            plt.scatter(x_coords[1], x_coords[0], marker=\"x\", s=2000, c='black', linewidths=3.0)\n",
        "\n",
        "\n",
        "    max_coord = divmod(np.argmax(policy), policy.shape[1]) #가장 높은 확률의 행동 강조\n",
        "    #np.argmax(policy): 가장 높으 확률의 인덱스 추출\n",
        "    #divmod: 1D 인덱스를 2D 좌표 (row, col)로 변환\n",
        "\n",
        "\n",
        "    if is_first_player: #첫번째 플레이어 차례\n",
        "        plt.scatter(max_coord[1], max_coord[0], marker=\"o\", s=2000, c='red', linewidths=3.0) # #FF19A3\n",
        "        #정책에서 가장 높은 확률의 행동을 빨간색으로 강조 -> 알파제로 차례일 때\n",
        "\n",
        "    else: #두번째 플레이어 차례\n",
        "        plt.scatter(max_coord[1], max_coord[0], marker=\"x\", s=2000, c='red', linewidths=3.0) # #FF19A3\n",
        "        #정책에서 가장 높은 확률의 행동을 빨간색으로 강조 -> 인간 차례일 때\n",
        "\n",
        "\n",
        "    # Add text annotations (optional) / 정책 값 주석 추가\n",
        "    for (i, j), value in np.ndenumerate(policy): #정책 배열의 각 위치와 값을 반환\n",
        "        if value != 0.0:\n",
        "            ax.text(j, i, f\"{value:.2f}\", ha='center', va='center', fontsize='15',\n",
        "                    fontweight='bold' if value == np.max(policy) else \"normal\",\n",
        "                    color=\"white\" if value == np.max(policy) else \"black\")\n",
        "\n",
        "\n",
        "    # Colorbar and titles / 색상 바와 타이틀 추가\n",
        "    plt.colorbar(cax, ax=ax)\n",
        "    ax.set_title(\"Current State, Policy with Highlighted Highest Action\")\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfL9uygTpFAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8919b7-80c0-4cae-82d3-7ede4dbe1a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 가중치 로드 성공!\n"
          ]
        }
      ],
      "source": [
        "#1 모델 초기화 및 가중치 로드\n",
        "model = Network(N_RESIDUAL_BLOCK, N_KERNEL, STATE_SIZE, N_ACTIONS)\n",
        "model = nn.DataParallel(model).to(device)  # DataParallel로 병렬화\n",
        "\n",
        "# self_play 객체 독립적으로 초기화\n",
        "self_play = SelfPlay(model, SP_TEMPERATURE, SP_GAME_COUNT, EVAL_COUNT)\n",
        "\n",
        "params = torch.load(\"/content/drive/MyDrive/3-2/강화/best_model_env_weights.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
        "\n",
        "# Check if the checkpoint was saved using DataParallel by checking if keys have the 'module.' prefix\n",
        "#The error occurs when the model is initialized directly with the saved weights without accounting for the 'module.' prefix if the model was saved using DataParallel.\n",
        "#To fix this, you should modify the loading logic to add the 'module.' prefix to the keys of the saved checkpoint\n",
        "if any(k.startswith('module.') for k in params.keys()):\n",
        "    # If saved with DataParallel, load directly\n",
        "    model.load_state_dict(params)\n",
        "else:\n",
        "    # If not saved with DataParallel, use the modified state_dict with prefix\n",
        "    state_dict = {f\"module.{k}\": v for k, v in params.items()}  # Add 'module.' prefix\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "print(\"모델 가중치 로드 성공!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0rpDAHQLPf"
      },
      "source": [
        "사용자가 직접 숫자를 입력하면 현재 틱택토 게임의 보드 상태가 그림으로 나타난다.\n",
        "\n",
        "- 플레이어 1 (알파제로 에이전트)의 말은 O이고, 플레이어 2 (인간)의 말은 X로 표현된다!\n",
        "\n",
        "- 행동 확률에 따라 알파제로가 선택한 칸이 빨간색으로 강조되기도 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEG6LuT5cZeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5350470c-b899-43ec-f5cd-dae330a6c69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "선공(O)으로 플레이하려면 '1', 후공(X)으로 플레이하려면 '2'를 입력하세요: 1\n",
            "  |   |  \n",
            "  |   |  \n",
            "  |   |  \n",
            "-----------\n",
            "Choose Your Action (0, 1, 2, 3, 4, 5, 6, 7, 8): 4\n",
            "Human (O) chose: 4\n",
            "  |   |  \n",
            "  | O |  \n",
            "  |   |  \n",
            "-----------\n",
            "AlphaZero (X) chose: 1\n",
            "  | X |  \n",
            "  | O |  \n",
            "  |   |  \n",
            "-----------\n",
            "Choose Your Action (0, 2, 3, 5, 6, 7, 8): 0\n",
            "Human (O) chose: 0\n",
            "O | X |  \n",
            "  | O |  \n",
            "  |   |  \n",
            "-----------\n",
            "AlphaZero (X) chose: 3\n",
            "O | X |  \n",
            "X | O |  \n",
            "  |   |  \n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "#2. 모델 대 인간 대결\n",
        "# vs_human = ModelvsHuman(eval_network.best_model)\n",
        "\n",
        "vs_human = ModelvsHuman(model) #객체를 초기화\n",
        "vs_human() #__call__ 매서드를 통해 게임을 시작"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNF7nt9cSYyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOB19bGUd2jI"
      },
      "outputs": [],
      "source": [
        "#2. self-play 기록 시각화\n",
        "for idx in range(len(self_play.history)):\n",
        "  visualize_history(self_play.history[idx])\n",
        "#self_play.history: self-play에서 생성된 상태, 정책, 가치를 기록한 리스트\n",
        "#self_play.history의 길이를 기반으로 인덱스를 생성하는 이터레이터 생성\n",
        "#루프를 사용하여 기록 반복:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9k0XSodSURB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hf3wEB3LvWX"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olb7xTXcLwr1"
      },
      "source": [
        "## 손실 (loss) 변화 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUetHP0VK68c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(loss_file=\"my_train_losses.csv\"):\n",
        "    if not os.path.exists(loss_file):\n",
        "        print(f\"{loss_file} 파일이 없습니다. 학습을 먼저 실행하세요.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(loss_file)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(df[\"epoch\"], df[\"p_loss\"], label=\"Policy Loss\", color=\"blue\")\n",
        "    plt.plot(df[\"epoch\"], df[\"v_loss\"], label=\"Value Loss\", color=\"red\")\n",
        "    plt.plot(df[\"epoch\"], df[\"total_loss\"], label=\"Total Loss\", color=\"green\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# 손실 그래프 출력\n",
        "plot_losses()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}