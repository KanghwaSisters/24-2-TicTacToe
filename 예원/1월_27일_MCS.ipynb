{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "060215d2-7b75-4240-9013-eb91c75f9b5b",
      "metadata": {
        "id": "060215d2-7b75-4240-9013-eb91c75f9b5b"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clFYMYS9eFai",
        "outputId": "468ee4a7-a62d-4033-ffaf-f9e82ba5c206"
      },
      "id": "clFYMYS9eFai",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1999e8fc-6414-4d9b-9aab-baccb5e9e394",
      "metadata": {
        "id": "1999e8fc-6414-4d9b-9aab-baccb5e9e394"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb90d19-1adb-4a6c-a863-3efdee71eb91",
      "metadata": {
        "id": "ffb90d19-1adb-4a6c-a863-3efdee71eb91"
      },
      "source": [
        "# 00 Game Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a747d193-2a5d-402f-91df-945a5e38dd01",
      "metadata": {
        "id": "a747d193-2a5d-402f-91df-945a5e38dd01"
      },
      "outputs": [],
      "source": [
        "STATE_SIZE = (3,3) #틱택토 보드 크기\n",
        "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
        "STATE_DIM = 3 # first player 정보 넣음\n",
        "BOARD_SHAPE = (STATE_DIM, 3, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef1969c-cb98-45a5-97a6-dfa5e54a0d48",
      "metadata": {
        "id": "3ef1969c-cb98-45a5-97a6-dfa5e54a0d48"
      },
      "source": [
        "# 01 HYPER PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6099b13b-076f-4dee-bac1-bf8efb5e7e1c",
      "metadata": {
        "id": "6099b13b-076f-4dee-bac1-bf8efb5e7e1c",
        "outputId": "397b87c8-067a-4fe8-b4dd-0170280261de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcfb6d0-ef59-403f-a07c-23c2e6b8985f",
      "metadata": {
        "id": "8dcfb6d0-ef59-403f-a07c-23c2e6b8985f"
      },
      "source": [
        "# 02 Env+State"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 파일 복사 명령어 실행\n",
        "os.system('cp \"/content/drive/My Drive/Colab Notebooks/공통 environment+state.ipynb\" \"/content/\"')"
      ],
      "metadata": {
        "id": "Z61XvMOPedS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c0ac0e-ae21-40f7-8f88-0358c95cff91"
      },
      "id": "Z61XvMOPedS2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "notebook_path = \"/content/공통 environment+state.ipynb\"\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    notebook_content = nbformat.read(f, as_version=4)\n",
        "\n",
        "# 각 코드 셀 출력 및 실행\n",
        "for cell in notebook_content.cells:\n",
        "    if cell.cell_type == \"code\":\n",
        "        print(f\"실행 중인 코드:\\n{cell.source}\\n{'='*40}\")\n",
        "        exec(cell.source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbr0cbD0edF2",
        "outputId": "75a511ba-9479-4dcd-a04b-5fdd0a58f03d"
      },
      "id": "Rbr0cbD0edF2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc1cecc-9934-4d03-81ea-b7b80b938d46",
      "metadata": {
        "id": "4bc1cecc-9934-4d03-81ea-b7b80b938d46"
      },
      "source": [
        "# 03 MCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3abfc6-40e0-4a6c-9474-cdd1be6c691e",
      "metadata": {
        "id": "0c3abfc6-40e0-4a6c-9474-cdd1be6c691e"
      },
      "outputs": [],
      "source": [
        "class MCSAgent: # 몬테카를로 탐색 알고리즘 클래스\n",
        "    def __init__(self, n_simulations=1000):\n",
        "        self.n_simulations = n_simulations\n",
        "\n",
        "    # 시뮬레이션 실행\n",
        "    # 주어진 상태와 행동에서 시뮬레이션을 실행하고 결과를 반환\n",
        "    def run_simulation(self, state, action):\n",
        "        current_state = state.next(action)  # 행동 수행 후의 상태\n",
        "\n",
        "        while True:\n",
        "            is_done, _ = current_state.check_done()\n",
        "            if is_done:\n",
        "                break  # 게임이 종료되면 루프 탈출\n",
        "\n",
        "            # 가능한 행동 중 무작위로 선택하여 진행\n",
        "            legal_actions = np.where(current_state.get_legal_actions() != 0)[0]\n",
        "            if len(legal_actions) == 0:\n",
        "                break  # 더 이상 가능한 행동이 없으면 종료\n",
        "\n",
        "            # 무작위 행동 선택 (순수한 MCS)\n",
        "            action = np.random.choice(legal_actions)\n",
        "            current_state = current_state.next(action)\n",
        "\n",
        "        # 게임이 종료되면 보상을 반환\n",
        "        return current_state.get_reward(current_state)\n",
        "\n",
        "\n",
        "    # 각 행동에 대한 시뮬레이션 수행\n",
        "    def evaluate_actions(self, state):\n",
        "        legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "        action_scores = {action: [] for action in legal_actions}\n",
        "\n",
        "        for action in legal_actions:  # 각 가능한 행동에 대해 시뮬레이션 수행\n",
        "            for _ in range(self.n_simulations):\n",
        "                result = self.run_simulation(state, action)\n",
        "                if result is not None:\n",
        "                    action_scores[action].append(result)\n",
        "\n",
        "        # 각 행동에 대한 평균 보상 계산 / 값이 없을 경우 기본값 0 설정\n",
        "        action_means = {\n",
        "            action: np.mean(scores) if len(scores) > 0 else 0\n",
        "            for action, scores in action_scores.items()\n",
        "        }\n",
        "        return action_means\n",
        "\n",
        "\n",
        "    def get_action(self, state): #시뮬레이션 결과를 기반으로 최적의 행동 반환\n",
        "        action_means = self.evaluate_actions(state)\n",
        "\n",
        "        # 예외 처리: 가능한 행동이 없는 경우 none 반환 방지\n",
        "        if not action_means:\n",
        "           legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "           if len(legal_actions) > 0:\n",
        "               print(\"[WARNING] No optimal actions found, choosing a random action.\")\n",
        "               return np.random.choice(legal_actions)  # 가능한 행동 중 랜덤 선택\n",
        "           else:\n",
        "               print(\"[ERROR] No valid actions available. Game might be in an invalid state.\")\n",
        "               return None  # 안전하게 None 반환\n",
        "\n",
        "\n",
        "        # 평균 점수가 가장 높은 행동을 찾음 / 동점이면 랜덤\n",
        "        max_value = max(action_means.values())\n",
        "        best_actions = [action for action, value in action_means.items() if value == max_value]  # 동점일 경우 랜덤 선택\n",
        "        return np.random.choice(best_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578224a6-84c6-4d1b-bd86-36a8c858c7ad",
      "metadata": {
        "id": "578224a6-84c6-4d1b-bd86-36a8c858c7ad"
      },
      "source": [
        "## Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23794481-24e2-4ff4-b8fb-09716a1dd8e3",
      "metadata": {
        "id": "23794481-24e2-4ff4-b8fb-09716a1dd8e3",
        "outputId": "db3deef4-55c6-4a0a-c967-7d25193105b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "초기 상태 보드:\n",
            "가능한 행동들: [0 1 2 3 4 5 6 7 8]\n"
          ]
        }
      ],
      "source": [
        "# 1. State 초기화\n",
        "state = State()\n",
        "print(\"초기 상태 보드:\")\n",
        "state.render(state)\n",
        "print(\"가능한 행동들:\", np.where(state.get_legal_actions() != 0)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26879c3-ec53-4adf-ba22-2cacf3150c3d",
      "metadata": {
        "id": "e26879c3-ec53-4adf-ba22-2cacf3150c3d"
      },
      "outputs": [],
      "source": [
        "# 2. MonteCarloSearch 초기화\n",
        "monte_carlo = MCSAgent(n_simulations=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87af3d2-aa4b-49a0-9276-2aff49869081",
      "metadata": {
        "id": "d87af3d2-aa4b-49a0-9276-2aff49869081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5453e557-582a-4eaa-fc04-320fd8e48b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "각 행동의 평균 점수: {0: -0.746, 1: -0.776, 2: -0.772, 3: -0.754, 4: -0.755, 5: -0.756, 6: -0.759, 7: -0.759, 8: -0.76}\n",
            "몬테카를로 탐색이 선택한 최적 행동: 7\n"
          ]
        }
      ],
      "source": [
        "# 3. 행동 평가 및 최적 행동 확인\n",
        "action_means = monte_carlo.evaluate_actions(state)\n",
        "print(\"각 행동의 평균 점수:\", action_means)\n",
        "\n",
        "best_action = monte_carlo.get_action(state)\n",
        "print(\"몬테카를로 탐색이 선택한 최적 행동:\", best_action)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}