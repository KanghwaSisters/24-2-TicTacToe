{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "060215d2-7b75-4240-9013-eb91c75f9b5b",
      "metadata": {
        "id": "060215d2-7b75-4240-9013-eb91c75f9b5b"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "06NR2_ZaqDIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd74c6e5-f38d-4201-dc41-908d0a23bca2"
      },
      "id": "06NR2_ZaqDIb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1999e8fc-6414-4d9b-9aab-baccb5e9e394",
      "metadata": {
        "id": "1999e8fc-6414-4d9b-9aab-baccb5e9e394"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb90d19-1adb-4a6c-a863-3efdee71eb91",
      "metadata": {
        "id": "ffb90d19-1adb-4a6c-a863-3efdee71eb91"
      },
      "source": [
        "# 00 Game Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a747d193-2a5d-402f-91df-945a5e38dd01",
      "metadata": {
        "id": "a747d193-2a5d-402f-91df-945a5e38dd01"
      },
      "outputs": [],
      "source": [
        "STATE_SIZE = (3,3)\n",
        "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
        "STATE_DIM = 3 # first player 정보 넣음\n",
        "BOARD_SHAPE = (STATE_DIM, 3, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef1969c-cb98-45a5-97a6-dfa5e54a0d48",
      "metadata": {
        "id": "3ef1969c-cb98-45a5-97a6-dfa5e54a0d48"
      },
      "source": [
        "# 01 HYPER PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f7bfcf-bc09-4c75-88a4-03b32baee960",
      "metadata": {
        "id": "42f7bfcf-bc09-4c75-88a4-03b32baee960"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 Env+State"
      ],
      "metadata": {
        "id": "GvyuGookkf0C"
      },
      "id": "GvyuGookkf0C"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 파일 복사 명령어 실행\n",
        "os.system('cp \"/content/drive/My Drive/Colab Notebooks/공통 environment+state.ipynb\" \"/content/\"')"
      ],
      "metadata": {
        "id": "ITAnF1wIzmz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a808d9-82ee-4374-d4af-58f661dba7ef"
      },
      "id": "ITAnF1wIzmz5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "notebook_path = \"/content/공통 environment+state.ipynb\"\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    notebook_content = nbformat.read(f, as_version=4)\n",
        "\n",
        "# 각 코드 셀 출력 및 실행\n",
        "for cell in notebook_content.cells:\n",
        "    if cell.cell_type == \"code\":\n",
        "        print(f\"실행 중인 코드:\\n{cell.source}\\n{'='*40}\")\n",
        "        exec(cell.source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpGCkWjqrtPf",
        "outputId": "35d08fa3-bc7f-416e-8ea7-910e6000719d"
      },
      "id": "hpGCkWjqrtPf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc1cecc-9934-4d03-81ea-b7b80b938d46",
      "metadata": {
        "id": "4bc1cecc-9934-4d03-81ea-b7b80b938d46"
      },
      "source": [
        "# 03 MinMax\n",
        "### 틱택토 상태 평가 신경망"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcb7cb0-d56a-426a-99b3-c4874b0eebe9",
      "metadata": {
        "id": "dfcb7cb0-d56a-426a-99b3-c4874b0eebe9"
      },
      "outputs": [],
      "source": [
        "class TicTacToeEvaluator(nn.Module):\n",
        "#입력: 3채널 보드 상태\n",
        "#출력: [-1, 1] (1=승리, -1=패배, 0=무승부/균형 상태)\n",
        "    def __init__(self, input_shape, hidden_units=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(np.prod(input_shape), hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.fc3 = nn.Linear(hidden_units, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()  # 출력 범위 제한 [-1, 1]\n",
        "\n",
        "\n",
        "    def forward(self, state_tensor):\n",
        "        x = state_tensor.view(state_tensor.size(0), -1)  # Flatten\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.tanh(self.fc3(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4b415f-fc95-4722-82f2-8a9b4d51d0e7",
      "metadata": {
        "id": "8e4b415f-fc95-4722-82f2-8a9b4d51d0e7"
      },
      "outputs": [],
      "source": [
        "class MinMaxAgent: #Minimax 알고리즘을 이용해 최적의 행동을 선택하는 에이전트\n",
        "    def __init__(self, evaluator, environment, depth=3):\n",
        "        self.evaluator = evaluator\n",
        "        self.environment = environment\n",
        "        self.depth = depth\n",
        "\n",
        "\n",
        "    def get_action(self, state): #현재 상태에서 minmax 탐색을 수행하고 최적의 행동을 반환\n",
        "        _, best_action = self._minmax(state, self.depth, True)\n",
        "        # 예외 처리 추가: 가능한 행동이 없을 경우 랜덤 선택\n",
        "        if best_action is None:\n",
        "            legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "            if len(legal_actions) > 0:\n",
        "                best_action = np.random.choice(legal_actions)  # 가능한 행동 중 랜덤 선택\n",
        "            else:\n",
        "                print(\"[WARNING] No valid actions available. Returning None.\")\n",
        "                return None\n",
        "\n",
        "        return best_action\n",
        "\n",
        "\n",
        "    def _minmax(self, state, depth, is_maximizing_player): #Minimax 알고리즘의 재귀 구현\n",
        "        legal_actions = np.where(state.get_legal_actions() != 0)[0]\n",
        "\n",
        "        # 게임 종료 상태이거나 탐색 깊이에 도달한 경우\n",
        "        if len(legal_actions) == 0 or depth == 0:\n",
        "            # 종료 상태에서 공통 보상 체계 활용\n",
        "            if state.check_done()[0]:\n",
        "                return state.get_reward(state), None\n",
        "            # 그렇지 않다면 신경망 평가값 반환\n",
        "            return self._evaluate_state(state), None\n",
        "\n",
        "        # 가능한 행동 목록 가져오기\n",
        "        if is_maximizing_player: #현재 플레이어가 최적의 행동을 하도록 max_eval 사용\n",
        "            max_eval = -float('inf')\n",
        "            best_action = None\n",
        "\n",
        "            for action in legal_actions:\n",
        "                next_state = state.next(action)\n",
        "                eval_value, _ = self._minmax(next_state, depth - 1, False)\n",
        "                if eval_value > max_eval:\n",
        "                    max_eval = eval_value\n",
        "                    best_action = action\n",
        "\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "\n",
        "            for action in legal_actions:\n",
        "                next_state = state.next(action)\n",
        "                eval_value, _ = self._minmax(next_state, depth - 1, True)\n",
        "                if eval_value < min_eval:\n",
        "                    min_eval = eval_value\n",
        "                    best_action = action\n",
        "\n",
        "            return min_eval, best_action\n",
        "\n",
        "\n",
        "    def _evaluate_state(self, state): #현재 상태를 신경망을 이용해 평가\n",
        "        board_tensor = torch.tensor(\n",
        "            state.state - state.enemy_state, dtype=torch.float32\n",
        "        ).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            value = self.evaluator(board_tensor).item()\n",
        "        return value #[-1,1] / 승리 또는 패배 가능성 반영"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578224a6-84c6-4d1b-bd86-36a8c858c7ad",
      "metadata": {
        "id": "578224a6-84c6-4d1b-bd86-36a8c858c7ad"
      },
      "source": [
        "## Test Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cpu\"  # GPU를 사용하려면 \"cuda\"로 변경"
      ],
      "metadata": {
        "id": "PiLN8JjWi0V4"
      },
      "id": "PiLN8JjWi0V4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TicTacToeEvaluator 초기화\n",
        "evaluator = TicTacToeEvaluator(input_shape=(3, 3), hidden_units=128).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(evaluator.parameters(), lr=0.005)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# TicTacToeEvaluator 학습을 위한 데이터 생성\n",
        "train_states = []\n",
        "train_labels = []\n",
        "\n",
        "for _ in range(5000):  # 5000개의 학습 데이터 생성\n",
        "    board_state = torch.randint(-1, 2, (3, 3), dtype=torch.float32)  # 무작위 보드 상태\n",
        "    label = torch.tensor([[torch.sum(board_state).item() * 0.1]], dtype=torch.float32)  # 단순한 가치 평가\n",
        "    train_states.append(board_state.view(-1))\n",
        "    train_labels.append(label)\n",
        "\n",
        "train_states = torch.stack(train_states).to(DEVICE)\n",
        "train_labels = torch.stack(train_labels).to(DEVICE)"
      ],
      "metadata": {
        "id": "6TUIN_z1jdrS"
      },
      "id": "6TUIN_z1jdrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 학습\n",
        "print(\"[DEBUG] Starting TicTacToeEvaluator training...\")\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = evaluator(train_states)\n",
        "    loss = criterion(predictions, train_labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"[DEBUG] Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"[DEBUG] TicTacToeEvaluator training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbeLpU3rjE-b",
        "outputId": "279e6612-eae1-4e9c-ad91-349672834196"
      },
      "id": "UbeLpU3rjE-b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Starting TicTacToeEvaluator training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([5000, 1, 1])) that is different to the input size (torch.Size([5000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Epoch 0, Loss: 0.0659\n",
            "[DEBUG] Epoch 10, Loss: 0.0613\n",
            "[DEBUG] Epoch 20, Loss: 0.0613\n",
            "[DEBUG] Epoch 30, Loss: 0.0611\n",
            "[DEBUG] Epoch 40, Loss: 0.0611\n",
            "[DEBUG] Epoch 50, Loss: 0.0610\n",
            "[DEBUG] Epoch 60, Loss: 0.0610\n",
            "[DEBUG] Epoch 70, Loss: 0.0610\n",
            "[DEBUG] Epoch 80, Loss: 0.0610\n",
            "[DEBUG] Epoch 90, Loss: 0.0610\n",
            "[DEBUG] TicTacToeEvaluator training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MinMaxAgent 생성\n",
        "environment = Environment()\n",
        "minmax_agent = MinMaxAgent(evaluator, environment, depth=3)"
      ],
      "metadata": {
        "id": "tYA9ui6bibGB"
      },
      "id": "tYA9ui6bibGB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 1: 초기 상태에서 가능한 행동 확인\n",
        "initial_state = State()\n",
        "print(\"초기 상태 보드:\")\n",
        "initial_state.render(initial_state)\n",
        "\n",
        "# Minimax 알고리즘이 선택한 행동 확인\n",
        "best_action = minmax_agent.get_action(initial_state)\n",
        "print(\"Minmax가 선택한 최적의 행동:\", best_action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akXQ5xcTjZAF",
        "outputId": "93bc8692-3932-44ee-af66-94dc11c57cb3"
      },
      "id": "akXQ5xcTjZAF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "초기 상태 보드:\n",
            "Minmax가 선택한 최적의 행동: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 2: 특정 상태에서 평가\n",
        "custom_state = State()\n",
        "custom_state = custom_state.next(3)  # 첫 번째 플레이어가 3번 칸에 놓음\n",
        "custom_state = custom_state.next(5)  # 두 번째 플레이어가 5번 칸에 놓음\n",
        "custom_state = custom_state.next(1)  # 첫 번째 플레이어가 1번 칸에 놓음\n",
        "custom_state = custom_state.next(2)  # 두 번째 플레이어가 2번 칸에 놓음\n",
        "\n",
        "print(\"테스트 상태 보드:\")\n",
        "custom_state.render(custom_state)\n",
        "\n",
        "# Minmax 알고리즘이 선택한 행동 확인\n",
        "best_action_test = minmax_agent.get_action(custom_state)\n",
        "print(\"테스트 상태에서 Minmax 선택 행동:\", best_action_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUL-u_Cvjfc2",
        "outputId": "78be12aa-0dc9-449c-a0a6-b402136c0658"
      },
      "id": "TUL-u_Cvjfc2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 상태 보드:\n",
            "테스트 상태에서 Minmax 선택 행동: 4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}