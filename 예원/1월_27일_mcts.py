# -*- coding: utf-8 -*-
"""1ì›”_27ì¼_MCTS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-zg4uafHAWhaPyXCGt6jh9lQYkWf4mV

# Import
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from math import sqrt

"""# 00 Game Info"""

STATE_SIZE = (3,3)
N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]
STATE_DIM = 3 # first player ì •ë³´ ë„£ìŒ
BOARD_SHAPE = (STATE_DIM, 3, 3)

"""# 01 HYPER PARAMS"""

C_PUCT = 2.5
N_SIMULATIONS = 100

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

"""# 02 Env+State"""

import os

# íŒŒì¼ ë³µì‚¬ ëª…ë ¹ì–´ ì‹¤í–‰
os.system('cp "/content/drive/My Drive/Colab Notebooks/ê³µí†µ environment+state.ipynb" "/content/"')

import nbformat

notebook_path = "/content/ê³µí†µ environment+state.ipynb"
with open(notebook_path, "r", encoding="utf-8") as f:
    notebook_content = nbformat.read(f, as_version=4)

# ê° ì½”ë“œ ì…€ ì¶œë ¥ ë° ì‹¤í–‰
for cell in notebook_content.cells:
    if cell.cell_type == "code":
        print(f"ì‹¤í–‰ ì¤‘ì¸ ì½”ë“œ:\n{cell.source}\n{'='*40}")
        exec(cell.source)

"""# 03 MCTS"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

"""### ì‚¬ì „ ì¤€ë¹„ (PolicyValueNet ì´ˆê¸°í™” ë° í•™ìŠµ)"""

# ìƒˆë¡œìš´ ëª¨ë¸ ì„¤ê³„
class PolicyValueNet(nn.Module):
    def __init__(self):
        super(PolicyValueNet, self).__init__()
        self.fc1 = nn.Linear(9, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 9)  # ì •ì±… ì˜ˆì¸¡
        self.value_head = nn.Linear(64, 1)  # ê°€ì¹˜ ì˜ˆì¸¡

        for layer in [self.fc1, self.fc2, self.fc3, self.value_head]:
            nn.init.kaiming_normal_(layer.weight, nonlinearity="leaky_relu")
            if layer.bias is not None:
                nn.init.constant_(layer.bias, 0.01)

    def forward(self, x):
        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)
        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)
        policy_logits = self.fc3(x)
        value = torch.tanh(self.value_head(x))  # [-1, 1] ë²”ìœ„ ë³´ì •
        return policy_logits, value


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
policy_model = PolicyValueNet().to(device)

# ê°€ì¹˜ í•™ìŠµì„ ìœ„í•œ ê°„ë‹¨í•œ í•™ìŠµ ê³¼ì •
optimizer = torch.optim.Adam(policy_model.parameters(), lr=0.005)
criterion = nn.MSELoss()

print("[DEBUG] Starting PolicyValueNet pre-training...")

for epoch in range(100):
    optimizer.zero_grad()

    board_tensor = torch.rand(1, 9).to(device)
    _, value = policy_model(board_tensor)

    target_value = torch.tensor([[0.5]], device=device)  # í•™ìŠµ ëª©í‘œ ê°’
    loss = criterion(value, target_value)
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"[DEBUG] Epoch {epoch}, Loss: {loss.item():.4f}")

print("[DEBUG] PolicyValueNet pre-training completed!")

"""###  Node_m í´ë˜ìŠ¤ (MCTS ë…¸ë“œ)
ê° MCTS ë…¸ë“œê°€ ê²Œì„ ìƒíƒœì™€ ì •ì±… í™•ë¥ , ë°©ë¬¸ íšŸìˆ˜, ê°€ì¹˜ ì´í•©ì„ ì €ì¥í•˜ë„ë¡ ì„¤ê³„
"""

class Node_m: #MCTSì—ì„œ ì‚¬ìš©ë˜ëŠ” ë…¸ë“œ í´ë˜ìŠ¤
    def __init__(self, state, parent=None, prior_prob=1.0):
        self.state = state
        self.parent = parent
        self.prior_prob = prior_prob # ì •ì±… ì‹ ê²½ë§ì—ì„œ ì˜ˆì¸¡ëœ í–‰ë™ í™•ë¥ 
        self.visit_count = 0 # ë…¸ë“œ ë°©ë¬¸ íšŸìˆ˜
        self.total_value = 0.0 # ë…¸ë“œì˜ ì´ í‰ê°€ ê°€ì¹˜
        self.children = {} # í–‰ë™(action) -> Node_m ë§¤í•‘


    def expand(self, policy_probs): # ìì‹ ë…¸ë“œ í™•ì¥
        print(f"[DEBUG] expand() called with policy_probs type: {type(policy_probs)}")
        # policy_probsë¥¼ ê¸°ë°˜ìœ¼ë¡œ valid_actionsì„ ê°€ì ¸ì™€ì„œ, ìƒˆë¡œìš´ ìì‹ ë…¸ë“œë¥¼ ìƒì„±
        # policy_probsê°€ ndarrayì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(policy_probs, np.ndarray): # policy_probsê°€ ndarrayì¸ì§€ í™•ì¸
            valid_actions = self.state.get_legal_actions()  # ìœ íš¨í•œ í–‰ë™ ê°€ì ¸ì˜¤ê¸°
            print(f"[DEBUG] Valid actions: {valid_actions}")
            print(f"[DEBUG] Policy probs: {policy_probs}")

            for action, prob in enumerate(policy_probs): # ì¸ë±ìŠ¤ì™€ í™•ë¥ ë¡œ ìˆœíšŒ
                # ìœ íš¨í•˜ê³  ì•„ì§ í™•ì¥ë˜ì§€ ì•Šì€ ê²½ìš°
                if action in valid_actions and action not in self.children:
                    if prob > 0:  # ğŸ”¥ ì •ì±… í™•ë¥ ì´ 0ë³´ë‹¤ í° ê²½ìš°ì—ë§Œ í™•ì¥
                        print(f"[DEBUG] Expanding action {action} with prob {prob:.4f}")
                        next_state = self.state.next(action) # ë‹¤ìŒ ìƒíƒœ ìƒì„±
                        # ìì‹ ë…¸ë“œ ì¶”ê°€
                        self.children[action] = Node_m(next_state, parent=self, prior_prob=prob)
                    else:
                        print(f"[WARNING] Skipping action {action} due to low policy prob: {prob:.6f}")

        else:
            for action, prob in policy_probs.items(): # ê¸°ì¡´ ë¡œì§ ìœ ì§€
                if action not in self.children: # ì•„ì§ í™•ì¥ë˜ì§€ ì•Šì€ ê²½ìš°
                    if prob > 0:
                        print(f"[DEBUG] Expanding action {action} with prob {prob:.4f}")
                        next_state = self.state.next(action) # ë‹¤ìŒ ìƒíƒœ ìƒì„±
                        # ìì‹ ë…¸ë“œ ì¶”ê°€
                        self.children[action] = Node_m(next_state, parent=self, prior_prob=prob)
                    else:
                        print(f"[WARNING] Skipping action {action} due to low policy prob: {prob:.6f}")


    def is_leaf(self): #ìì‹ ë…¸ë“œê°€ ì—†ëŠ” ê²½ìš° í™•ì¸
        return len(self.children) == 0


    def get_ucb_score(self, total_visits): #UCB ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ íƒí—˜ê³¼ í™œìš©ì˜ ê· í˜•ì„ ë§ì¶¤
        if self.visit_count == 0:
            return float('inf')
        q_value = self.total_value / self.visit_count
        exploration_term = C_PUCT * self.prior_prob * (math.sqrt(total_visits) / (1 + self.visit_count))
        return q_value + exploration_term #qê°’ê³¼ íƒìƒ‰ ê°€ì¤‘ì¹˜(C_PUCT)ë¥¼ ì´ìš©í•œ ê³„ì‚°

"""### MCTS ì‹¤í–‰"""

class MCTSAgent: #ëª¬í…Œì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤
    def __init__(self, policy_model):
        self.policy_model = policy_model


    def run_simulation(self, root): #MCTS ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        node = root
        search_path = [node]

        # Selection
        while not node.is_leaf(): # 1. Selection: íŠ¸ë¦¬ë¥¼ ë”°ë¼ í™•ì¥ë˜ì§€ ì•Šì€ ë…¸ë“œë¡œ ì´ë™
            total_visits = sum(child.visit_count for child in node.children.values())
            # ìì‹ ë…¸ë“œ ì¤‘ UCB ì ìˆ˜ê°€ ìµœëŒ€ì¸ ë…¸ë“œ ì„ íƒ
            node = max(node.children.values(), key=lambda child: child.get_ucb_score(total_visits))
            search_path.append(node)

        # Expansion & Evaluation: ë¦¬í”„ ë…¸ë“œ í™•ì¥
        if not node.state.check_done()[0]:  #check_done: ê²Œì„ ì¢…ë£Œ ì—¬ë¶€ & íŒ¨ë°° ì—¬ë¶€ ë°˜í™˜
            policy, value = self._evaluate_policy(node.state)
            print(f"[DEBUG] Evaluated value: {value:.2f}")  # ë””ë²„ê¹… ì¶”ê°€
            node.expand(policy)
        else:
            value = self._evaluate_terminal(node.state)

        # Backpropagation: ë¶€ëª¨ ë…¸ë“œë¡œ ê°’ì„ ì „ë‹¬
        self._backpropagate(search_path, value)


    def _evaluate_policy(self, state):
        board_tensor = torch.tensor(state.state - state.enemy_state, dtype=torch.float32).view(1, -1).to(device)

        with torch.no_grad(): #ì •ì±… ë° ìƒíƒœ í‰ê°€ (ì •ì±… ì‹ ê²½ë§ í™œìš©)
            policy_logits, value = self.policy_model(board_tensor)

        policy_probs = torch.softmax(policy_logits, dim=-1).squeeze(0).cpu().numpy() #ì •ì±…ì„ í™•ë¥ ë¡œ ë³€í™˜
        legal_actions = np.where(state.get_legal_actions() != 0)[0]
        action_probs = {action: policy_probs[action] for action in legal_actions}

        print(f"Policy probs from model: {policy_probs}")
        print(f"Action probs after filtering: {action_probs}")

        return action_probs, value.item()


    def _evaluate_terminal(self, state): #í„°ë¯¸ë„ ìƒíƒœì—ì„œ ê°€ì¹˜ í‰ê°€
        return state.get_reward(state)


    def _backpropagate(self, search_path, value): #ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ì—­ì „íŒŒ
        for node in reversed(search_path):
            node.visit_count += 1
            node.total_value += value  # ê°€ì¹˜ ì—…ë°ì´íŠ¸ê°€ í™•ì‹¤íˆ ë°˜ì˜ë¨
            print(f"[DEBUG] Backpropagation - Node visit_count: {node.visit_count}, total_value: {node.total_value:.2f}")

            value = -value  # ìƒëŒ€ë°© ê´€ì ì—ì„œ ë°˜ì „


    def get_action_probs(self, root, temp=1.5): #í–‰ë™ í™•ë¥  ê³„ì‚° / ì˜¨ë„ ì ìš©í•˜ì—¬ íƒìƒ‰ê³¼ í™œìš© ì¡°ì •
        if not isinstance(root, Node_m):  # rootê°€ Nodeì¸ì§€ í™•ì¸
            root = Node_m(root)  # ìë™ ë³€í™˜ ì¶”ê°€

        if not root.children:  # ìì‹ ë…¸ë“œê°€ ì—†ì„ ê²½ìš°
            num_actions = len(root.state.get_legal_actions())  # ê°€ëŠ¥í•œ í–‰ë™ ê°œìˆ˜
            return np.ones(num_actions) / num_actions  # ê· ë“± í™•ë¥  ë°˜í™˜

        visits = np.array([child.visit_count for child in root.children.values()])

        if visits.sum() == 0:  # ë°©ë¬¸ íšŸìˆ˜ê°€ 0ì¸ ê²½ìš° ì²˜ë¦¬
            return np.ones_like(visits) / len(visits)

        if temp == 0:  # ê°€ì¥ ë§ì´ ë°©ë¬¸í•œ í–‰ë™ ì„ íƒ (íƒìƒ‰ X)
            best_action = np.argmax(visits)
            probs = np.zeros_like(visits)
            probs[best_action] = 1
            return probs

        # ì˜¨ë„(temp) ì¡°ì •: ë‚®ì„ìˆ˜ë¡ ê°€ì¥ ë°©ë¬¸ ë§ì€ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²½í–¥
        visits = visits ** (1 / temp)
        return visits / visits.sum()


    def get_action(self, root, temp=1.0): #í–‰ë™ í™•ë¥ ì— ë”°ë¼ ìµœì¢… í–‰ë™ ì„ íƒ
        probs = self.get_action_probs(root, temp)
        if probs is None or len(probs) == 0:
            return np.random.choice(len(probs))  # ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ ëœë¤ ì„ íƒ
        return np.random.choice(len(probs), p=probs)  # í–‰ë™ í™•ë¥  ê¸°ë°˜ ì„ íƒ

"""## Test Code"""

state = State()
# ì‹¤í–‰
mcts = MCTSAgent(policy_model)
root = Node_m(state)  # ì´ˆê¸° ìƒíƒœ

for i in range(N_SIMULATIONS):
    print(f"\n[SIMULATION {i + 1}]")
    mcts.run_simulation(root)

    # íƒìƒ‰ í›„ `get_action()`ì„ ì‚¬ìš©í•˜ì—¬ í–‰ë™ ì„ íƒ
    action = mcts.get_action(root, temp=1.0)
    print(f"[RESULT] Chosen action: {action}")

print("MCTS ì‹¤í–‰ ì™„ë£Œ!")