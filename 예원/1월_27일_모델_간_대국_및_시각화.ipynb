{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "KB9HXwXd0hAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ieyt00Is0fdt",
        "outputId": "c96b9153-398d-469d-c155-e720b0ad885a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "rll8__p70j3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 00 Game Info"
      ],
      "metadata": {
        "id": "AjR-BQ7-0oJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATE_SIZE = (3,3)\n",
        "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
        "STATE_DIM = 3 # first player 정보 넣음\n",
        "BOARD_SHAPE = (STATE_DIM, 3, 3)"
      ],
      "metadata": {
        "id": "jfWpSOhs0otZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "ClHqQn3v0qV1",
        "outputId": "04217f57-f0f2-4666-a2a6-d6cd6b41f149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env"
      ],
      "metadata": {
        "id": "zpzOCDpV0yKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 파일 복사 명령어 실행\n",
        "os.system('cp \"/content/drive/My Drive/Colab Notebooks/공통 environment+state.ipynb\" \"/content/\"')"
      ],
      "metadata": {
        "id": "YDp25gn60zOk",
        "outputId": "5664ac00-9ae6-474e-dfea-fd4e6b8b04ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "notebook_path = \"/content/공통 environment+state.ipynb\"\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    notebook_content = nbformat.read(f, as_version=4)\n",
        "\n",
        "# 각 코드 셀 출력 및 실행\n",
        "for cell in notebook_content.cells:\n",
        "    if cell.cell_type == \"code\":\n",
        "        print(f\"실행 중인 코드:\\n{cell.source}\\n{'='*40}\")\n",
        "        exec(cell.source)"
      ],
      "metadata": {
        "id": "iVL8RqXR0zG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0cd55f-6c38-4960-e678-3dd2868ea4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BestModelAnalysis 모델 간 성능 비교하는 클래스\n",
        "각 모델끼리 대국 붙여서 승률 비교!\n",
        "한 두 번 하는 걸로는 두 모델 사이의 승률을 보기에 충분하진 않으니깐, 좀 여러 번 반복해서!"
      ],
      "metadata": {
        "id": "xC2svHPrA-FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자료 시각화\n",
        "1. 최종적으로 여러 모델 - 알파제로들(하이퍼 파라미터 설정에 따라 승률이 다를거라서 여러 개 ) 간의 승률을 비교해서 뭐가 제일 좋은 모델이었는지 자료를 시각화(or 도식화)해서 보고서에 넣기 -> 시각화 방법론 고민 ( + 여러 샘플 만들어보기 )\n",
        "\n",
        "2. 승률 엑셀로 정리하기(판다스 사용해서 자동화할 수 있을거야) 이거 두 개야!!\n"
      ],
      "metadata": {
        "id": "xKN2Ii9Hv7Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 가이드라인\n",
        "1. 지는 것 / 무승부 / 이기는 것 -> 지표 3개 만들기!\n",
        "2. (*이슈) 자기가 이길 생각만 하고 남이 이기는 걸 막지 못할 수가 있음 -> 한 게임 내에서 어떻게 행동하는지 지표 1개 만들기 -> 이상하다면 파라미터를 바꿔보자!\n",
        "3. 1000에포크를 돌릴 때 100에포크마다 어떻게 행동하는지 지표 만들기! valid 코드도 삽입하기"
      ],
      "metadata": {
        "id": "l_MPng7qwYOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------"
      ],
      "metadata": {
        "id": "c5-FkCEOCH5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크 100 돌리고, 20마다 출력!"
      ],
      "metadata": {
        "id": "ZMrUgZ1CF8Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5개 에이전트 파일 업로드"
      ],
      "metadata": {
        "id": "TbuRAoU3CLsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#현재 디렉터리 파일 목록 확인\n",
        "!ls"
      ],
      "metadata": {
        "id": "VSur7lfVmBVe",
        "outputId": "6c9358f4-5dea-413b-f482-af31ca25cb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1월_27일_alphabeta.py\t 1월_27일_minmax.py  '공통 environment+state.ipynb'   __pycache__\n",
            " 1월_27일_alphazero.py\t alphabeta_agent.py   mcs_agent.py\t\t      sample_data\n",
            " 1월_27일_mcs.py\t alphazero_agent.py   mcts_agent.py\t\t      train_losses.csv\n",
            " 1월_27일_mcts.py\t drive\t\t      minmax_agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Google Drive로 파일 이동 + 파일 이름 변경하여 처리\n",
        "!cp \"1월_27일_alphazero.py\" /content/alphazero_agent.py\n",
        "!cp \"1월_27일_minmax.py\" /content/minmax_agent.py\n",
        "!cp \"1월_27일_mcts.py\" /content/mcts_agent.py\n",
        "!cp \"1월_27일_mcs.py\" /content/mcs_agent.py\n",
        "!cp \"1월_27일_alphabeta.py\" /content/alphabeta_agent.py"
      ],
      "metadata": {
        "id": "E9g_ymssM6oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "ETJ0Uz8ZggEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 대국 실행 및 결과 저장"
      ],
      "metadata": {
        "id": "8_whBrdd7OAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BestModelAnalysis:\n",
        "    def __init__(self, agents, results_path='./results/validation_results.txt', ep_game_count=10):\n",
        "        self.agents = agents #AlphaZero vs 에이전트 딕셔너리\n",
        "        self.results_path = results_path #결과 저장 경로\n",
        "        self.ep_game_count = ep_game_count #각 평가당 실행할 게임 수\n",
        "\n",
        "\n",
        "    # 환경에서 정의된 보상 시스템을 그대로 사용\n",
        "    def _first_player_point(self, ended_state): #ended_state: 게임이 종료된 상태\n",
        "        return ended_state.get_reward(ended_state)\n",
        "\n",
        "\n",
        "    #두 에이전트 간 대국 실행 및 결과 반환 (승/패/무 비율, 경기 시간, 평균 보상)\n",
        "    def _play(self, next_actions):\n",
        "        state = State()  # 초기 상태 생성\n",
        "        # MCTS를 사용하는 경우 루트 노드 생성\n",
        "        root = Node_m(state) if hasattr(next_actions[0], 'get_action_probs') else None\n",
        "\n",
        "        start_time = time.time()  # 경기 시작 시간 기록\n",
        "        moves = 0  # 턴 수 계산\n",
        "        rewards = []  # 보상 기록\n",
        "\n",
        "        while not state.check_done()[0]:\n",
        "            is_first_player = state.check_first_player()\n",
        "\n",
        "            if is_first_player:\n",
        "                next_action = next_actions[0](root if isinstance(root, Node_m) else state)\n",
        "            else:\n",
        "                next_action = next_actions[1](root if isinstance(root, Node_m) else state)\n",
        "\n",
        "            state = state.next(next_action)\n",
        "            reward = state.get_reward(state)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # 보상 누적 과정 디버깅 추가\n",
        "            print(f\"[DEBUG] Move {moves+1}: Action {next_action}, Reward: {reward}\")\n",
        "            moves += 1  # 턴 수 증가\n",
        "\n",
        "            # 루트 노드 업데이트 (MCTS를 사용하는 경우)\n",
        "            if root and isinstance(root, Node_m):  # Node_m 타입 체크\n",
        "               if next_action in root.children:\n",
        "                   print(f\"[DEBUG] Root updated to existing child: {next_action}\")\n",
        "                   root = root.children[next_action]  # 루트 노드를 자식 노드로 갱신\n",
        "               elif not root.children and root.visit_count < 10:  # 탐색이 충분하지 않은 경우에만 초기화\n",
        "                   print(f\"[DEBUG] Root reset for new state.\")\n",
        "                   root = Node_m(state)  # 새 루트 노드 생성\n",
        "\n",
        "        end_time = time.time()  # 경기 종료 시간 기록\n",
        "        game_duration = end_time - start_time  # 총 경기 시간\n",
        "\n",
        "        final_reward = state.get_reward(state) # final_reward 변수에 최종 보상 값 할당\n",
        "\n",
        "        # 후공일 경우 보상 반전 적용\n",
        "        adjusted_final_reward = final_reward if state.check_first_player() else -final_reward\n",
        "        print(f\"[DEBUG] Final reward before return: {final_reward}, Adjusted: {adjusted_final_reward}, is_first_player={state.check_first_player()}\")\n",
        "\n",
        "        return {\n",
        "            \"reward\": adjusted_final_reward ,\n",
        "            \"duration\": game_duration,\n",
        "            \"average_reward\": sum(rewards) / len(rewards) if rewards else 0\n",
        "        }\n",
        "\n",
        "\n",
        "    def _evaluate_algorithm_of(self, label, next_actions):  # AlphaZero vs 다른 에이전트 대국\n",
        "        total_rewards = []\n",
        "        total_duration = 0\n",
        "        win_count, lose_count, draw_count = 0, 0, 0\n",
        "\n",
        "        for i in range(self.ep_game_count):\n",
        "            print(f'\\\\nEvaluate {label} {i + 1}/{self.ep_game_count}')\n",
        "            is_first_player = (i % 2 == 0)\n",
        "\n",
        "            result = self._play(next_actions if is_first_player else list(reversed(next_actions)))\n",
        "            reward = result[\"reward\"]\n",
        "\n",
        "            print(f\"[DEBUG] Adjusted reward after processing: {reward}, is_first_player={is_first_player}\")\n",
        "\n",
        "            total_rewards.append(reward)\n",
        "            total_duration += result[\"duration\"]\n",
        "\n",
        "            if reward == 1:\n",
        "                win_count += 1\n",
        "                print(f\"[DEBUG] AlphaZero WIN (is_first_player={is_first_player}, reward={reward})\")\n",
        "            elif reward == -1:\n",
        "                lose_count += 1\n",
        "                print(f\"[DEBUG] AlphaZero LOSE (is_first_player={is_first_player}, reward={reward})\")\n",
        "            else:\n",
        "                draw_count += 1\n",
        "                print(f\"[DEBUG] DRAW (is_first_player={is_first_player}, reward={reward})\")\n",
        "\n",
        "        avg_reward = sum(total_rewards) / len(total_rewards)\n",
        "        avg_duration = total_duration / self.ep_game_count\n",
        "\n",
        "        print(f'{label}: Avg Reward: {avg_reward:.3f} | Duration: {avg_duration:.2f}s')\n",
        "        print(f'Wins: {win_count}, Draws: {draw_count}, Losses: {lose_count}')\n",
        "\n",
        "        return {\n",
        "            \"average_reward\": avg_reward,\n",
        "            \"average_duration\": avg_duration,\n",
        "            \"win_rate\": win_count / self.ep_game_count,\n",
        "            \"draw_rate\": draw_count / self.ep_game_count,\n",
        "            \"lose_rate\": lose_count / self.ep_game_count\n",
        "        }\n",
        "\n",
        "\n",
        "    def run_validation(self, epoch): # AlphaZero와 다른 에이전트 간의 대결 평가 및 결과 저장\n",
        "        print(f'\\nEpoch {epoch} - 모델 검증 시작') #epoch: 현재 평가의 에포크 번호\n",
        "        results = {} #대결 결과(평균 점수) 저장할 딕셔너리 초기화\n",
        "\n",
        "        for name, agent in self.agents.items():\n",
        "        #{'AlphaZero': AlphaZeroAgent, 'MinMax': MinMaxAgent}\n",
        "            if name == 'AlphaZero':  # AlphaZero는 선 수로 고정되어 다른 에이전트와 비교\n",
        "                continue\n",
        "\n",
        "            results[f'VS_{name}'] = self._evaluate_algorithm_of( #메서드 호출하여 대결 수행\n",
        "                f'VS_{name}', (self.agents['AlphaZero'].get_action, agent.get_action)\n",
        "            ) #AlphaZero의 행동 함수(get_action)와 다른 에이전트의 행동 함수를 전달\n",
        "\n",
        "        # 결과를 파일에 저장\n",
        "        with open(self.results_path, 'a') as f:\n",
        "            f.write(f'Epoch {epoch}:\\n') #에포크 결과 기록\n",
        "            for key, value in results.items(): #각 에이전트와의 대결 결과를 저장\n",
        "                f.write(f'{key}: Avg Reward: {value[\"average_reward\"]:.3f} | '\n",
        "                        f'Duration: {value[\"average_duration\"]:.2f}s | '\n",
        "                        f'Win: {value[\"win_rate\"]:.2%}, Draw: {value[\"draw_rate\"]:.2%}, Lose: {value[\"lose_rate\"]:.2%}\\n')\n",
        "            f.write('\\n')\n",
        "\n",
        "        print(\"결과가 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "V4CoQHen0CKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "K4rKD1GYvkGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BestModelAnalysisWithPersistence(BestModelAnalysis):\n",
        "    def __init__(self, agents, results_path='./results/validation_results.txt',\n",
        "                 csv_path='./results/validation_results.csv', ep_game_count=10):\n",
        "        super().__init__(agents, results_path, ep_game_count)\n",
        "        self.csv_path = csv_path  # CSV 파일 경로\n",
        "\n",
        "\n",
        "    def save_to_csv(self, epoch, results):\n",
        "        \"\"\"대국 결과를 CSV 파일로 저장\"\"\"\n",
        "        data = []\n",
        "        for key, value in results.items():\n",
        "            opponent = key.replace('VS_', '')  # VS_ 접두어 제거\n",
        "\n",
        "            # Adjusted Reward를 기반으로 승/패/무 비율 계산\n",
        "            win_rate = value[\"win_rate\"]\n",
        "            draw_rate = value[\"draw_rate\"]\n",
        "            lose_rate = value[\"lose_rate\"]\n",
        "\n",
        "            # 승/패/무 비율이 정상적으로 저장되는지 디버깅 로그 추가\n",
        "            print(f\"[DEBUG] Saving to CSV - Epoch {epoch}, Opponent: {opponent}, \"\n",
        "                  f\"Win Rate: {win_rate:.2%}, \"\n",
        "                  f\"Draw Rate: {draw_rate:.2%}, \"\n",
        "                  f\"Lose Rate: {lose_rate:.2%}\")\n",
        "\n",
        "\n",
        "            # 새로운 데이터 구조 추가 (평균 보상, 경기 시간, 승/패/무 비율 포함)\n",
        "            data.append({\n",
        "                'Epoch': epoch,\n",
        "                'Model': 'AlphaZero',\n",
        "                'Opponent': opponent,\n",
        "                'Win Rate': win_rate,\n",
        "                'Draw Rate': draw_rate,\n",
        "                'Lose Rate': lose_rate,\n",
        "                'Avg Reward': value[\"average_reward\"],\n",
        "                'Duration (s)': value[\"average_duration\"]\n",
        "            })\n",
        "\n",
        "        # 기존 CSV 파일이 있으면 안전하게 읽고 추가 저장\n",
        "        try:\n",
        "            if os.path.exists(self.csv_path):\n",
        "                existing_data = pd.read_csv(self.csv_path)\n",
        "                df = pd.concat([existing_data, pd.DataFrame(data)], ignore_index=True)\n",
        "            else:\n",
        "                df = pd.DataFrame(data)\n",
        "\n",
        "            df.to_csv(self.csv_path, index=False)\n",
        "            print(f\"Epoch {epoch} 결과가 CSV 파일에 저장되었습니다: {self.csv_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" CSV 파일 저장 중 오류 발생: {e}\")\n",
        "\n",
        "\n",
        "    def parse_results_from_csv(self):\n",
        "        \"\"\"CSV 파일에서 데이터를 불러와서 기존 결과를 딕셔너리 형태로 변환\"\"\"\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            print(\" CSV 파일이 존재하지 않습니다.\")\n",
        "            return {}  # 파일이 없으면 빈 딕셔너리 반환\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "            cached_results = {}\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                epoch = int(row['Epoch'])\n",
        "                if epoch not in cached_results:\n",
        "                    cached_results[epoch] = {}\n",
        "\n",
        "                cached_results[epoch][f\"VS_{row['Opponent']}\"] = {\n",
        "                    \"win_rate\": float(row['Win Rate']),\n",
        "                    \"draw_rate\": float(row['Draw Rate']),\n",
        "                    \"lose_rate\": float(row['Lose Rate']),\n",
        "                    \"average_reward\": float(row['Avg Reward']),\n",
        "                    \"average_duration\": float(row['Duration (s)'])\n",
        "                }\n",
        "\n",
        "            return cached_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" CSV 파일을 읽는 중 오류 발생: {e}\")\n",
        "            return {}\n",
        "\n",
        "\n",
        "    def validate_model(self, epoch):\n",
        "        \"\"\"항상 새로운 대국을 실행하고 결과를 저장\"\"\"\n",
        "        print(f'\\nEpoch {epoch} - 모델 검증 시작')\n",
        "\n",
        "        results = {}  # 새로운 결과 저장\n",
        "        for name, agent in self.agents.items():\n",
        "            if name == 'AlphaZero':  # AlphaZero는 항상 선 수로 고정됨\n",
        "                continue\n",
        "            results[f'VS_{name}'] = self._evaluate_algorithm_of(\n",
        "                f'VS_{name}', (self.agents['AlphaZero'].get_action, agent.get_action)\n",
        "            )\n",
        "\n",
        "        # CSV에 저장\n",
        "        self.save_to_csv(epoch, results)\n",
        "\n",
        "        # 저장된 데이터 개수 확인\n",
        "        df = pd.read_csv(self.csv_path) if os.path.exists(self.csv_path) else None\n",
        "        num_rows = len(df) if df is not None else 0\n",
        "        print(f\"Epoch {epoch} 결과 저장 완료. 현재 CSV 데이터 개수: {num_rows} 행\\n\")\n"
      ],
      "metadata": {
        "id": "lHLSR-q4MvXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Code"
      ],
      "metadata": {
        "id": "xb1auJO3G0qS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "F-UqwVvrHF1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 강제로 캐시 제거\n",
        "if \"minmax_agent\" in sys.modules:\n",
        "    del sys.modules[\"minmax_agent\"]\n",
        "if \"mcts_agent\" in sys.modules:\n",
        "    del sys.modules[\"mcts_agent\"]\n",
        "if \"mcs_agent\" in sys.modules:\n",
        "    del sys.modules[\"mcs_agent\"]\n",
        "if \"alphabeta_agent\" in sys.modules:\n",
        "    del sys.modules[\"alphabeta_agent\"]"
      ],
      "metadata": {
        "id": "rq49Ttm_GrEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 객체 생성\n",
        "env = Environment()"
      ],
      "metadata": {
        "id": "H5MvF7RxKbbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 에이전트 초기화\n",
        "from alphazero_agent import AlphaZeroAgent, Network, predict\n",
        "from minmax_agent import MinMaxAgent, TicTacToeEvaluator\n",
        "from mcts_agent import MCTSAgent, Node_m\n",
        "from mcs_agent import MCSAgent\n",
        "from alphabeta_agent import AlphaBetaAgent"
      ],
      "metadata": {
        "id": "tWOKDazx1G42",
        "outputId": "ae7e689c-ee4c-498f-c1a4-b7e36119104d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n",
            "[DEBUG] Starting TicTacToeEvaluator training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([5000, 1, 1])) that is different to the input size (torch.Size([5000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Epoch 0, Loss: 0.0688\n",
            "[DEBUG] Epoch 10, Loss: 0.0607\n",
            "[DEBUG] Epoch 20, Loss: 0.0607\n",
            "[DEBUG] Epoch 30, Loss: 0.0607\n",
            "[DEBUG] Epoch 40, Loss: 0.0604\n",
            "[DEBUG] Epoch 50, Loss: 0.0604\n",
            "[DEBUG] Epoch 60, Loss: 0.0604\n",
            "[DEBUG] Epoch 70, Loss: 0.0604\n",
            "[DEBUG] Epoch 80, Loss: 0.0604\n",
            "[DEBUG] Epoch 90, Loss: 0.0604\n",
            "[DEBUG] TicTacToeEvaluator training completed!\n",
            "초기 상태 보드:\n",
            "Minmax가 선택한 최적의 행동: 4\n",
            "테스트 상태 보드:\n",
            "테스트 상태에서 Minmax 선택 행동: 0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n",
            "[DEBUG] Starting PolicyValueNet pre-training...\n",
            "[DEBUG] Epoch 0, Loss: 1.6974\n",
            "[DEBUG] Epoch 10, Loss: 0.0185\n",
            "[DEBUG] Epoch 20, Loss: 0.1634\n",
            "[DEBUG] Epoch 30, Loss: 0.0003\n",
            "[DEBUG] Epoch 40, Loss: 0.0228\n",
            "[DEBUG] Epoch 50, Loss: 0.0076\n",
            "[DEBUG] Epoch 60, Loss: 0.0422\n",
            "[DEBUG] Epoch 70, Loss: 0.0020\n",
            "[DEBUG] Epoch 80, Loss: 0.0331\n",
            "[DEBUG] Epoch 90, Loss: 0.0006\n",
            "[DEBUG] PolicyValueNet pre-training completed!\n",
            "\n",
            "[SIMULATION 1]\n",
            "Policy probs from model: [0.10357085 0.11191069 0.10419758 0.10860981 0.11419972 0.118283\n",
            " 0.11776529 0.10708439 0.11437871]\n",
            "Action probs after filtering: {0: 0.10357085, 1: 0.111910686, 2: 0.104197584, 3: 0.10860981, 4: 0.11419972, 5: 0.118283, 6: 0.11776529, 7: 0.107084386, 8: 0.114378706}\n",
            "[DEBUG] Evaluated value: 0.20\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1036\n",
            "[DEBUG] Expanding action 1 with prob 0.1119\n",
            "[DEBUG] Expanding action 2 with prob 0.1042\n",
            "[DEBUG] Expanding action 3 with prob 0.1086\n",
            "[DEBUG] Expanding action 4 with prob 0.1142\n",
            "[DEBUG] Expanding action 5 with prob 0.1183\n",
            "[DEBUG] Expanding action 6 with prob 0.1178\n",
            "[DEBUG] Expanding action 7 with prob 0.1071\n",
            "[DEBUG] Expanding action 8 with prob 0.1144\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.20\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 2]\n",
            "Policy probs from model: [0.05906527 0.076848   0.14639854 0.17514044 0.10630849 0.07927194\n",
            " 0.15187186 0.13374676 0.07134871]\n",
            "Action probs after filtering: {1: 0.076848, 2: 0.14639854, 3: 0.17514044, 4: 0.10630849, 5: 0.07927194, 6: 0.15187186, 7: 0.13374676, 8: 0.07134871}\n",
            "[DEBUG] Evaluated value: 0.15\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0768\n",
            "[DEBUG] Expanding action 2 with prob 0.1464\n",
            "[DEBUG] Expanding action 3 with prob 0.1751\n",
            "[DEBUG] Expanding action 4 with prob 0.1063\n",
            "[DEBUG] Expanding action 5 with prob 0.0793\n",
            "[DEBUG] Expanding action 6 with prob 0.1519\n",
            "[DEBUG] Expanding action 7 with prob 0.1337\n",
            "[DEBUG] Expanding action 8 with prob 0.0713\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.05\n",
            "[RESULT] Chosen action: 0\n",
            "\n",
            "[SIMULATION 3]\n",
            "Policy probs from model: [0.04769986 0.10813638 0.09501954 0.04807501 0.06126873 0.09704109\n",
            " 0.31953862 0.06839286 0.1548279 ]\n",
            "Action probs after filtering: {0: 0.04769986, 2: 0.09501954, 3: 0.048075005, 4: 0.06126873, 5: 0.09704109, 6: 0.31953862, 7: 0.06839286, 8: 0.1548279}\n",
            "[DEBUG] Evaluated value: 0.25\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0477\n",
            "[DEBUG] Expanding action 2 with prob 0.0950\n",
            "[DEBUG] Expanding action 3 with prob 0.0481\n",
            "[DEBUG] Expanding action 4 with prob 0.0613\n",
            "[DEBUG] Expanding action 5 with prob 0.0970\n",
            "[DEBUG] Expanding action 6 with prob 0.3195\n",
            "[DEBUG] Expanding action 7 with prob 0.0684\n",
            "[DEBUG] Expanding action 8 with prob 0.1548\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.25\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.20\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 4]\n",
            "Policy probs from model: [0.08180094 0.09439614 0.07532758 0.08698722 0.10552124 0.17810324\n",
            " 0.10066676 0.13231352 0.1448833 ]\n",
            "Action probs after filtering: {0: 0.08180094, 1: 0.094396144, 3: 0.08698722, 4: 0.10552124, 5: 0.17810324, 6: 0.10066676, 7: 0.13231352, 8: 0.1448833}\n",
            "[DEBUG] Evaluated value: 0.25\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0818\n",
            "[DEBUG] Expanding action 1 with prob 0.0944\n",
            "[DEBUG] Expanding action 3 with prob 0.0870\n",
            "[DEBUG] Expanding action 4 with prob 0.1055\n",
            "[DEBUG] Expanding action 5 with prob 0.1781\n",
            "[DEBUG] Expanding action 6 with prob 0.1007\n",
            "[DEBUG] Expanding action 7 with prob 0.1323\n",
            "[DEBUG] Expanding action 8 with prob 0.1449\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.25\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.45\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 5]\n",
            "Policy probs from model: [0.08829146 0.08869606 0.09693943 0.08649222 0.07310209 0.2391977\n",
            " 0.10987753 0.074477   0.14292644]\n",
            "Action probs after filtering: {0: 0.08829146, 1: 0.08869606, 2: 0.09693943, 4: 0.073102094, 5: 0.2391977, 6: 0.109877534, 7: 0.074477, 8: 0.14292644}\n",
            "[DEBUG] Evaluated value: 0.18\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0883\n",
            "[DEBUG] Expanding action 1 with prob 0.0887\n",
            "[DEBUG] Expanding action 2 with prob 0.0969\n",
            "[DEBUG] Expanding action 4 with prob 0.0731\n",
            "[DEBUG] Expanding action 5 with prob 0.2392\n",
            "[DEBUG] Expanding action 6 with prob 0.1099\n",
            "[DEBUG] Expanding action 7 with prob 0.0745\n",
            "[DEBUG] Expanding action 8 with prob 0.1429\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.18\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.63\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 6]\n",
            "Policy probs from model: [0.07531449 0.06545361 0.07218415 0.0764135  0.08772641 0.24712752\n",
            " 0.1417482  0.1137272  0.12030498]\n",
            "Action probs after filtering: {0: 0.07531449, 1: 0.06545361, 2: 0.07218415, 3: 0.0764135, 5: 0.24712752, 6: 0.1417482, 7: 0.1137272, 8: 0.12030498}\n",
            "[DEBUG] Evaluated value: 0.10\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0753\n",
            "[DEBUG] Expanding action 1 with prob 0.0655\n",
            "[DEBUG] Expanding action 2 with prob 0.0722\n",
            "[DEBUG] Expanding action 3 with prob 0.0764\n",
            "[DEBUG] Expanding action 5 with prob 0.2471\n",
            "[DEBUG] Expanding action 6 with prob 0.1417\n",
            "[DEBUG] Expanding action 7 with prob 0.1137\n",
            "[DEBUG] Expanding action 8 with prob 0.1203\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.10\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: -0.73\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 7]\n",
            "Policy probs from model: [0.0344605  0.17475484 0.09810916 0.10730343 0.10856167 0.12166435\n",
            " 0.12266051 0.03772906 0.19475648]\n",
            "Action probs after filtering: {0: 0.034460496, 1: 0.17475484, 2: 0.09810916, 3: 0.10730343, 4: 0.10856167, 6: 0.12266051, 7: 0.03772906, 8: 0.19475648}\n",
            "[DEBUG] Evaluated value: -0.16\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0345\n",
            "[DEBUG] Expanding action 1 with prob 0.1748\n",
            "[DEBUG] Expanding action 2 with prob 0.0981\n",
            "[DEBUG] Expanding action 3 with prob 0.1073\n",
            "[DEBUG] Expanding action 4 with prob 0.1086\n",
            "[DEBUG] Expanding action 6 with prob 0.1227\n",
            "[DEBUG] Expanding action 7 with prob 0.0377\n",
            "[DEBUG] Expanding action 8 with prob 0.1948\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.16\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: -0.57\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 8]\n",
            "Policy probs from model: [0.03472052 0.15647872 0.06747457 0.08281661 0.09104405 0.20516519\n",
            " 0.07408231 0.06002461 0.22819336]\n",
            "Action probs after filtering: {0: 0.034720518, 1: 0.15647872, 2: 0.067474574, 3: 0.08281661, 4: 0.091044046, 5: 0.20516519, 7: 0.060024608, 8: 0.22819336}\n",
            "[DEBUG] Evaluated value: 0.12\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0347\n",
            "[DEBUG] Expanding action 1 with prob 0.1565\n",
            "[DEBUG] Expanding action 2 with prob 0.0675\n",
            "[DEBUG] Expanding action 3 with prob 0.0828\n",
            "[DEBUG] Expanding action 4 with prob 0.0910\n",
            "[DEBUG] Expanding action 5 with prob 0.2052\n",
            "[DEBUG] Expanding action 7 with prob 0.0600\n",
            "[DEBUG] Expanding action 8 with prob 0.2282\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: -0.69\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 9]\n",
            "Policy probs from model: [0.13784237 0.10440315 0.07870396 0.07359367 0.07639794 0.2031944\n",
            " 0.09932177 0.05643783 0.17010494]\n",
            "Action probs after filtering: {0: 0.13784237, 1: 0.10440315, 2: 0.07870396, 3: 0.07359367, 4: 0.07639794, 5: 0.2031944, 6: 0.09932177, 8: 0.17010494}\n",
            "[DEBUG] Evaluated value: 0.16\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1378\n",
            "[DEBUG] Expanding action 1 with prob 0.1044\n",
            "[DEBUG] Expanding action 2 with prob 0.0787\n",
            "[DEBUG] Expanding action 3 with prob 0.0736\n",
            "[DEBUG] Expanding action 4 with prob 0.0764\n",
            "[DEBUG] Expanding action 5 with prob 0.2032\n",
            "[DEBUG] Expanding action 6 with prob 0.0993\n",
            "[DEBUG] Expanding action 8 with prob 0.1701\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.16\n",
            "[DEBUG] Backpropagation - Node visit_count: 9, total_value: -0.86\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 10]\n",
            "Policy probs from model: [0.07775135 0.08241286 0.08267805 0.12721814 0.03767352 0.21253584\n",
            " 0.16487396 0.05424546 0.1606107 ]\n",
            "Action probs after filtering: {0: 0.077751346, 1: 0.08241286, 2: 0.08267805, 3: 0.12721814, 4: 0.03767352, 5: 0.21253584, 6: 0.16487396, 7: 0.05424546}\n",
            "[DEBUG] Evaluated value: 0.13\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0778\n",
            "[DEBUG] Expanding action 1 with prob 0.0824\n",
            "[DEBUG] Expanding action 2 with prob 0.0827\n",
            "[DEBUG] Expanding action 3 with prob 0.1272\n",
            "[DEBUG] Expanding action 4 with prob 0.0377\n",
            "[DEBUG] Expanding action 5 with prob 0.2125\n",
            "[DEBUG] Expanding action 6 with prob 0.1649\n",
            "[DEBUG] Expanding action 7 with prob 0.0542\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 10, total_value: -0.98\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 11]\n",
            "Policy probs from model: [0.03279284 0.06517234 0.1112702  0.14064829 0.13123964 0.24179736\n",
            " 0.0933409  0.11678317 0.06695525]\n",
            "Action probs after filtering: {2: 0.1112702, 3: 0.14064829, 4: 0.13123964, 5: 0.24179736, 6: 0.0933409, 7: 0.11678317, 8: 0.06695525}\n",
            "[DEBUG] Evaluated value: -0.14\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.1113\n",
            "[DEBUG] Expanding action 3 with prob 0.1406\n",
            "[DEBUG] Expanding action 4 with prob 0.1312\n",
            "[DEBUG] Expanding action 5 with prob 0.2418\n",
            "[DEBUG] Expanding action 6 with prob 0.0933\n",
            "[DEBUG] Expanding action 7 with prob 0.1168\n",
            "[DEBUG] Expanding action 8 with prob 0.0670\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.39\n",
            "[DEBUG] Backpropagation - Node visit_count: 11, total_value: -1.12\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 12]\n",
            "Policy probs from model: [0.02616048 0.05419735 0.13180284 0.17207868 0.08575205 0.07548246\n",
            " 0.25839195 0.0347364  0.16139786]\n",
            "Action probs after filtering: {1: 0.054197352, 3: 0.17207868, 4: 0.08575205, 5: 0.07548246, 6: 0.25839195, 7: 0.0347364, 8: 0.16139786}\n",
            "[DEBUG] Evaluated value: 0.13\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0542\n",
            "[DEBUG] Expanding action 3 with prob 0.1721\n",
            "[DEBUG] Expanding action 4 with prob 0.0858\n",
            "[DEBUG] Expanding action 5 with prob 0.0755\n",
            "[DEBUG] Expanding action 6 with prob 0.2584\n",
            "[DEBUG] Expanding action 7 with prob 0.0347\n",
            "[DEBUG] Expanding action 8 with prob 0.1614\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 12, total_value: -1.00\n",
            "[RESULT] Chosen action: 0\n",
            "\n",
            "[SIMULATION 13]\n",
            "Policy probs from model: [0.0171093  0.03880208 0.10885809 0.08505936 0.13545743 0.0782978\n",
            " 0.13674632 0.3398848  0.05978481]\n",
            "Action probs after filtering: {1: 0.03880208, 2: 0.10885809, 4: 0.13545743, 5: 0.0782978, 6: 0.13674632, 7: 0.3398848, 8: 0.059784815}\n",
            "[DEBUG] Evaluated value: -0.05\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0388\n",
            "[DEBUG] Expanding action 2 with prob 0.1089\n",
            "[DEBUG] Expanding action 4 with prob 0.1355\n",
            "[DEBUG] Expanding action 5 with prob 0.0783\n",
            "[DEBUG] Expanding action 6 with prob 0.1367\n",
            "[DEBUG] Expanding action 7 with prob 0.3399\n",
            "[DEBUG] Expanding action 8 with prob 0.0598\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.23\n",
            "[DEBUG] Backpropagation - Node visit_count: 13, total_value: -1.05\n",
            "[RESULT] Chosen action: 0\n",
            "\n",
            "[SIMULATION 14]\n",
            "Policy probs from model: [0.18596475 0.04472388 0.07713199 0.079637   0.08928286 0.0563622\n",
            " 0.24635534 0.15548992 0.06505217]\n",
            "Action probs after filtering: {1: 0.04472388, 2: 0.07713199, 3: 0.079637, 4: 0.08928286, 5: 0.056362197, 7: 0.15548992, 8: 0.06505217}\n",
            "[DEBUG] Evaluated value: -0.06\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0447\n",
            "[DEBUG] Expanding action 2 with prob 0.0771\n",
            "[DEBUG] Expanding action 3 with prob 0.0796\n",
            "[DEBUG] Expanding action 4 with prob 0.0893\n",
            "[DEBUG] Expanding action 5 with prob 0.0564\n",
            "[DEBUG] Expanding action 7 with prob 0.1555\n",
            "[DEBUG] Expanding action 8 with prob 0.0651\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.06\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.18\n",
            "[DEBUG] Backpropagation - Node visit_count: 14, total_value: -1.10\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 15]\n",
            "Policy probs from model: [0.03374436 0.08388451 0.13139823 0.13655701 0.10698318 0.07721178\n",
            " 0.20694613 0.12050925 0.10276562]\n",
            "Action probs after filtering: {1: 0.083884515, 2: 0.13139823, 3: 0.13655701, 4: 0.106983185, 5: 0.07721178, 6: 0.20694613, 8: 0.10276562}\n",
            "[DEBUG] Evaluated value: 0.02\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0839\n",
            "[DEBUG] Expanding action 2 with prob 0.1314\n",
            "[DEBUG] Expanding action 3 with prob 0.1366\n",
            "[DEBUG] Expanding action 4 with prob 0.1070\n",
            "[DEBUG] Expanding action 5 with prob 0.0772\n",
            "[DEBUG] Expanding action 6 with prob 0.2069\n",
            "[DEBUG] Expanding action 8 with prob 0.1028\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 15, total_value: -1.08\n",
            "[RESULT] Chosen action: 0\n",
            "\n",
            "[SIMULATION 16]\n",
            "Policy probs from model: [0.03102981 0.09516213 0.1891773  0.11068607 0.10907913 0.0608504\n",
            " 0.16324687 0.1550258  0.08574251]\n",
            "Action probs after filtering: {1: 0.09516213, 2: 0.1891773, 3: 0.11068607, 4: 0.10907913, 5: 0.060850397, 6: 0.16324687, 7: 0.1550258}\n",
            "[DEBUG] Evaluated value: 0.11\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0952\n",
            "[DEBUG] Expanding action 2 with prob 0.1892\n",
            "[DEBUG] Expanding action 3 with prob 0.1107\n",
            "[DEBUG] Expanding action 4 with prob 0.1091\n",
            "[DEBUG] Expanding action 5 with prob 0.0609\n",
            "[DEBUG] Expanding action 6 with prob 0.1632\n",
            "[DEBUG] Expanding action 7 with prob 0.1550\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.11\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 16, total_value: -0.98\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 17]\n",
            "Policy probs from model: [0.03316702 0.08645437 0.11995125 0.0975244  0.09217425 0.04174568\n",
            " 0.30264926 0.10200343 0.12433039]\n",
            "Action probs after filtering: {1: 0.08645437, 2: 0.11995125, 3: 0.0975244, 5: 0.041745678, 6: 0.30264926, 7: 0.10200343, 8: 0.12433039}\n",
            "[DEBUG] Evaluated value: 0.07\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0865\n",
            "[DEBUG] Expanding action 2 with prob 0.1200\n",
            "[DEBUG] Expanding action 3 with prob 0.0975\n",
            "[DEBUG] Expanding action 5 with prob 0.0417\n",
            "[DEBUG] Expanding action 6 with prob 0.3026\n",
            "[DEBUG] Expanding action 7 with prob 0.1020\n",
            "[DEBUG] Expanding action 8 with prob 0.1243\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.07\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.03\n",
            "[DEBUG] Backpropagation - Node visit_count: 17, total_value: -0.90\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 18]\n",
            "Policy probs from model: [0.04004978 0.14705817 0.07135367 0.03469705 0.06178746 0.07636991\n",
            " 0.3391737  0.05758392 0.17192636]\n",
            "Action probs after filtering: {2: 0.07135367, 3: 0.034697052, 4: 0.061787456, 5: 0.07636991, 6: 0.3391737, 7: 0.057583917, 8: 0.17192636}\n",
            "[DEBUG] Evaluated value: 0.41\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.0714\n",
            "[DEBUG] Expanding action 3 with prob 0.0347\n",
            "[DEBUG] Expanding action 4 with prob 0.0618\n",
            "[DEBUG] Expanding action 5 with prob 0.0764\n",
            "[DEBUG] Expanding action 6 with prob 0.3392\n",
            "[DEBUG] Expanding action 7 with prob 0.0576\n",
            "[DEBUG] Expanding action 8 with prob 0.1719\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.41\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.26\n",
            "[DEBUG] Backpropagation - Node visit_count: 18, total_value: -0.49\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 19]\n",
            "Policy probs from model: [0.04749061 0.06716838 0.06413785 0.09067805 0.14709131 0.2591048\n",
            " 0.07766388 0.15021352 0.0964516 ]\n",
            "Action probs after filtering: {0: 0.047490608, 3: 0.09067805, 4: 0.14709131, 5: 0.2591048, 6: 0.077663876, 7: 0.15021352, 8: 0.096451595}\n",
            "[DEBUG] Evaluated value: 0.10\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0475\n",
            "[DEBUG] Expanding action 3 with prob 0.0907\n",
            "[DEBUG] Expanding action 4 with prob 0.1471\n",
            "[DEBUG] Expanding action 5 with prob 0.2591\n",
            "[DEBUG] Expanding action 6 with prob 0.0777\n",
            "[DEBUG] Expanding action 7 with prob 0.1502\n",
            "[DEBUG] Expanding action 8 with prob 0.0965\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.10\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 19, total_value: -0.39\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 20]\n",
            "Policy probs from model: [0.08451198 0.08626994 0.11482268 0.02086266 0.04198203 0.05314681\n",
            " 0.407429   0.10913203 0.08184282]\n",
            "Action probs after filtering: {0: 0.08451198, 2: 0.11482268, 3: 0.020862661, 4: 0.041982032, 5: 0.053146806, 7: 0.10913203, 8: 0.081842825}\n",
            "[DEBUG] Evaluated value: 0.08\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0845\n",
            "[DEBUG] Expanding action 2 with prob 0.1148\n",
            "[DEBUG] Expanding action 3 with prob 0.0209\n",
            "[DEBUG] Expanding action 4 with prob 0.0420\n",
            "[DEBUG] Expanding action 5 with prob 0.0531\n",
            "[DEBUG] Expanding action 7 with prob 0.1091\n",
            "[DEBUG] Expanding action 8 with prob 0.0818\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.08\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.10\n",
            "[DEBUG] Backpropagation - Node visit_count: 20, total_value: -0.31\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 21]\n",
            "Policy probs from model: [0.01401543 0.06049304 0.10423266 0.02653301 0.07005578 0.09126855\n",
            " 0.3590828  0.10254208 0.17177665]\n",
            "Action probs after filtering: {0: 0.014015432, 2: 0.10423266, 4: 0.07005578, 5: 0.09126855, 6: 0.3590828, 7: 0.10254208, 8: 0.17177665}\n",
            "[DEBUG] Evaluated value: 0.06\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0140\n",
            "[DEBUG] Expanding action 2 with prob 0.1042\n",
            "[DEBUG] Expanding action 4 with prob 0.0701\n",
            "[DEBUG] Expanding action 5 with prob 0.0913\n",
            "[DEBUG] Expanding action 6 with prob 0.3591\n",
            "[DEBUG] Expanding action 7 with prob 0.1025\n",
            "[DEBUG] Expanding action 8 with prob 0.1718\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.06\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.18\n",
            "[DEBUG] Backpropagation - Node visit_count: 21, total_value: -0.26\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 22]\n",
            "Policy probs from model: [0.09449463 0.08677527 0.1277826  0.06226363 0.09639029 0.08474069\n",
            " 0.19782107 0.16375121 0.0859806 ]\n",
            "Action probs after filtering: {1: 0.086775266, 2: 0.1277826, 3: 0.06226363, 4: 0.09639029, 6: 0.19782107, 7: 0.16375121, 8: 0.0859806}\n",
            "[DEBUG] Evaluated value: 0.63\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0868\n",
            "[DEBUG] Expanding action 2 with prob 0.1278\n",
            "[DEBUG] Expanding action 3 with prob 0.0623\n",
            "[DEBUG] Expanding action 4 with prob 0.0964\n",
            "[DEBUG] Expanding action 6 with prob 0.1978\n",
            "[DEBUG] Expanding action 7 with prob 0.1638\n",
            "[DEBUG] Expanding action 8 with prob 0.0860\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.63\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.78\n",
            "[DEBUG] Backpropagation - Node visit_count: 22, total_value: 0.37\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 23]\n",
            "Policy probs from model: [0.03579303 0.08669388 0.06963439 0.04745503 0.05063183 0.05340173\n",
            " 0.4282585  0.06774564 0.16038592]\n",
            "Action probs after filtering: {0: 0.03579303, 2: 0.06963439, 3: 0.04745503, 4: 0.05063183, 5: 0.053401727, 6: 0.4282585, 8: 0.16038592}\n",
            "[DEBUG] Evaluated value: 0.38\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0358\n",
            "[DEBUG] Expanding action 2 with prob 0.0696\n",
            "[DEBUG] Expanding action 3 with prob 0.0475\n",
            "[DEBUG] Expanding action 4 with prob 0.0506\n",
            "[DEBUG] Expanding action 5 with prob 0.0534\n",
            "[DEBUG] Expanding action 6 with prob 0.4283\n",
            "[DEBUG] Expanding action 8 with prob 0.1604\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.38\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.24\n",
            "[DEBUG] Backpropagation - Node visit_count: 23, total_value: 0.75\n",
            "[RESULT] Chosen action: 5\n",
            "\n",
            "[SIMULATION 24]\n",
            "Policy probs from model: [0.02854213 0.06586638 0.09134579 0.06765729 0.04664007 0.07196975\n",
            " 0.4326617  0.04796071 0.14735611]\n",
            "Action probs after filtering: {0: 0.028542127, 3: 0.06765729, 4: 0.04664007, 5: 0.07196975, 6: 0.4326617, 7: 0.04796071, 8: 0.14735611}\n",
            "[DEBUG] Evaluated value: -0.02\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0285\n",
            "[DEBUG] Expanding action 3 with prob 0.0677\n",
            "[DEBUG] Expanding action 4 with prob 0.0466\n",
            "[DEBUG] Expanding action 5 with prob 0.0720\n",
            "[DEBUG] Expanding action 6 with prob 0.4327\n",
            "[DEBUG] Expanding action 7 with prob 0.0480\n",
            "[DEBUG] Expanding action 8 with prob 0.1474\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 24, total_value: 0.73\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 25]\n",
            "Policy probs from model: [0.04445614 0.07631205 0.06363638 0.06061017 0.11805349 0.06561906\n",
            " 0.3856902  0.06902146 0.11660103]\n",
            "Action probs after filtering: {0: 0.044456135, 2: 0.063636385, 3: 0.06061017, 5: 0.06561906, 6: 0.3856902, 7: 0.069021456, 8: 0.11660103}\n",
            "[DEBUG] Evaluated value: 0.27\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0445\n",
            "[DEBUG] Expanding action 2 with prob 0.0636\n",
            "[DEBUG] Expanding action 3 with prob 0.0606\n",
            "[DEBUG] Expanding action 5 with prob 0.0656\n",
            "[DEBUG] Expanding action 6 with prob 0.3857\n",
            "[DEBUG] Expanding action 7 with prob 0.0690\n",
            "[DEBUG] Expanding action 8 with prob 0.1166\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.27\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.24\n",
            "[DEBUG] Backpropagation - Node visit_count: 25, total_value: 1.00\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 26]\n",
            "Policy probs from model: [0.02319733 0.18802188 0.07415228 0.01892857 0.08666176 0.0954702\n",
            " 0.27442256 0.06843721 0.17070824]\n",
            "Action probs after filtering: {0: 0.023197332, 2: 0.07415228, 3: 0.018928567, 4: 0.08666176, 5: 0.095470205, 6: 0.27442256, 7: 0.06843721}\n",
            "[DEBUG] Evaluated value: 0.31\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0232\n",
            "[DEBUG] Expanding action 2 with prob 0.0742\n",
            "[DEBUG] Expanding action 3 with prob 0.0189\n",
            "[DEBUG] Expanding action 4 with prob 0.0867\n",
            "[DEBUG] Expanding action 5 with prob 0.0955\n",
            "[DEBUG] Expanding action 6 with prob 0.2744\n",
            "[DEBUG] Expanding action 7 with prob 0.0684\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.31\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 26, total_value: 1.31\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 27]\n",
            "Policy probs from model: [0.0673076  0.04893392 0.13955006 0.03561711 0.05256276 0.4105818\n",
            " 0.09109192 0.05946226 0.09489255]\n",
            "Action probs after filtering: {0: 0.0673076, 2: 0.13955006, 4: 0.052562762, 5: 0.4105818, 6: 0.091091916, 7: 0.059462264, 8: 0.094892554}\n",
            "[DEBUG] Evaluated value: -0.00\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0673\n",
            "[DEBUG] Expanding action 2 with prob 0.1396\n",
            "[DEBUG] Expanding action 4 with prob 0.0526\n",
            "[DEBUG] Expanding action 5 with prob 0.4106\n",
            "[DEBUG] Expanding action 6 with prob 0.0911\n",
            "[DEBUG] Expanding action 7 with prob 0.0595\n",
            "[DEBUG] Expanding action 8 with prob 0.0949\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.00\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: 0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 27, total_value: 1.31\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 28]\n",
            "Policy probs from model: [0.1539956  0.07400295 0.06692068 0.0826101  0.09552543 0.10317966\n",
            " 0.18185654 0.13105795 0.11085109]\n",
            "Action probs after filtering: {0: 0.1539956, 1: 0.07400295, 3: 0.0826101, 4: 0.09552543, 5: 0.10317966, 7: 0.13105795, 8: 0.11085109}\n",
            "[DEBUG] Evaluated value: 0.40\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1540\n",
            "[DEBUG] Expanding action 1 with prob 0.0740\n",
            "[DEBUG] Expanding action 3 with prob 0.0826\n",
            "[DEBUG] Expanding action 4 with prob 0.0955\n",
            "[DEBUG] Expanding action 5 with prob 0.1032\n",
            "[DEBUG] Expanding action 7 with prob 0.1311\n",
            "[DEBUG] Expanding action 8 with prob 0.1109\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.40\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.30\n",
            "[DEBUG] Backpropagation - Node visit_count: 28, total_value: 1.71\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 29]\n",
            "Policy probs from model: [0.02581849 0.08141433 0.068773   0.05222811 0.16671163 0.16376181\n",
            " 0.1347949  0.22443317 0.08206446]\n",
            "Action probs after filtering: {0: 0.025818491, 1: 0.081414334, 4: 0.16671163, 5: 0.16376181, 6: 0.1347949, 7: 0.22443317, 8: 0.08206446}\n",
            "[DEBUG] Evaluated value: 0.30\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0258\n",
            "[DEBUG] Expanding action 1 with prob 0.0814\n",
            "[DEBUG] Expanding action 4 with prob 0.1667\n",
            "[DEBUG] Expanding action 5 with prob 0.1638\n",
            "[DEBUG] Expanding action 6 with prob 0.1348\n",
            "[DEBUG] Expanding action 7 with prob 0.2244\n",
            "[DEBUG] Expanding action 8 with prob 0.0821\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.30\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 29, total_value: 2.01\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 30]\n",
            "Policy probs from model: [0.05672807 0.10059245 0.09447458 0.08012342 0.04288008 0.15753613\n",
            " 0.32701033 0.03617924 0.10447568]\n",
            "Action probs after filtering: {0: 0.056728072, 1: 0.10059245, 4: 0.042880084, 5: 0.15753613, 6: 0.32701033, 7: 0.036179245, 8: 0.10447568}\n",
            "[DEBUG] Evaluated value: 0.12\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0567\n",
            "[DEBUG] Expanding action 1 with prob 0.1006\n",
            "[DEBUG] Expanding action 4 with prob 0.0429\n",
            "[DEBUG] Expanding action 5 with prob 0.1575\n",
            "[DEBUG] Expanding action 6 with prob 0.3270\n",
            "[DEBUG] Expanding action 7 with prob 0.0362\n",
            "[DEBUG] Expanding action 8 with prob 0.1045\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: 0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 30, total_value: 2.13\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 31]\n",
            "Policy probs from model: [0.0445945  0.03479913 0.05775226 0.06995527 0.1439686  0.3827379\n",
            " 0.06743915 0.09968621 0.09906701]\n",
            "Action probs after filtering: {0: 0.044594504, 2: 0.05775226, 3: 0.06995527, 5: 0.3827379, 6: 0.067439154, 7: 0.09968621, 8: 0.09906701}\n",
            "[DEBUG] Evaluated value: 0.15\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0446\n",
            "[DEBUG] Expanding action 2 with prob 0.0578\n",
            "[DEBUG] Expanding action 3 with prob 0.0700\n",
            "[DEBUG] Expanding action 5 with prob 0.3827\n",
            "[DEBUG] Expanding action 6 with prob 0.0674\n",
            "[DEBUG] Expanding action 7 with prob 0.0997\n",
            "[DEBUG] Expanding action 8 with prob 0.0991\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: 0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 31, total_value: 2.27\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 32]\n",
            "Policy probs from model: [0.07606398 0.12114576 0.07950438 0.07396701 0.08231355 0.19049916\n",
            " 0.14347616 0.08059631 0.15243372]\n",
            "Action probs after filtering: {1: 0.121145755, 3: 0.07396701, 4: 0.08231355, 5: 0.19049916, 6: 0.14347616, 7: 0.080596305, 8: 0.15243372}\n",
            "[DEBUG] Evaluated value: 0.53\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.1211\n",
            "[DEBUG] Expanding action 3 with prob 0.0740\n",
            "[DEBUG] Expanding action 4 with prob 0.0823\n",
            "[DEBUG] Expanding action 5 with prob 0.1905\n",
            "[DEBUG] Expanding action 6 with prob 0.1435\n",
            "[DEBUG] Expanding action 7 with prob 0.0806\n",
            "[DEBUG] Expanding action 8 with prob 0.1524\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.53\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.79\n",
            "[DEBUG] Backpropagation - Node visit_count: 32, total_value: 2.81\n",
            "[RESULT] Chosen action: 5\n",
            "\n",
            "[SIMULATION 33]\n",
            "Policy probs from model: [0.06328262 0.13863178 0.07876118 0.07713682 0.1071035  0.16185345\n",
            " 0.10901114 0.1331831  0.13103639]\n",
            "Action probs after filtering: {0: 0.063282624, 1: 0.13863178, 3: 0.07713682, 5: 0.16185345, 6: 0.109011136, 7: 0.1331831, 8: 0.13103639}\n",
            "[DEBUG] Evaluated value: 0.37\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0633\n",
            "[DEBUG] Expanding action 1 with prob 0.1386\n",
            "[DEBUG] Expanding action 3 with prob 0.0771\n",
            "[DEBUG] Expanding action 5 with prob 0.1619\n",
            "[DEBUG] Expanding action 6 with prob 0.1090\n",
            "[DEBUG] Expanding action 7 with prob 0.1332\n",
            "[DEBUG] Expanding action 8 with prob 0.1310\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.37\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.61\n",
            "[DEBUG] Backpropagation - Node visit_count: 33, total_value: 3.18\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 34]\n",
            "Policy probs from model: [0.0828021  0.08100662 0.11348631 0.06655163 0.12211418 0.15030783\n",
            " 0.15775107 0.11740402 0.10857628]\n",
            "Action probs after filtering: {0: 0.0828021, 1: 0.081006624, 3: 0.066551626, 4: 0.12211418, 5: 0.15030783, 6: 0.15775107, 7: 0.11740402}\n",
            "[DEBUG] Evaluated value: 0.21\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0828\n",
            "[DEBUG] Expanding action 1 with prob 0.0810\n",
            "[DEBUG] Expanding action 3 with prob 0.0666\n",
            "[DEBUG] Expanding action 4 with prob 0.1221\n",
            "[DEBUG] Expanding action 5 with prob 0.1503\n",
            "[DEBUG] Expanding action 6 with prob 0.1578\n",
            "[DEBUG] Expanding action 7 with prob 0.1174\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.21\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.49\n",
            "[DEBUG] Backpropagation - Node visit_count: 34, total_value: 3.39\n",
            "[RESULT] Chosen action: 7\n",
            "\n",
            "[SIMULATION 35]\n",
            "Policy probs from model: [0.09134994 0.09277697 0.07724297 0.07176697 0.09975658 0.09722591\n",
            " 0.14896664 0.20526004 0.11565397]\n",
            "Action probs after filtering: {0: 0.091349944, 1: 0.09277697, 3: 0.07176697, 4: 0.09975658, 5: 0.09722591, 6: 0.14896664, 8: 0.11565397}\n",
            "[DEBUG] Evaluated value: 0.18\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0913\n",
            "[DEBUG] Expanding action 1 with prob 0.0928\n",
            "[DEBUG] Expanding action 3 with prob 0.0718\n",
            "[DEBUG] Expanding action 4 with prob 0.0998\n",
            "[DEBUG] Expanding action 5 with prob 0.0972\n",
            "[DEBUG] Expanding action 6 with prob 0.1490\n",
            "[DEBUG] Expanding action 8 with prob 0.1157\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.18\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.42\n",
            "[DEBUG] Backpropagation - Node visit_count: 35, total_value: 3.57\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 36]\n",
            "Policy probs from model: [0.04711015 0.07430489 0.14728622 0.07760155 0.04725619 0.28231338\n",
            " 0.18779972 0.05168573 0.08464215]\n",
            "Action probs after filtering: {0: 0.04711015, 1: 0.07430489, 3: 0.07760155, 5: 0.28231338, 6: 0.18779972, 7: 0.051685728, 8: 0.08464215}\n",
            "[DEBUG] Evaluated value: 0.10\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0471\n",
            "[DEBUG] Expanding action 1 with prob 0.0743\n",
            "[DEBUG] Expanding action 3 with prob 0.0776\n",
            "[DEBUG] Expanding action 5 with prob 0.2823\n",
            "[DEBUG] Expanding action 6 with prob 0.1878\n",
            "[DEBUG] Expanding action 7 with prob 0.0517\n",
            "[DEBUG] Expanding action 8 with prob 0.0846\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.10\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.09\n",
            "[DEBUG] Backpropagation - Node visit_count: 36, total_value: 3.67\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 37]\n",
            "Policy probs from model: [0.05688005 0.10542139 0.08767956 0.09278772 0.13974984 0.19369869\n",
            " 0.11378153 0.08842768 0.12157355]\n",
            "Action probs after filtering: {0: 0.056880053, 2: 0.08767956, 3: 0.09278772, 4: 0.13974984, 6: 0.11378153, 7: 0.08842768, 8: 0.12157355}\n",
            "[DEBUG] Evaluated value: -0.14\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0569\n",
            "[DEBUG] Expanding action 2 with prob 0.0877\n",
            "[DEBUG] Expanding action 3 with prob 0.0928\n",
            "[DEBUG] Expanding action 4 with prob 0.1397\n",
            "[DEBUG] Expanding action 6 with prob 0.1138\n",
            "[DEBUG] Expanding action 7 with prob 0.0884\n",
            "[DEBUG] Expanding action 8 with prob 0.1216\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: 0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 37, total_value: 3.53\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 38]\n",
            "Policy probs from model: [0.03648231 0.0413544  0.05399529 0.05739462 0.12831627 0.22603616\n",
            " 0.1605139  0.2244342  0.07147282]\n",
            "Action probs after filtering: {0: 0.036482308, 1: 0.0413544, 2: 0.053995293, 5: 0.22603616, 6: 0.1605139, 7: 0.2244342, 8: 0.07147282}\n",
            "[DEBUG] Evaluated value: -0.06\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0365\n",
            "[DEBUG] Expanding action 1 with prob 0.0414\n",
            "[DEBUG] Expanding action 2 with prob 0.0540\n",
            "[DEBUG] Expanding action 5 with prob 0.2260\n",
            "[DEBUG] Expanding action 6 with prob 0.1605\n",
            "[DEBUG] Expanding action 7 with prob 0.2244\n",
            "[DEBUG] Expanding action 8 with prob 0.0715\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.06\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.07\n",
            "[DEBUG] Backpropagation - Node visit_count: 38, total_value: 3.48\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 39]\n",
            "Policy probs from model: [0.02515756 0.13819686 0.09713146 0.0920036  0.10246002 0.24133953\n",
            " 0.07877132 0.08771601 0.13722363]\n",
            "Action probs after filtering: {0: 0.025157563, 2: 0.09713146, 3: 0.0920036, 4: 0.10246002, 5: 0.24133953, 7: 0.08771601, 8: 0.13722363}\n",
            "[DEBUG] Evaluated value: -0.01\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0252\n",
            "[DEBUG] Expanding action 2 with prob 0.0971\n",
            "[DEBUG] Expanding action 3 with prob 0.0920\n",
            "[DEBUG] Expanding action 4 with prob 0.1025\n",
            "[DEBUG] Expanding action 5 with prob 0.2413\n",
            "[DEBUG] Expanding action 7 with prob 0.0877\n",
            "[DEBUG] Expanding action 8 with prob 0.1372\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.01\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: 0.30\n",
            "[DEBUG] Backpropagation - Node visit_count: 39, total_value: 3.47\n",
            "[RESULT] Chosen action: 0\n",
            "\n",
            "[SIMULATION 40]\n",
            "Policy probs from model: [0.16025239 0.09419801 0.07137864 0.04386437 0.07433793 0.18182579\n",
            " 0.20150337 0.07464379 0.09799572]\n",
            "Action probs after filtering: {0: 0.16025239, 1: 0.09419801, 2: 0.07137864, 4: 0.07433793, 5: 0.18182579, 7: 0.07464379, 8: 0.09799572}\n",
            "[DEBUG] Evaluated value: 0.36\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1603\n",
            "[DEBUG] Expanding action 1 with prob 0.0942\n",
            "[DEBUG] Expanding action 2 with prob 0.0714\n",
            "[DEBUG] Expanding action 4 with prob 0.0743\n",
            "[DEBUG] Expanding action 5 with prob 0.1818\n",
            "[DEBUG] Expanding action 7 with prob 0.0746\n",
            "[DEBUG] Expanding action 8 with prob 0.0980\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.36\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.67\n",
            "[DEBUG] Backpropagation - Node visit_count: 40, total_value: 3.83\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 41]\n",
            "Policy probs from model: [0.00911881 0.10164202 0.08452389 0.11755197 0.18224463 0.19007942\n",
            " 0.13560219 0.06675778 0.11247923]\n",
            "Action probs after filtering: {0: 0.009118806, 1: 0.10164202, 2: 0.08452389, 4: 0.18224463, 6: 0.13560219, 7: 0.06675778, 8: 0.11247923}\n",
            "[DEBUG] Evaluated value: -0.15\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0091\n",
            "[DEBUG] Expanding action 1 with prob 0.1016\n",
            "[DEBUG] Expanding action 2 with prob 0.0845\n",
            "[DEBUG] Expanding action 4 with prob 0.1822\n",
            "[DEBUG] Expanding action 6 with prob 0.1356\n",
            "[DEBUG] Expanding action 7 with prob 0.0668\n",
            "[DEBUG] Expanding action 8 with prob 0.1125\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: 0.09\n",
            "[DEBUG] Backpropagation - Node visit_count: 41, total_value: 3.68\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 42]\n",
            "Policy probs from model: [0.09434736 0.04447749 0.09265134 0.05379233 0.10799813 0.33938655\n",
            " 0.08199717 0.08520427 0.10014537]\n",
            "Action probs after filtering: {0: 0.09434736, 2: 0.092651345, 3: 0.053792328, 4: 0.107998125, 5: 0.33938655, 6: 0.08199717, 8: 0.10014537}\n",
            "[DEBUG] Evaluated value: 0.02\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0943\n",
            "[DEBUG] Expanding action 2 with prob 0.0927\n",
            "[DEBUG] Expanding action 3 with prob 0.0538\n",
            "[DEBUG] Expanding action 4 with prob 0.1080\n",
            "[DEBUG] Expanding action 5 with prob 0.3394\n",
            "[DEBUG] Expanding action 6 with prob 0.0820\n",
            "[DEBUG] Expanding action 8 with prob 0.1001\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: 0.28\n",
            "[DEBUG] Backpropagation - Node visit_count: 42, total_value: 3.70\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 43]\n",
            "Policy probs from model: [0.01149105 0.14563571 0.07005501 0.07182001 0.11636775 0.21098533\n",
            " 0.08836077 0.09424987 0.19103451]\n",
            "Action probs after filtering: {0: 0.011491045, 1: 0.14563571, 2: 0.07005501, 4: 0.11636775, 5: 0.21098533, 7: 0.094249874, 8: 0.19103451}\n",
            "[DEBUG] Evaluated value: 0.38\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0115\n",
            "[DEBUG] Expanding action 1 with prob 0.1456\n",
            "[DEBUG] Expanding action 2 with prob 0.0701\n",
            "[DEBUG] Expanding action 4 with prob 0.1164\n",
            "[DEBUG] Expanding action 5 with prob 0.2110\n",
            "[DEBUG] Expanding action 7 with prob 0.0942\n",
            "[DEBUG] Expanding action 8 with prob 0.1910\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.38\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: -0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 43, total_value: 4.07\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 44]\n",
            "Policy probs from model: [0.02791636 0.16486982 0.0605929  0.09796704 0.06314851 0.05674535\n",
            " 0.29022276 0.01529455 0.22324272]\n",
            "Action probs after filtering: {0: 0.027916364, 1: 0.16486982, 3: 0.097967036, 4: 0.063148506, 6: 0.29022276, 7: 0.015294551, 8: 0.22324272}\n",
            "[DEBUG] Evaluated value: -0.25\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0279\n",
            "[DEBUG] Expanding action 1 with prob 0.1649\n",
            "[DEBUG] Expanding action 3 with prob 0.0980\n",
            "[DEBUG] Expanding action 4 with prob 0.0631\n",
            "[DEBUG] Expanding action 6 with prob 0.2902\n",
            "[DEBUG] Expanding action 7 with prob 0.0153\n",
            "[DEBUG] Expanding action 8 with prob 0.2232\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.25\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: 0.17\n",
            "[DEBUG] Backpropagation - Node visit_count: 44, total_value: 3.82\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 45]\n",
            "Policy probs from model: [0.02190682 0.08815183 0.07293368 0.14313607 0.05371274 0.16843058\n",
            " 0.17683458 0.02916475 0.245729  ]\n",
            "Action probs after filtering: {0: 0.02190682, 1: 0.088151835, 3: 0.14313607, 4: 0.05371274, 5: 0.16843058, 7: 0.02916475, 8: 0.245729}\n",
            "[DEBUG] Evaluated value: 0.07\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0219\n",
            "[DEBUG] Expanding action 1 with prob 0.0882\n",
            "[DEBUG] Expanding action 3 with prob 0.1431\n",
            "[DEBUG] Expanding action 4 with prob 0.0537\n",
            "[DEBUG] Expanding action 5 with prob 0.1684\n",
            "[DEBUG] Expanding action 7 with prob 0.0292\n",
            "[DEBUG] Expanding action 8 with prob 0.2457\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.07\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: 0.10\n",
            "[DEBUG] Backpropagation - Node visit_count: 45, total_value: 3.89\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 46]\n",
            "Policy probs from model: [0.05696008 0.06186027 0.0768246  0.03190087 0.05189035 0.08740193\n",
            " 0.4063948  0.09665281 0.13011432]\n",
            "Action probs after filtering: {0: 0.056960076, 2: 0.0768246, 3: 0.03190087, 4: 0.05189035, 6: 0.4063948, 7: 0.096652806, 8: 0.13011432}\n",
            "[DEBUG] Evaluated value: 0.69\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0570\n",
            "[DEBUG] Expanding action 2 with prob 0.0768\n",
            "[DEBUG] Expanding action 3 with prob 0.0319\n",
            "[DEBUG] Expanding action 4 with prob 0.0519\n",
            "[DEBUG] Expanding action 6 with prob 0.4064\n",
            "[DEBUG] Expanding action 7 with prob 0.0967\n",
            "[DEBUG] Expanding action 8 with prob 0.1301\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.69\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -1.48\n",
            "[DEBUG] Backpropagation - Node visit_count: 46, total_value: 4.59\n",
            "[RESULT] Chosen action: 0\n",
            "\n",
            "[SIMULATION 47]\n",
            "Policy probs from model: [0.0523579  0.09699475 0.13982593 0.03757566 0.11011826 0.21100703\n",
            " 0.13710137 0.10548835 0.10953075]\n",
            "Action probs after filtering: {0: 0.0523579, 1: 0.09699475, 2: 0.13982593, 4: 0.110118255, 5: 0.21100703, 6: 0.13710137, 7: 0.105488345}\n",
            "[DEBUG] Evaluated value: 0.41\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0524\n",
            "[DEBUG] Expanding action 1 with prob 0.0970\n",
            "[DEBUG] Expanding action 2 with prob 0.1398\n",
            "[DEBUG] Expanding action 4 with prob 0.1101\n",
            "[DEBUG] Expanding action 5 with prob 0.2110\n",
            "[DEBUG] Expanding action 6 with prob 0.1371\n",
            "[DEBUG] Expanding action 7 with prob 0.1055\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.41\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.90\n",
            "[DEBUG] Backpropagation - Node visit_count: 47, total_value: 4.99\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 48]\n",
            "Policy probs from model: [0.11914994 0.13897668 0.07733268 0.09031153 0.07610933 0.09999505\n",
            " 0.21501751 0.06534515 0.11776214]\n",
            "Action probs after filtering: {0: 0.11914994, 1: 0.13897668, 2: 0.077332675, 4: 0.076109335, 5: 0.09999505, 6: 0.21501751, 8: 0.11776214}\n",
            "[DEBUG] Evaluated value: 0.47\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1191\n",
            "[DEBUG] Expanding action 1 with prob 0.1390\n",
            "[DEBUG] Expanding action 2 with prob 0.0773\n",
            "[DEBUG] Expanding action 4 with prob 0.0761\n",
            "[DEBUG] Expanding action 5 with prob 0.1000\n",
            "[DEBUG] Expanding action 6 with prob 0.2150\n",
            "[DEBUG] Expanding action 8 with prob 0.1178\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.47\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.90\n",
            "[DEBUG] Backpropagation - Node visit_count: 48, total_value: 5.47\n",
            "[RESULT] Chosen action: 7\n",
            "\n",
            "[SIMULATION 49]\n",
            "Policy probs from model: [0.05294891 0.0655478  0.07096653 0.0604446  0.05223754 0.354188\n",
            " 0.1296794  0.05439097 0.15959628]\n",
            "Action probs after filtering: {0: 0.052948907, 2: 0.07096653, 3: 0.060444605, 4: 0.05223754, 5: 0.354188, 6: 0.1296794, 7: 0.054390974}\n",
            "[DEBUG] Evaluated value: 0.30\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0529\n",
            "[DEBUG] Expanding action 2 with prob 0.0710\n",
            "[DEBUG] Expanding action 3 with prob 0.0604\n",
            "[DEBUG] Expanding action 4 with prob 0.0522\n",
            "[DEBUG] Expanding action 5 with prob 0.3542\n",
            "[DEBUG] Expanding action 6 with prob 0.1297\n",
            "[DEBUG] Expanding action 7 with prob 0.0544\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.30\n",
            "[DEBUG] Backpropagation - Node visit_count: 9, total_value: -0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 49, total_value: 5.76\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 50]\n",
            "Policy probs from model: [0.07567149 0.08962568 0.06372484 0.11839558 0.09428208 0.1181159\n",
            " 0.20195484 0.06911249 0.16911706]\n",
            "Action probs after filtering: {0: 0.07567149, 1: 0.08962568, 2: 0.06372484, 5: 0.1181159, 6: 0.20195484, 7: 0.069112495, 8: 0.16911706}\n",
            "[DEBUG] Evaluated value: 0.15\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0757\n",
            "[DEBUG] Expanding action 1 with prob 0.0896\n",
            "[DEBUG] Expanding action 2 with prob 0.0637\n",
            "[DEBUG] Expanding action 5 with prob 0.1181\n",
            "[DEBUG] Expanding action 6 with prob 0.2020\n",
            "[DEBUG] Expanding action 7 with prob 0.0691\n",
            "[DEBUG] Expanding action 8 with prob 0.1691\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.76\n",
            "[DEBUG] Backpropagation - Node visit_count: 50, total_value: 5.92\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 51]\n",
            "Policy probs from model: [0.08427253 0.06250048 0.10039755 0.07813787 0.04999008 0.18105862\n",
            " 0.25563803 0.04983957 0.13816519]\n",
            "Action probs after filtering: {0: 0.08427253, 1: 0.062500484, 3: 0.07813787, 4: 0.049990077, 5: 0.18105862, 6: 0.25563803, 8: 0.13816519}\n",
            "[DEBUG] Evaluated value: 0.14\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0843\n",
            "[DEBUG] Expanding action 1 with prob 0.0625\n",
            "[DEBUG] Expanding action 3 with prob 0.0781\n",
            "[DEBUG] Expanding action 4 with prob 0.0500\n",
            "[DEBUG] Expanding action 5 with prob 0.1811\n",
            "[DEBUG] Expanding action 6 with prob 0.2556\n",
            "[DEBUG] Expanding action 8 with prob 0.1382\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: -0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 51, total_value: 6.06\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 52]\n",
            "Policy probs from model: [0.13033235 0.05362525 0.06578793 0.03615322 0.06301255 0.21915983\n",
            " 0.20357323 0.11695565 0.11139996]\n",
            "Action probs after filtering: {0: 0.13033235, 1: 0.05362525, 2: 0.06578793, 3: 0.036153223, 5: 0.21915983, 7: 0.11695565, 8: 0.11139996}\n",
            "[DEBUG] Evaluated value: 0.02\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1303\n",
            "[DEBUG] Expanding action 1 with prob 0.0536\n",
            "[DEBUG] Expanding action 2 with prob 0.0658\n",
            "[DEBUG] Expanding action 3 with prob 0.0362\n",
            "[DEBUG] Expanding action 5 with prob 0.2192\n",
            "[DEBUG] Expanding action 7 with prob 0.1170\n",
            "[DEBUG] Expanding action 8 with prob 0.1114\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: -0.69\n",
            "[DEBUG] Backpropagation - Node visit_count: 52, total_value: 6.08\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 53]\n",
            "Policy probs from model: [0.05330735 0.08029205 0.09099447 0.05900517 0.17048311 0.16854866\n",
            " 0.09835785 0.14408787 0.13492341]\n",
            "Action probs after filtering: {0: 0.05330735, 1: 0.08029205, 2: 0.09099447, 4: 0.17048311, 5: 0.16854866, 6: 0.09835785, 8: 0.13492341}\n",
            "[DEBUG] Evaluated value: -0.11\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0533\n",
            "[DEBUG] Expanding action 1 with prob 0.0803\n",
            "[DEBUG] Expanding action 2 with prob 0.0910\n",
            "[DEBUG] Expanding action 4 with prob 0.1705\n",
            "[DEBUG] Expanding action 5 with prob 0.1685\n",
            "[DEBUG] Expanding action 6 with prob 0.0984\n",
            "[DEBUG] Expanding action 8 with prob 0.1349\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.11\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: -0.18\n",
            "[DEBUG] Backpropagation - Node visit_count: 53, total_value: 5.97\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 54]\n",
            "Policy probs from model: [0.06080585 0.03626982 0.08627567 0.15453096 0.02245579 0.16514288\n",
            " 0.28610775 0.02684794 0.16156337]\n",
            "Action probs after filtering: {0: 0.060805853, 1: 0.03626982, 3: 0.15453096, 4: 0.022455793, 5: 0.16514288, 6: 0.28610775, 7: 0.026847938}\n",
            "[DEBUG] Evaluated value: 0.09\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0608\n",
            "[DEBUG] Expanding action 1 with prob 0.0363\n",
            "[DEBUG] Expanding action 3 with prob 0.1545\n",
            "[DEBUG] Expanding action 4 with prob 0.0225\n",
            "[DEBUG] Expanding action 5 with prob 0.1651\n",
            "[DEBUG] Expanding action 6 with prob 0.2861\n",
            "[DEBUG] Expanding action 7 with prob 0.0268\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.09\n",
            "[DEBUG] Backpropagation - Node visit_count: 9, total_value: -0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 54, total_value: 6.06\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 55]\n",
            "Policy probs from model: [0.09485705 0.11199027 0.07374891 0.0455701  0.06984906 0.20513095\n",
            " 0.18686289 0.0766728  0.13531798]\n",
            "Action probs after filtering: {1: 0.11199027, 2: 0.07374891, 4: 0.06984906, 5: 0.20513095, 6: 0.18686289, 7: 0.0766728, 8: 0.13531798}\n",
            "[DEBUG] Evaluated value: 0.56\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.1120\n",
            "[DEBUG] Expanding action 2 with prob 0.0737\n",
            "[DEBUG] Expanding action 4 with prob 0.0698\n",
            "[DEBUG] Expanding action 5 with prob 0.2051\n",
            "[DEBUG] Expanding action 6 with prob 0.1869\n",
            "[DEBUG] Expanding action 7 with prob 0.0767\n",
            "[DEBUG] Expanding action 8 with prob 0.1353\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.56\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -1.35\n",
            "[DEBUG] Backpropagation - Node visit_count: 55, total_value: 6.62\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 56]\n",
            "Policy probs from model: [0.11535415 0.04451825 0.13027656 0.03407641 0.03087938 0.01877758\n",
            " 0.48638695 0.09598196 0.04374871]\n",
            "Action probs after filtering: {2: 0.13027656, 3: 0.03407641, 4: 0.030879384, 5: 0.018777585, 7: 0.09598196, 8: 0.04374871}\n",
            "[DEBUG] Evaluated value: 0.14\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.1303\n",
            "[DEBUG] Expanding action 3 with prob 0.0341\n",
            "[DEBUG] Expanding action 4 with prob 0.0309\n",
            "[DEBUG] Expanding action 5 with prob 0.0188\n",
            "[DEBUG] Expanding action 7 with prob 0.0960\n",
            "[DEBUG] Expanding action 8 with prob 0.0437\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 10, total_value: 0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 56, total_value: 6.47\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 57]\n",
            "Policy probs from model: [0.02861922 0.06572242 0.07566379 0.05123493 0.07239623 0.2249885\n",
            " 0.17205824 0.13897167 0.17034501]\n",
            "Action probs after filtering: {0: 0.028619224, 1: 0.06572242, 2: 0.07566379, 4: 0.072396226, 5: 0.2249885, 6: 0.17205824, 7: 0.13897167}\n",
            "[DEBUG] Evaluated value: 0.36\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0286\n",
            "[DEBUG] Expanding action 1 with prob 0.0657\n",
            "[DEBUG] Expanding action 2 with prob 0.0757\n",
            "[DEBUG] Expanding action 4 with prob 0.0724\n",
            "[DEBUG] Expanding action 5 with prob 0.2250\n",
            "[DEBUG] Expanding action 6 with prob 0.1721\n",
            "[DEBUG] Expanding action 7 with prob 0.1390\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.36\n",
            "[DEBUG] Backpropagation - Node visit_count: 9, total_value: -0.53\n",
            "[DEBUG] Backpropagation - Node visit_count: 57, total_value: 6.83\n",
            "[RESULT] Chosen action: 5\n",
            "\n",
            "[SIMULATION 58]\n",
            "Policy probs from model: [0.03139991 0.20871127 0.07179797 0.08302743 0.09743376 0.06508102\n",
            " 0.16515613 0.0277779  0.2496146 ]\n",
            "Action probs after filtering: {0: 0.031399906, 1: 0.20871127, 2: 0.071797974, 3: 0.08302743, 6: 0.16515613, 7: 0.027777903, 8: 0.2496146}\n",
            "[DEBUG] Evaluated value: -0.17\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0314\n",
            "[DEBUG] Expanding action 1 with prob 0.2087\n",
            "[DEBUG] Expanding action 2 with prob 0.0718\n",
            "[DEBUG] Expanding action 3 with prob 0.0830\n",
            "[DEBUG] Expanding action 6 with prob 0.1652\n",
            "[DEBUG] Expanding action 7 with prob 0.0278\n",
            "[DEBUG] Expanding action 8 with prob 0.2496\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.17\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: -0.59\n",
            "[DEBUG] Backpropagation - Node visit_count: 58, total_value: 6.65\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 59]\n",
            "Policy probs from model: [0.02656158 0.11135512 0.05946883 0.1478494  0.07466995 0.19310358\n",
            " 0.17891474 0.0520376  0.1560392 ]\n",
            "Action probs after filtering: {0: 0.026561575, 1: 0.11135512, 2: 0.059468832, 3: 0.1478494, 5: 0.19310358, 7: 0.052037604, 8: 0.1560392}\n",
            "[DEBUG] Evaluated value: 0.31\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0266\n",
            "[DEBUG] Expanding action 1 with prob 0.1114\n",
            "[DEBUG] Expanding action 2 with prob 0.0595\n",
            "[DEBUG] Expanding action 3 with prob 0.1478\n",
            "[DEBUG] Expanding action 5 with prob 0.1931\n",
            "[DEBUG] Expanding action 7 with prob 0.0520\n",
            "[DEBUG] Expanding action 8 with prob 0.1560\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.31\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: -0.90\n",
            "[DEBUG] Backpropagation - Node visit_count: 59, total_value: 6.96\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 60]\n",
            "Policy probs from model: [0.01989054 0.12826687 0.16105987 0.0359429  0.07396168 0.04489665\n",
            " 0.31403655 0.08572303 0.13622192]\n",
            "Action probs after filtering: {2: 0.16105987, 3: 0.035942905, 4: 0.07396168, 5: 0.04489665, 6: 0.31403655, 7: 0.08572303}\n",
            "[DEBUG] Evaluated value: 0.27\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.1611\n",
            "[DEBUG] Expanding action 3 with prob 0.0359\n",
            "[DEBUG] Expanding action 4 with prob 0.0740\n",
            "[DEBUG] Expanding action 5 with prob 0.0449\n",
            "[DEBUG] Expanding action 6 with prob 0.3140\n",
            "[DEBUG] Expanding action 7 with prob 0.0857\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.27\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.02\n",
            "[DEBUG] Backpropagation - Node visit_count: 11, total_value: 0.39\n",
            "[DEBUG] Backpropagation - Node visit_count: 60, total_value: 6.69\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 61]\n",
            "Policy probs from model: [0.1203998  0.11092842 0.09802496 0.02325376 0.03139437 0.04950482\n",
            " 0.31880835 0.11737979 0.13030572]\n",
            "Action probs after filtering: {0: 0.1203998, 3: 0.023253765, 4: 0.031394374, 5: 0.049504817, 7: 0.11737979, 8: 0.13030572}\n",
            "[DEBUG] Evaluated value: 0.26\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1204\n",
            "[DEBUG] Expanding action 3 with prob 0.0233\n",
            "[DEBUG] Expanding action 4 with prob 0.0314\n",
            "[DEBUG] Expanding action 5 with prob 0.0495\n",
            "[DEBUG] Expanding action 7 with prob 0.1174\n",
            "[DEBUG] Expanding action 8 with prob 0.1303\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.26\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.41\n",
            "[DEBUG] Backpropagation - Node visit_count: 12, total_value: 0.65\n",
            "[DEBUG] Backpropagation - Node visit_count: 61, total_value: 6.43\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 62]\n",
            "Policy probs from model: [0.15885602 0.1000349  0.11702038 0.03378357 0.035316   0.06385843\n",
            " 0.32553387 0.09319296 0.07240386]\n",
            "Action probs after filtering: {0: 0.15885602, 2: 0.11702038, 4: 0.035316, 5: 0.063858435, 7: 0.09319296, 8: 0.072403856}\n",
            "[DEBUG] Evaluated value: 0.19\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1589\n",
            "[DEBUG] Expanding action 2 with prob 0.1170\n",
            "[DEBUG] Expanding action 4 with prob 0.0353\n",
            "[DEBUG] Expanding action 5 with prob 0.0639\n",
            "[DEBUG] Expanding action 7 with prob 0.0932\n",
            "[DEBUG] Expanding action 8 with prob 0.0724\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.19\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.60\n",
            "[DEBUG] Backpropagation - Node visit_count: 13, total_value: 0.84\n",
            "[DEBUG] Backpropagation - Node visit_count: 62, total_value: 6.24\n",
            "[RESULT] Chosen action: 7\n",
            "\n",
            "[SIMULATION 63]\n",
            "Policy probs from model: [0.02126857 0.03672056 0.11218829 0.10070352 0.03698581 0.04046797\n",
            " 0.48670682 0.02715899 0.13779944]\n",
            "Action probs after filtering: {3: 0.10070352, 4: 0.036985815, 5: 0.04046797, 6: 0.48670682, 7: 0.027158994, 8: 0.13779944}\n",
            "[DEBUG] Evaluated value: 0.05\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 3 with prob 0.1007\n",
            "[DEBUG] Expanding action 4 with prob 0.0370\n",
            "[DEBUG] Expanding action 5 with prob 0.0405\n",
            "[DEBUG] Expanding action 6 with prob 0.4867\n",
            "[DEBUG] Expanding action 7 with prob 0.0272\n",
            "[DEBUG] Expanding action 8 with prob 0.1378\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 14, total_value: 0.90\n",
            "[DEBUG] Backpropagation - Node visit_count: 63, total_value: 6.19\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 64]\n",
            "Policy probs from model: [0.11128585 0.09306376 0.12243567 0.05903198 0.13910258 0.08641192\n",
            " 0.19731094 0.11145755 0.07989985]\n",
            "Action probs after filtering: {0: 0.11128585, 1: 0.093063764, 2: 0.12243567, 3: 0.059031982, 4: 0.13910258, 7: 0.11145755, 8: 0.07989985}\n",
            "[DEBUG] Evaluated value: -0.25\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1113\n",
            "[DEBUG] Expanding action 1 with prob 0.0931\n",
            "[DEBUG] Expanding action 2 with prob 0.1224\n",
            "[DEBUG] Expanding action 3 with prob 0.0590\n",
            "[DEBUG] Expanding action 4 with prob 0.1391\n",
            "[DEBUG] Expanding action 7 with prob 0.1115\n",
            "[DEBUG] Expanding action 8 with prob 0.0799\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.25\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: -0.44\n",
            "[DEBUG] Backpropagation - Node visit_count: 64, total_value: 5.94\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 65]\n",
            "Policy probs from model: [0.21491095 0.05529355 0.08906441 0.0402251  0.0578346  0.16829558\n",
            " 0.17893748 0.09566528 0.09977309]\n",
            "Action probs after filtering: {0: 0.21491095, 1: 0.055293545, 2: 0.08906441, 3: 0.040225103, 4: 0.057834603, 5: 0.16829558, 8: 0.09977309}\n",
            "[DEBUG] Evaluated value: 0.03\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.2149\n",
            "[DEBUG] Expanding action 1 with prob 0.0553\n",
            "[DEBUG] Expanding action 2 with prob 0.0891\n",
            "[DEBUG] Expanding action 3 with prob 0.0402\n",
            "[DEBUG] Expanding action 4 with prob 0.0578\n",
            "[DEBUG] Expanding action 5 with prob 0.1683\n",
            "[DEBUG] Expanding action 8 with prob 0.0998\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.03\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: -0.47\n",
            "[DEBUG] Backpropagation - Node visit_count: 65, total_value: 5.97\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 66]\n",
            "Policy probs from model: [0.06315504 0.23677643 0.09832737 0.02452143 0.0647169  0.08560614\n",
            " 0.23890905 0.06290893 0.12507875]\n",
            "Action probs after filtering: {0: 0.06315504, 3: 0.024521425, 4: 0.0647169, 5: 0.085606135, 6: 0.23890905, 7: 0.062908925}\n",
            "[DEBUG] Evaluated value: 0.52\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0632\n",
            "[DEBUG] Expanding action 3 with prob 0.0245\n",
            "[DEBUG] Expanding action 4 with prob 0.0647\n",
            "[DEBUG] Expanding action 5 with prob 0.0856\n",
            "[DEBUG] Expanding action 6 with prob 0.2389\n",
            "[DEBUG] Expanding action 7 with prob 0.0629\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.52\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.49\n",
            "[DEBUG] Backpropagation - Node visit_count: 15, total_value: 1.41\n",
            "[DEBUG] Backpropagation - Node visit_count: 66, total_value: 5.45\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 67]\n",
            "Policy probs from model: [0.08858163 0.09493364 0.10200223 0.01732243 0.0168295  0.09982037\n",
            " 0.43068877 0.08989757 0.0599238 ]\n",
            "Action probs after filtering: {0: 0.08858163, 2: 0.102002226, 3: 0.017322427, 5: 0.09982037, 7: 0.08989757, 8: 0.0599238}\n",
            "[DEBUG] Evaluated value: 0.05\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0886\n",
            "[DEBUG] Expanding action 2 with prob 0.1020\n",
            "[DEBUG] Expanding action 3 with prob 0.0173\n",
            "[DEBUG] Expanding action 5 with prob 0.0998\n",
            "[DEBUG] Expanding action 7 with prob 0.0899\n",
            "[DEBUG] Expanding action 8 with prob 0.0599\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 5, total_value: -0.65\n",
            "[DEBUG] Backpropagation - Node visit_count: 16, total_value: 1.46\n",
            "[DEBUG] Backpropagation - Node visit_count: 67, total_value: 5.40\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 68]\n",
            "Policy probs from model: [0.02658018 0.06769706 0.0994261  0.06601932 0.07570908 0.03126217\n",
            " 0.4546809  0.08154247 0.09708266]\n",
            "Action probs after filtering: {2: 0.0994261, 3: 0.06601932, 5: 0.031262174, 6: 0.4546809, 7: 0.08154247, 8: 0.09708266}\n",
            "[DEBUG] Evaluated value: 0.17\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.0994\n",
            "[DEBUG] Expanding action 3 with prob 0.0660\n",
            "[DEBUG] Expanding action 5 with prob 0.0313\n",
            "[DEBUG] Expanding action 6 with prob 0.4547\n",
            "[DEBUG] Expanding action 7 with prob 0.0815\n",
            "[DEBUG] Expanding action 8 with prob 0.0971\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.17\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.03\n",
            "[DEBUG] Backpropagation - Node visit_count: 17, total_value: 1.64\n",
            "[DEBUG] Backpropagation - Node visit_count: 68, total_value: 5.23\n",
            "[RESULT] Chosen action: 7\n",
            "\n",
            "[SIMULATION 69]\n",
            "Policy probs from model: [0.05096086 0.13997096 0.15053163 0.04289774 0.05343656 0.04905208\n",
            " 0.35520226 0.06979272 0.08815516]\n",
            "Action probs after filtering: {0: 0.050960865, 2: 0.15053163, 3: 0.04289774, 4: 0.053436562, 7: 0.06979272, 8: 0.08815516}\n",
            "[DEBUG] Evaluated value: -0.50\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0510\n",
            "[DEBUG] Expanding action 2 with prob 0.1505\n",
            "[DEBUG] Expanding action 3 with prob 0.0429\n",
            "[DEBUG] Expanding action 4 with prob 0.0534\n",
            "[DEBUG] Expanding action 7 with prob 0.0698\n",
            "[DEBUG] Expanding action 8 with prob 0.0882\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.50\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: -0.15\n",
            "[DEBUG] Backpropagation - Node visit_count: 18, total_value: 1.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 69, total_value: 5.73\n",
            "[RESULT] Chosen action: 8\n",
            "\n",
            "[SIMULATION 70]\n",
            "Policy probs from model: [0.05017226 0.08741213 0.13013141 0.04932953 0.11061334 0.16972092\n",
            " 0.20656139 0.12026817 0.07579087]\n",
            "Action probs after filtering: {0: 0.05017226, 1: 0.08741213, 2: 0.13013141, 3: 0.04932953, 5: 0.16972092, 6: 0.20656139, 7: 0.12026817}\n",
            "[DEBUG] Evaluated value: -0.34\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0502\n",
            "[DEBUG] Expanding action 1 with prob 0.0874\n",
            "[DEBUG] Expanding action 2 with prob 0.1301\n",
            "[DEBUG] Expanding action 3 with prob 0.0493\n",
            "[DEBUG] Expanding action 5 with prob 0.1697\n",
            "[DEBUG] Expanding action 6 with prob 0.2066\n",
            "[DEBUG] Expanding action 7 with prob 0.1203\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.34\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: -0.56\n",
            "[DEBUG] Backpropagation - Node visit_count: 70, total_value: 5.39\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 71]\n",
            "Policy probs from model: [0.01200301 0.25450882 0.12970732 0.06032952 0.10329415 0.12933904\n",
            " 0.14529023 0.03062711 0.13490072]\n",
            "Action probs after filtering: {0: 0.012003013, 1: 0.25450882, 2: 0.12970732, 3: 0.060329515, 4: 0.10329415, 6: 0.14529023, 7: 0.030627115}\n",
            "[DEBUG] Evaluated value: -0.16\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0120\n",
            "[DEBUG] Expanding action 1 with prob 0.2545\n",
            "[DEBUG] Expanding action 2 with prob 0.1297\n",
            "[DEBUG] Expanding action 3 with prob 0.0603\n",
            "[DEBUG] Expanding action 4 with prob 0.1033\n",
            "[DEBUG] Expanding action 6 with prob 0.1453\n",
            "[DEBUG] Expanding action 7 with prob 0.0306\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.16\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: -0.40\n",
            "[DEBUG] Backpropagation - Node visit_count: 71, total_value: 5.23\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 72]\n",
            "Policy probs from model: [0.0182507  0.19098096 0.07631004 0.06233942 0.1429249  0.1409497\n",
            " 0.12096405 0.06332119 0.18395905]\n",
            "Action probs after filtering: {0: 0.018250696, 1: 0.19098096, 2: 0.07631004, 3: 0.062339418, 4: 0.1429249, 5: 0.1409497, 7: 0.06332119}\n",
            "[DEBUG] Evaluated value: -0.05\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0183\n",
            "[DEBUG] Expanding action 1 with prob 0.1910\n",
            "[DEBUG] Expanding action 2 with prob 0.0763\n",
            "[DEBUG] Expanding action 3 with prob 0.0623\n",
            "[DEBUG] Expanding action 4 with prob 0.1429\n",
            "[DEBUG] Expanding action 5 with prob 0.1409\n",
            "[DEBUG] Expanding action 7 with prob 0.0633\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: -0.35\n",
            "[DEBUG] Backpropagation - Node visit_count: 72, total_value: 5.18\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 73]\n",
            "Policy probs from model: [0.04390831 0.10792273 0.13310899 0.02896627 0.12232184 0.23254162\n",
            " 0.15164451 0.05566043 0.12392525]\n",
            "Action probs after filtering: {0: 0.043908313, 1: 0.10792273, 2: 0.13310899, 3: 0.028966267, 4: 0.122321844, 5: 0.23254162, 6: 0.15164451}\n",
            "[DEBUG] Evaluated value: 0.38\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0439\n",
            "[DEBUG] Expanding action 1 with prob 0.1079\n",
            "[DEBUG] Expanding action 2 with prob 0.1331\n",
            "[DEBUG] Expanding action 3 with prob 0.0290\n",
            "[DEBUG] Expanding action 4 with prob 0.1223\n",
            "[DEBUG] Expanding action 5 with prob 0.2325\n",
            "[DEBUG] Expanding action 6 with prob 0.1516\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.38\n",
            "[DEBUG] Backpropagation - Node visit_count: 9, total_value: -0.73\n",
            "[DEBUG] Backpropagation - Node visit_count: 73, total_value: 5.56\n",
            "[RESULT] Chosen action: 5\n",
            "\n",
            "[SIMULATION 74]\n",
            "Policy probs from model: [0.1895078  0.06027966 0.06688188 0.08506714 0.05614118 0.1012439\n",
            " 0.25240895 0.07336163 0.11510789]\n",
            "Action probs after filtering: {0: 0.1895078, 1: 0.060279656, 2: 0.06688188, 3: 0.08506714, 4: 0.05614118, 5: 0.1012439, 7: 0.07336163}\n",
            "[DEBUG] Evaluated value: 0.22\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1895\n",
            "[DEBUG] Expanding action 1 with prob 0.0603\n",
            "[DEBUG] Expanding action 2 with prob 0.0669\n",
            "[DEBUG] Expanding action 3 with prob 0.0851\n",
            "[DEBUG] Expanding action 4 with prob 0.0561\n",
            "[DEBUG] Expanding action 5 with prob 0.1012\n",
            "[DEBUG] Expanding action 7 with prob 0.0734\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.22\n",
            "[DEBUG] Backpropagation - Node visit_count: 9, total_value: -0.69\n",
            "[DEBUG] Backpropagation - Node visit_count: 74, total_value: 5.77\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 75]\n",
            "Policy probs from model: [0.04666498 0.05965205 0.11498021 0.09617197 0.08661669 0.0727587\n",
            " 0.11778849 0.3595026  0.04586431]\n",
            "Action probs after filtering: {1: 0.059652045, 3: 0.09617197, 4: 0.08661669, 5: 0.0727587, 6: 0.117788486, 8: 0.045864314}\n",
            "[DEBUG] Evaluated value: -0.28\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0597\n",
            "[DEBUG] Expanding action 3 with prob 0.0962\n",
            "[DEBUG] Expanding action 4 with prob 0.0866\n",
            "[DEBUG] Expanding action 5 with prob 0.0728\n",
            "[DEBUG] Expanding action 6 with prob 0.1178\n",
            "[DEBUG] Expanding action 8 with prob 0.0459\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.28\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.42\n",
            "[DEBUG] Backpropagation - Node visit_count: 10, total_value: -0.42\n",
            "[DEBUG] Backpropagation - Node visit_count: 75, total_value: 6.05\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 76]\n",
            "Policy probs from model: [0.09201986 0.08765899 0.07199594 0.07922763 0.06380077 0.08300386\n",
            " 0.3319191  0.11443733 0.07593653]\n",
            "Action probs after filtering: {0: 0.09201986, 1: 0.08765899, 2: 0.071995944, 3: 0.079227634, 5: 0.083003856, 6: 0.3319191, 8: 0.07593653}\n",
            "[DEBUG] Evaluated value: 0.20\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0920\n",
            "[DEBUG] Expanding action 1 with prob 0.0877\n",
            "[DEBUG] Expanding action 2 with prob 0.0720\n",
            "[DEBUG] Expanding action 3 with prob 0.0792\n",
            "[DEBUG] Expanding action 5 with prob 0.0830\n",
            "[DEBUG] Expanding action 6 with prob 0.3319\n",
            "[DEBUG] Expanding action 8 with prob 0.0759\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.20\n",
            "[DEBUG] Backpropagation - Node visit_count: 6, total_value: -1.10\n",
            "[DEBUG] Backpropagation - Node visit_count: 76, total_value: 6.25\n",
            "[RESULT] Chosen action: 7\n",
            "\n",
            "[SIMULATION 77]\n",
            "Policy probs from model: [0.12937981 0.07163524 0.12357672 0.01646201 0.01901165 0.09544334\n",
            " 0.390403   0.07072276 0.08336552]\n",
            "Action probs after filtering: {0: 0.12937981, 2: 0.123576716, 3: 0.016462013, 4: 0.01901165, 5: 0.09544334, 8: 0.08336552}\n",
            "[DEBUG] Evaluated value: -0.29\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1294\n",
            "[DEBUG] Expanding action 2 with prob 0.1236\n",
            "[DEBUG] Expanding action 3 with prob 0.0165\n",
            "[DEBUG] Expanding action 4 with prob 0.0190\n",
            "[DEBUG] Expanding action 5 with prob 0.0954\n",
            "[DEBUG] Expanding action 8 with prob 0.0834\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 7, total_value: 0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 19, total_value: 0.85\n",
            "[DEBUG] Backpropagation - Node visit_count: 77, total_value: 6.54\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 78]\n",
            "Policy probs from model: [0.09989258 0.1136155  0.05138104 0.10186953 0.09059776 0.10489791\n",
            " 0.23527902 0.05054442 0.15192226]\n",
            "Action probs after filtering: {0: 0.09989258, 1: 0.1136155, 2: 0.051381044, 3: 0.10186953, 5: 0.10489791, 6: 0.23527902, 8: 0.15192226}\n",
            "[DEBUG] Evaluated value: 0.21\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0999\n",
            "[DEBUG] Expanding action 1 with prob 0.1136\n",
            "[DEBUG] Expanding action 2 with prob 0.0514\n",
            "[DEBUG] Expanding action 3 with prob 0.1019\n",
            "[DEBUG] Expanding action 5 with prob 0.1049\n",
            "[DEBUG] Expanding action 6 with prob 0.2353\n",
            "[DEBUG] Expanding action 8 with prob 0.1519\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.21\n",
            "[DEBUG] Backpropagation - Node visit_count: 8, total_value: -1.11\n",
            "[DEBUG] Backpropagation - Node visit_count: 78, total_value: 6.75\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 79]\n",
            "Policy probs from model: [0.00772158 0.21613261 0.1328835  0.11415238 0.11638278 0.07054264\n",
            " 0.0985798  0.10817876 0.13542594]\n",
            "Action probs after filtering: {1: 0.21613261, 2: 0.1328835, 3: 0.11415238, 4: 0.11638278, 5: 0.07054264, 7: 0.10817876}\n",
            "[DEBUG] Evaluated value: 0.22\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.2161\n",
            "[DEBUG] Expanding action 2 with prob 0.1329\n",
            "[DEBUG] Expanding action 3 with prob 0.1142\n",
            "[DEBUG] Expanding action 4 with prob 0.1164\n",
            "[DEBUG] Expanding action 5 with prob 0.0705\n",
            "[DEBUG] Expanding action 7 with prob 0.1082\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.22\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.00\n",
            "[DEBUG] Backpropagation - Node visit_count: 10, total_value: -0.47\n",
            "[DEBUG] Backpropagation - Node visit_count: 79, total_value: 6.53\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 80]\n",
            "Policy probs from model: [0.00291883 0.05968602 0.10930736 0.10136524 0.22076698 0.11179435\n",
            " 0.11357493 0.17476644 0.10581997]\n",
            "Action probs after filtering: {1: 0.059686024, 2: 0.109307356, 4: 0.22076698, 5: 0.11179435, 7: 0.17476644, 8: 0.10581997}\n",
            "[DEBUG] Evaluated value: 0.21\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0597\n",
            "[DEBUG] Expanding action 2 with prob 0.1093\n",
            "[DEBUG] Expanding action 4 with prob 0.2208\n",
            "[DEBUG] Expanding action 5 with prob 0.1118\n",
            "[DEBUG] Expanding action 7 with prob 0.1748\n",
            "[DEBUG] Expanding action 8 with prob 0.1058\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.21\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.16\n",
            "[DEBUG] Backpropagation - Node visit_count: 11, total_value: -0.26\n",
            "[DEBUG] Backpropagation - Node visit_count: 80, total_value: 6.33\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 81]\n",
            "Policy probs from model: [0.0121421  0.11657341 0.11359986 0.14846756 0.1650253  0.1741567\n",
            " 0.07558232 0.10617325 0.08827958]\n",
            "Action probs after filtering: {2: 0.11359986, 3: 0.14846756, 4: 0.1650253, 5: 0.1741567, 7: 0.106173255, 8: 0.088279575}\n",
            "[DEBUG] Evaluated value: -0.12\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.1136\n",
            "[DEBUG] Expanding action 3 with prob 0.1485\n",
            "[DEBUG] Expanding action 4 with prob 0.1650\n",
            "[DEBUG] Expanding action 5 with prob 0.1742\n",
            "[DEBUG] Expanding action 7 with prob 0.1062\n",
            "[DEBUG] Expanding action 8 with prob 0.0883\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.20\n",
            "[DEBUG] Backpropagation - Node visit_count: 12, total_value: -0.38\n",
            "[DEBUG] Backpropagation - Node visit_count: 81, total_value: 6.45\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 82]\n",
            "Policy probs from model: [0.05424747 0.14300609 0.15045968 0.08108733 0.12019506 0.10496417\n",
            " 0.10230421 0.08848017 0.15525573]\n",
            "Action probs after filtering: {1: 0.14300609, 2: 0.15045968, 4: 0.12019506, 5: 0.10496417, 6: 0.102304205, 7: 0.088480175}\n",
            "[DEBUG] Evaluated value: 0.24\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.1430\n",
            "[DEBUG] Expanding action 2 with prob 0.1505\n",
            "[DEBUG] Expanding action 4 with prob 0.1202\n",
            "[DEBUG] Expanding action 5 with prob 0.1050\n",
            "[DEBUG] Expanding action 6 with prob 0.1023\n",
            "[DEBUG] Expanding action 7 with prob 0.0885\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.24\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 10, total_value: -0.30\n",
            "[DEBUG] Backpropagation - Node visit_count: 82, total_value: 6.21\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 83]\n",
            "Policy probs from model: [0.20378311 0.06788362 0.06876236 0.09718071 0.13074546 0.12069652\n",
            " 0.11742716 0.07292014 0.1206009 ]\n",
            "Action probs after filtering: {1: 0.06788362, 2: 0.06876236, 4: 0.13074546, 5: 0.12069652, 7: 0.07292014, 8: 0.1206009}\n",
            "[DEBUG] Evaluated value: 0.09\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0679\n",
            "[DEBUG] Expanding action 2 with prob 0.0688\n",
            "[DEBUG] Expanding action 4 with prob 0.1307\n",
            "[DEBUG] Expanding action 5 with prob 0.1207\n",
            "[DEBUG] Expanding action 7 with prob 0.0729\n",
            "[DEBUG] Expanding action 8 with prob 0.1206\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.09\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.29\n",
            "[DEBUG] Backpropagation - Node visit_count: 11, total_value: -0.21\n",
            "[DEBUG] Backpropagation - Node visit_count: 83, total_value: 6.12\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 84]\n",
            "Policy probs from model: [0.12775439 0.12112556 0.10161485 0.08491898 0.05318752 0.2136149\n",
            " 0.11284534 0.07284512 0.1120934 ]\n",
            "Action probs after filtering: {1: 0.121125564, 2: 0.10161485, 4: 0.053187516, 6: 0.11284534, 7: 0.07284512, 8: 0.1120934}\n",
            "[DEBUG] Evaluated value: 0.54\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.1211\n",
            "[DEBUG] Expanding action 2 with prob 0.1016\n",
            "[DEBUG] Expanding action 4 with prob 0.0532\n",
            "[DEBUG] Expanding action 6 with prob 0.1128\n",
            "[DEBUG] Expanding action 7 with prob 0.0728\n",
            "[DEBUG] Expanding action 8 with prob 0.1121\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.54\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.69\n",
            "[DEBUG] Backpropagation - Node visit_count: 12, total_value: 0.33\n",
            "[DEBUG] Backpropagation - Node visit_count: 84, total_value: 5.58\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 85]\n",
            "Policy probs from model: [0.04942334 0.06303394 0.16920905 0.17534587 0.05308137 0.10628359\n",
            " 0.26197234 0.02680839 0.0948422 ]\n",
            "Action probs after filtering: {1: 0.06303394, 4: 0.05308137, 5: 0.10628359, 6: 0.26197234, 7: 0.026808394, 8: 0.094842196}\n",
            "[DEBUG] Evaluated value: -0.12\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0630\n",
            "[DEBUG] Expanding action 4 with prob 0.0531\n",
            "[DEBUG] Expanding action 5 with prob 0.1063\n",
            "[DEBUG] Expanding action 6 with prob 0.2620\n",
            "[DEBUG] Expanding action 7 with prob 0.0268\n",
            "[DEBUG] Expanding action 8 with prob 0.0948\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.42\n",
            "[DEBUG] Backpropagation - Node visit_count: 13, total_value: 0.21\n",
            "[DEBUG] Backpropagation - Node visit_count: 85, total_value: 5.71\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 86]\n",
            "Policy probs from model: [0.03770498 0.08039144 0.0797478  0.08986188 0.04403637 0.08262038\n",
            " 0.43314284 0.03100408 0.12149028]\n",
            "Action probs after filtering: {0: 0.037704982, 4: 0.044036366, 5: 0.08262038, 6: 0.43314284, 7: 0.031004077, 8: 0.12149028}\n",
            "[DEBUG] Evaluated value: 0.16\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0377\n",
            "[DEBUG] Expanding action 4 with prob 0.0440\n",
            "[DEBUG] Expanding action 5 with prob 0.0826\n",
            "[DEBUG] Expanding action 6 with prob 0.4331\n",
            "[DEBUG] Expanding action 7 with prob 0.0310\n",
            "[DEBUG] Expanding action 8 with prob 0.1215\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.16\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.27\n",
            "[DEBUG] Backpropagation - Node visit_count: 14, total_value: 0.37\n",
            "[DEBUG] Backpropagation - Node visit_count: 86, total_value: 5.55\n",
            "[RESULT] Chosen action: 7\n",
            "\n",
            "[SIMULATION 87]\n",
            "Policy probs from model: [0.03094994 0.17586426 0.11142373 0.02357246 0.07289398 0.14793777\n",
            " 0.21323477 0.06259449 0.16152856]\n",
            "Action probs after filtering: {0: 0.030949945, 2: 0.11142373, 4: 0.072893985, 5: 0.14793777, 6: 0.21323477, 7: 0.06259449}\n",
            "[DEBUG] Evaluated value: 0.28\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0309\n",
            "[DEBUG] Expanding action 2 with prob 0.1114\n",
            "[DEBUG] Expanding action 4 with prob 0.0729\n",
            "[DEBUG] Expanding action 5 with prob 0.1479\n",
            "[DEBUG] Expanding action 6 with prob 0.2132\n",
            "[DEBUG] Expanding action 7 with prob 0.0626\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.28\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.17\n",
            "[DEBUG] Backpropagation - Node visit_count: 15, total_value: 0.65\n",
            "[DEBUG] Backpropagation - Node visit_count: 87, total_value: 5.26\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 88]\n",
            "Policy probs from model: [0.15885602 0.1000349  0.11702038 0.03378357 0.035316   0.06385843\n",
            " 0.32553387 0.09319296 0.07240386]\n",
            "Action probs after filtering: {0: 0.15885602, 2: 0.11702038, 4: 0.035316, 5: 0.063858435, 7: 0.09319296, 8: 0.072403856}\n",
            "[DEBUG] Evaluated value: 0.19\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1589\n",
            "[DEBUG] Expanding action 2 with prob 0.1170\n",
            "[DEBUG] Expanding action 4 with prob 0.0353\n",
            "[DEBUG] Expanding action 5 with prob 0.0639\n",
            "[DEBUG] Expanding action 7 with prob 0.0932\n",
            "[DEBUG] Expanding action 8 with prob 0.0724\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.19\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.09\n",
            "[DEBUG] Backpropagation - Node visit_count: 16, total_value: 0.85\n",
            "[DEBUG] Backpropagation - Node visit_count: 88, total_value: 5.07\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 89]\n",
            "Policy probs from model: [0.04147263 0.05365101 0.16931967 0.10322319 0.1108689  0.29308614\n",
            " 0.08335439 0.06417383 0.08085028]\n",
            "Action probs after filtering: {2: 0.16931967, 4: 0.1108689, 5: 0.29308614, 6: 0.08335439, 7: 0.06417383, 8: 0.08085028}\n",
            "[DEBUG] Evaluated value: -0.13\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.1693\n",
            "[DEBUG] Expanding action 4 with prob 0.1109\n",
            "[DEBUG] Expanding action 5 with prob 0.2931\n",
            "[DEBUG] Expanding action 6 with prob 0.0834\n",
            "[DEBUG] Expanding action 7 with prob 0.0642\n",
            "[DEBUG] Expanding action 8 with prob 0.0809\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.18\n",
            "[DEBUG] Backpropagation - Node visit_count: 17, total_value: 0.72\n",
            "[DEBUG] Backpropagation - Node visit_count: 89, total_value: 5.20\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 90]\n",
            "Policy probs from model: [0.09937745 0.05385147 0.08966427 0.06766602 0.08705503 0.18695572\n",
            " 0.10564207 0.19297822 0.1168097 ]\n",
            "Action probs after filtering: {0: 0.09937745, 1: 0.053851474, 3: 0.067666024, 4: 0.08705503, 6: 0.105642065, 7: 0.19297822, 8: 0.1168097}\n",
            "[DEBUG] Evaluated value: 0.44\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0994\n",
            "[DEBUG] Expanding action 1 with prob 0.0539\n",
            "[DEBUG] Expanding action 3 with prob 0.0677\n",
            "[DEBUG] Expanding action 4 with prob 0.0871\n",
            "[DEBUG] Expanding action 6 with prob 0.1056\n",
            "[DEBUG] Expanding action 7 with prob 0.1930\n",
            "[DEBUG] Expanding action 8 with prob 0.1168\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.44\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -1.92\n",
            "[DEBUG] Backpropagation - Node visit_count: 90, total_value: 5.64\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 91]\n",
            "Policy probs from model: [0.01262327 0.02727605 0.08720984 0.06768294 0.0905652  0.21378592\n",
            " 0.2385152  0.13956538 0.12277627]\n",
            "Action probs after filtering: {1: 0.027276054, 2: 0.08720984, 4: 0.0905652, 5: 0.21378592, 6: 0.2385152, 7: 0.13956538}\n",
            "[DEBUG] Evaluated value: 0.19\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0273\n",
            "[DEBUG] Expanding action 2 with prob 0.0872\n",
            "[DEBUG] Expanding action 4 with prob 0.0906\n",
            "[DEBUG] Expanding action 5 with prob 0.2138\n",
            "[DEBUG] Expanding action 6 with prob 0.2385\n",
            "[DEBUG] Expanding action 7 with prob 0.1396\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.19\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.22\n",
            "[DEBUG] Backpropagation - Node visit_count: 10, total_value: -0.54\n",
            "[DEBUG] Backpropagation - Node visit_count: 91, total_value: 5.45\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 92]\n",
            "Policy probs from model: [0.07589622 0.06370117 0.10523951 0.05374203 0.03317778 0.23066548\n",
            " 0.25356627 0.06547303 0.1185385 ]\n",
            "Action probs after filtering: {1: 0.06370117, 2: 0.10523951, 3: 0.053742032, 4: 0.033177778, 6: 0.25356627, 7: 0.065473035}\n",
            "[DEBUG] Evaluated value: 0.67\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0637\n",
            "[DEBUG] Expanding action 2 with prob 0.1052\n",
            "[DEBUG] Expanding action 3 with prob 0.0537\n",
            "[DEBUG] Expanding action 4 with prob 0.0332\n",
            "[DEBUG] Expanding action 6 with prob 0.2536\n",
            "[DEBUG] Expanding action 7 with prob 0.0655\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.67\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.82\n",
            "[DEBUG] Backpropagation - Node visit_count: 11, total_value: 0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 92, total_value: 4.78\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 93]\n",
            "Policy probs from model: [0.03421193 0.05289302 0.06678935 0.07970184 0.07669836 0.43162236\n",
            " 0.10696861 0.04758337 0.10353115]\n",
            "Action probs after filtering: {2: 0.06678935, 3: 0.07970184, 4: 0.07669836, 5: 0.43162236, 6: 0.10696861, 7: 0.04758337}\n",
            "[DEBUG] Evaluated value: 0.08\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.0668\n",
            "[DEBUG] Expanding action 3 with prob 0.0797\n",
            "[DEBUG] Expanding action 4 with prob 0.0767\n",
            "[DEBUG] Expanding action 5 with prob 0.4316\n",
            "[DEBUG] Expanding action 6 with prob 0.1070\n",
            "[DEBUG] Expanding action 7 with prob 0.0476\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.08\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.23\n",
            "[DEBUG] Backpropagation - Node visit_count: 12, total_value: 0.20\n",
            "[DEBUG] Backpropagation - Node visit_count: 93, total_value: 4.70\n",
            "[RESULT] Chosen action: 2\n",
            "\n",
            "[SIMULATION 94]\n",
            "Policy probs from model: [0.1624808  0.04418407 0.05860028 0.08249897 0.06272066 0.08891649\n",
            " 0.3284569  0.08146168 0.09068009]\n",
            "Action probs after filtering: {1: 0.04418407, 2: 0.05860028, 3: 0.08249897, 4: 0.06272066, 5: 0.08891649, 7: 0.081461675}\n",
            "[DEBUG] Evaluated value: -0.13\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0442\n",
            "[DEBUG] Expanding action 2 with prob 0.0586\n",
            "[DEBUG] Expanding action 3 with prob 0.0825\n",
            "[DEBUG] Expanding action 4 with prob 0.0627\n",
            "[DEBUG] Expanding action 5 with prob 0.0889\n",
            "[DEBUG] Expanding action 7 with prob 0.0815\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: -0.13\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.08\n",
            "[DEBUG] Backpropagation - Node visit_count: 13, total_value: 0.08\n",
            "[DEBUG] Backpropagation - Node visit_count: 94, total_value: 4.83\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 95]\n",
            "Policy probs from model: [0.02680096 0.05783994 0.11272937 0.10530609 0.06591929 0.14028725\n",
            " 0.29901072 0.0585384  0.13356797]\n",
            "Action probs after filtering: {1: 0.057839945, 2: 0.11272937, 3: 0.10530609, 4: 0.065919295, 5: 0.14028725, 6: 0.29901072}\n",
            "[DEBUG] Evaluated value: 0.31\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0578\n",
            "[DEBUG] Expanding action 2 with prob 0.1127\n",
            "[DEBUG] Expanding action 3 with prob 0.1053\n",
            "[DEBUG] Expanding action 4 with prob 0.0659\n",
            "[DEBUG] Expanding action 5 with prob 0.1403\n",
            "[DEBUG] Expanding action 6 with prob 0.2990\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.31\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: 0.07\n",
            "[DEBUG] Backpropagation - Node visit_count: 14, total_value: 0.38\n",
            "[DEBUG] Backpropagation - Node visit_count: 95, total_value: 4.52\n",
            "[RESULT] Chosen action: 3\n",
            "\n",
            "[SIMULATION 96]\n",
            "Policy probs from model: [0.03573347 0.02932772 0.08035371 0.16766207 0.02335402 0.16490686\n",
            " 0.31229666 0.01389689 0.17246862]\n",
            "Action probs after filtering: {1: 0.029327717, 3: 0.16766207, 4: 0.02335402, 5: 0.16490686, 6: 0.31229666, 7: 0.01389689}\n",
            "[DEBUG] Evaluated value: 0.35\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 1 with prob 0.0293\n",
            "[DEBUG] Expanding action 3 with prob 0.1677\n",
            "[DEBUG] Expanding action 4 with prob 0.0234\n",
            "[DEBUG] Expanding action 5 with prob 0.1649\n",
            "[DEBUG] Expanding action 6 with prob 0.3123\n",
            "[DEBUG] Expanding action 7 with prob 0.0139\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.35\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.14\n",
            "[DEBUG] Backpropagation - Node visit_count: 15, total_value: 0.73\n",
            "[DEBUG] Backpropagation - Node visit_count: 96, total_value: 4.17\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 97]\n",
            "Policy probs from model: [0.11536633 0.05258634 0.10927254 0.04588828 0.02677523 0.04864502\n",
            " 0.44854468 0.063115   0.08980658]\n",
            "Action probs after filtering: {0: 0.11536633, 2: 0.10927254, 3: 0.045888282, 4: 0.026775232, 5: 0.048645016, 7: 0.063115}\n",
            "[DEBUG] Evaluated value: 0.12\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.1154\n",
            "[DEBUG] Expanding action 2 with prob 0.1093\n",
            "[DEBUG] Expanding action 3 with prob 0.0459\n",
            "[DEBUG] Expanding action 4 with prob 0.0268\n",
            "[DEBUG] Expanding action 5 with prob 0.0486\n",
            "[DEBUG] Expanding action 7 with prob 0.0631\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.12\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: -0.04\n",
            "[DEBUG] Backpropagation - Node visit_count: 16, total_value: 0.85\n",
            "[DEBUG] Backpropagation - Node visit_count: 97, total_value: 4.06\n",
            "[RESULT] Chosen action: 4\n",
            "\n",
            "[SIMULATION 98]\n",
            "Policy probs from model: [0.01971006 0.05195382 0.10470142 0.03312061 0.03889228 0.13140391\n",
            " 0.35031727 0.11817202 0.15172864]\n",
            "Action probs after filtering: {0: 0.019710064, 2: 0.10470142, 4: 0.038892284, 5: 0.13140391, 6: 0.35031727, 7: 0.11817202}\n",
            "[DEBUG] Evaluated value: 0.05\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.0197\n",
            "[DEBUG] Expanding action 2 with prob 0.1047\n",
            "[DEBUG] Expanding action 4 with prob 0.0389\n",
            "[DEBUG] Expanding action 5 with prob 0.1314\n",
            "[DEBUG] Expanding action 6 with prob 0.3503\n",
            "[DEBUG] Expanding action 7 with prob 0.1182\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.05\n",
            "[DEBUG] Backpropagation - Node visit_count: 3, total_value: 0.16\n",
            "[DEBUG] Backpropagation - Node visit_count: 17, total_value: 0.90\n",
            "[DEBUG] Backpropagation - Node visit_count: 98, total_value: 4.00\n",
            "[RESULT] Chosen action: 6\n",
            "\n",
            "[SIMULATION 99]\n",
            "Policy probs from model: [0.05813816 0.07661677 0.05451222 0.05567415 0.03918782 0.05102734\n",
            " 0.44837666 0.04686267 0.1696043 ]\n",
            "Action probs after filtering: {2: 0.054512218, 3: 0.055674147, 4: 0.039187822, 5: 0.051027335, 6: 0.44837666, 7: 0.046862666}\n",
            "[DEBUG] Evaluated value: 0.52\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 2 with prob 0.0545\n",
            "[DEBUG] Expanding action 3 with prob 0.0557\n",
            "[DEBUG] Expanding action 4 with prob 0.0392\n",
            "[DEBUG] Expanding action 5 with prob 0.0510\n",
            "[DEBUG] Expanding action 6 with prob 0.4484\n",
            "[DEBUG] Expanding action 7 with prob 0.0469\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.52\n",
            "[DEBUG] Backpropagation - Node visit_count: 2, total_value: -0.41\n",
            "[DEBUG] Backpropagation - Node visit_count: 18, total_value: 1.42\n",
            "[DEBUG] Backpropagation - Node visit_count: 99, total_value: 3.49\n",
            "[RESULT] Chosen action: 1\n",
            "\n",
            "[SIMULATION 100]\n",
            "Policy probs from model: [0.20494352 0.07237359 0.04163522 0.10512226 0.07295593 0.09455329\n",
            " 0.19306453 0.09999436 0.11535732]\n",
            "Action probs after filtering: {0: 0.20494352, 1: 0.07237359, 3: 0.10512226, 4: 0.07295593, 5: 0.09455329, 7: 0.09999436}\n",
            "[DEBUG] Evaluated value: 0.53\n",
            "[DEBUG] expand() called with policy_probs type: <class 'dict'>\n",
            "[DEBUG] Expanding action 0 with prob 0.2049\n",
            "[DEBUG] Expanding action 1 with prob 0.0724\n",
            "[DEBUG] Expanding action 3 with prob 0.1051\n",
            "[DEBUG] Expanding action 4 with prob 0.0730\n",
            "[DEBUG] Expanding action 5 with prob 0.0946\n",
            "[DEBUG] Expanding action 7 with prob 0.1000\n",
            "[DEBUG] Backpropagation - Node visit_count: 1, total_value: 0.53\n",
            "[DEBUG] Backpropagation - Node visit_count: 4, total_value: -0.58\n",
            "[DEBUG] Backpropagation - Node visit_count: 19, total_value: 1.96\n",
            "[DEBUG] Backpropagation - Node visit_count: 100, total_value: 2.95\n",
            "[RESULT] Chosen action: 1\n",
            "MCTS 실행 완료!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n",
            "초기 상태 보드:\n",
            "가능한 행동들: [0 1 2 3 4 5 6 7 8]\n",
            "각 행동의 평균 점수: {0: -0.721, 1: -0.751, 2: -0.756, 3: -0.769, 4: -0.792, 5: -0.758, 6: -0.772, 7: -0.764, 8: -0.764}\n",
            "몬테카를로 탐색이 선택한 최적 행동: 1\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "실행 중인 코드:\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "STATE_SIZE = (3,3)\n",
            "N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]\n",
            "STATE_DIM = 3 # first player 정보 넣음\n",
            "BOARD_SHAPE = (STATE_DIM, 3, 3)\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class Environment: #틱택토 게임 환경 정의 클래스 / 게임 규칙을 코드로 구현한 것\n",
            "    def __init__(self):\n",
            "        self.n = STATE_SIZE[0] #보드 행 또는 열 크기\n",
            "        self.num_actions = self.n ** 2\n",
            "        self.action_space = np.arange(self.num_actions)\n",
            "        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #결과에 따른 보상 정의\n",
            "        #승리 1점 / 패배 -1점 / 무승부 0점\n",
            "\n",
            "\n",
            "    def step(self, present_state, action_idx): #현재 상태에서 주어진 행동에 따라 게임 진행 / 행동 결과를 계산\n",
            "        \"\"\"\n",
            "        present_state에 대해 action_idx의 행동에 따라 게임을 한 턴 진행시키고\n",
            "        next_state, is_done, is_lose를 반환한다.\n",
            "        \"\"\"\n",
            "        #action_idx: 플레이어가 선택한 행동의 인덱스 / 보드 칸\n",
            "        #현재 상태에서 행동하는 행동을 수행하여 다음 상태를 계산\n",
            "        next_state = present_state.next(action_idx)\n",
            "        is_done, is_lose = next_state.check_done()\n",
            "        #next_state에서 게임이 종료되었는지 확인\n",
            "        #현재 플레이어가 패배했는지 확인\n",
            "\n",
            "        return next_state, is_done, is_lose\n",
            "\n",
            "\n",
            "    def get_reward(self, final_state): #게임 종료 후 보상 계산 / 승패 및 무승부 판정\n",
            "        \"\"\"\n",
            "        게임이 종료된 state에 대해 각 플레이어의 reward를 반환한다.\n",
            "        final_state: 게임이 종료된 state\n",
            "        note: final_state가 is_lose라면, 해당 state에서 행동할 차례였던 플레이어가 패배한 것.\n",
            "        \"\"\"\n",
            "        '''\n",
            "        장예원 수정!\n",
            "        '''\n",
            "        is_done, is_lose = final_state.check_done()  # 게임 종료 및 패배 여부 확인\n",
            "\n",
            "        # 승리, 패배, 무승부 정확하게 판별\n",
            "        if not is_done:\n",
            "            return 0  # 게임이 아직 끝나지 않았으면 보상을 주지 않음\n",
            "\n",
            "        if is_lose:\n",
            "            return self.reward_dict['lose']  # 패배 시 -1\n",
            "\n",
            "        if final_state.total_pieces_count() == self.num_actions:\n",
            "            return self.reward_dict['draw']  # 무승부 시 0\n",
            "\n",
            "        return self.reward_dict['win']  # 승리 시 +1\n",
            "\n",
            "\n",
            "    def render(self, state): #게임 상태 출력\n",
            "        '''\n",
            "        입력받은 state를 문자열로 출력한다.\n",
            "        X: first_player, O: second_player\n",
            "        '''\n",
            "        is_first_player = state.check_first_player() #현재 차례가 첫번째 플레이어인지 확인\n",
            "        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state\n",
            "        board = board.reshape(3,3) #현재 보드 상태를 정리\n",
            "\n",
            "        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]\n",
            "        #돌 상태를 문자로 변환 -> 읽기 쉽게 표시\n",
            "        #X: 첫번째 플레이어의 돌 / O: 두번째 플레이어의 돌 / .: 빈칸\n",
            "        formatted_board = \"\\n\".join([\" \".join(row) for row in board_list])\n",
            "        #보기 좋게 줄바꿈하여 출력\n",
            "        return formatted_board  #print() 대신 문자열 반환\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "import copy\n",
            "========================================\n",
            "실행 중인 코드:\n",
            "class State(Environment):\n",
            "    def __init__(self, state=None, enemy_state=None):\n",
            "        super().__init__()\n",
            "        self.state = state if state is not None else [0] * (self.n ** 2)\n",
            "        self.enemy_state = enemy_state if enemy_state is not None else [0] * (self.n ** 2)\n",
            "\n",
            "        self.state = np.array(self.state).reshape(STATE_SIZE)\n",
            "        self.enemy_state = np.array(self.enemy_state).reshape(STATE_SIZE)\n",
            "        self.move_count = 0 # 초기 상태에서 Player 1이 선공\n",
            "\n",
            "\n",
            "    def total_pieces_count(self):\n",
            "        '''\n",
            "        이 state의 전체 돌의 개수를 반환한다.\n",
            "        '''\n",
            "        total_state = self.state + self.enemy_state\n",
            "        return np.sum(total_state)\n",
            "\n",
            "\n",
            "    def get_legal_actions(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action을\n",
            "        one-hot encoding 형식의 array로 반환한다.\n",
            "        '''\n",
            "        total_state = (self.state + self.enemy_state).reshape(-1)\n",
            "        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)\n",
            "        return legal_actions\n",
            "\n",
            "\n",
            "    def check_done(self):\n",
            "        '''\n",
            "        이 state의 done, lose 여부를 반환한다.\n",
            "        note: 상대가 행동한 후, 자신의 행동을 하기 전 이 state를 확인한다.\n",
            "        따라서 이전 state에서 상대의 행동으로 상대가 이긴 경우는 이 state의 플레이어가 진 경우이다.\n",
            "        '''\n",
            "        is_done, is_lose = False, False\n",
            "\n",
            "        # Check draw\n",
            "        if self.total_pieces_count() == self.n ** 2:\n",
            "            is_done, is_lose = True, False\n",
            "\n",
            "        # Check lose\n",
            "        lose_condition = np.concatenate([self.enemy_state.sum(axis=0), self.enemy_state.sum(axis=1), [self.enemy_state.trace], [np.fliplr(self.enemy_state).trace()]])\n",
            "        if self.n in lose_condition:\n",
            "            is_done, is_lose = True, True\n",
            "\n",
            "        return is_done, is_lose\n",
            "\n",
            "\n",
            "    def next(self, action_idx):\n",
            "        '''\n",
            "        주어진 action에 따라 다음 state를 생성한다.\n",
            "        note: 다음 state는 상대의 차례이므로 state 순서를 바꾼다.\n",
            "        '''\n",
            "        x, y =np.divmod(action_idx, self.n)\n",
            "        state = self.state.copy()\n",
            "        state[x, y] = 1\n",
            "\n",
            "        state = list(state.reshape(-1))\n",
            "        enemy_state = list(copy.copy(self.enemy_state).reshape(-1))\n",
            "\n",
            "        next_state = State(enemy_state, state)  # state와 enemy_state를 바꿔서 전달\n",
            "        next_state.move_count = self.move_count + 1  # move_count 증가\n",
            "        return next_state\n",
            "\n",
            "\n",
            "    def check_first_player(self):\n",
            "        # State()를 기본적으로 생성하면 move_count = 0이므로 check_first_player()는 항상 True\n",
            "        return self.move_count % 2 == 0\n",
            "\n",
            "\n",
            "    def get_random_action(self):\n",
            "        '''\n",
            "        이 state에서 가능한 action 중 랜덤으로 action을 반환한다.\n",
            "        '''\n",
            "        legal_actions = self.get_legal_actions()\n",
            "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
            "        action = np.random.choice(legal_action_idxs)\n",
            "        return action\n",
            "\n",
            "\n",
            "    def __str__(self):\n",
            "        return Environment().render(self)  # state를 인자로 명확히 전달\n",
            "========================================\n",
            "초기 상태 보드:\n",
            "가능한 행동들: [0 1 2 3 4 5 6 7 8]\n",
            "AlphaBeta 탐색이 선택한 최적 행동: 0\n",
            "테스트 상태 보드:\n",
            "가능한 행동들: [0 4 6 7 8]\n",
            "테스트 상태에서 AlphaBeta 선택 행동: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlphaZero VS MCTS (추가)"
      ],
      "metadata": {
        "id": "8V_97gX84Vqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import os\n",
        "    import torch\n",
        "\n",
        "    # 필요한 상수 정의 (필요한 경우 기존 코드에서 가져옴)\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    N_RESIDUAL_BLOCK = 16\n",
        "    N_KERNEL = 128\n",
        "    STATE_SIZE = (3, 3)\n",
        "    N_ACTIONS = 9\n",
        "\n",
        "    # 결과 디렉토리 생성\n",
        "    results_dir = os.path.dirname('./results/validation_results.txt')\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # 디렉토리 내 모든 체크포인트 파일 검색\n",
        "    model_dir = '/content/drive/My Drive/3-2/강화'\n",
        "    checkpoint_files = [f for f in os.listdir(model_dir) if f.startswith('model_env_checkpoint_epoch_') and f.endswith('.pth')]\n",
        "\n",
        "    # 에포크 번호로 정렬하여 최신 파일 선택\n",
        "    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    latest_checkpoint = checkpoint_files[-1]\n",
        "\n",
        "    # 최신 체크포인트 파일 경로\n",
        "    model_path = os.path.join(model_dir, latest_checkpoint)\n",
        "    print(f\"가장 최신 체크포인트 파일: {model_path}\")\n",
        "\n",
        "    # AlphaZero 모델 초기화\n",
        "    alphazero_model = Network(N_RESIDUAL_BLOCK, N_KERNEL, STATE_SIZE, N_ACTIONS)\n",
        "\n",
        "    # 모델을 DataParallel로 래핑\n",
        "    alphazero_model = torch.nn.DataParallel(alphazero_model).to(DEVICE)\n",
        "\n",
        "    # 저장된 모델 가중치 로드\n",
        "    #현재 초기화된 모델에 맞게 동기화하려면, 현재 모델의 구조와 키를 확인해야함\n",
        "    params = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    # 현재 모델의 키 가져오기\n",
        "    model_keys = alphazero_model.module.state_dict().keys()\n",
        "\n",
        "    # 저장된 가중치의 키와 현재 모델 키를 동기화\n",
        "    if any(k.startswith(\"module.\") for k in model_keys) and not any(k.startswith(\"module.\") for k in params.keys()):\n",
        "        # 현재 모델 키에 \"module.\"이 있으나 저장된 키에 없을 경우\n",
        "        params = {f\"module.{k}\": v for k, v in params.items()}\n",
        "    elif not any(k.startswith(\"module.\") for k in model_keys) and any(k.startswith(\"module.\") for k in params.keys()):\n",
        "        # 현재 모델 키에 \"module.\"이 없으나 저장된 키에 있을 경우\n",
        "        params = {k.replace(\"module.\", \"\"): v for k, v in params.items()}\n",
        "\n",
        "    # 가중치 로드\n",
        "    alphazero_model.load_state_dict(params, strict=False)  # 키 불일치 허용\n",
        "\n",
        "    # AlphaZeroAgent 초기화 및 평가 설정\n",
        "    alphazero_agent = AlphaZeroAgent(\n",
        "        model_path=model_path,         # 학습된 모델 불러오기\n",
        "        eval_game_count=30,            # 평가 게임 수\n",
        "        pv_evaluate_count=100,         # MCTS 탐색 횟수 (대국 성능에 영향)\n",
        "        env=env,                       # 게임 환경 정보\n",
        "        sp_game_count=0,               # Self-play 게임 없음\n",
        "        learning_rate=0.01,            # 학습률 기본값 유지\n",
        "        batch_size=64,                 # 배치 크기 기본값 유지\n",
        "        epochs=30,                     # 학습 없음\n",
        "        learn_epoch=0                  # 학습 없음\n",
        "    )\n",
        "\n",
        "    alphazero_agent.selfplay(execute_self_play=False)  # self-play 비활성화\n",
        "\n",
        "    # TicTacToeEvaluator 초기화\n",
        "    evaluator = TicTacToeEvaluator(input_shape=(3, 3), hidden_units=128).to(DEVICE)\n",
        "\n",
        "    # 에이전트 딕셔너리 생성\n",
        "    agents = {\n",
        "        'AlphaZero': alphazero_agent,\n",
        "        'MCTS': MCTSAgent(policy_model=None)\n",
        "    }\n",
        "\n",
        "    # BestModelAnalysisWithPersistence 사용 (CSV 저장)\n",
        "    analysis = BestModelAnalysisWithPersistence(\n",
        "        agents=agents,\n",
        "        results_path='./results/validation_results.txt',\n",
        "        csv_path='./results/validation_results.csv',\n",
        "        ep_game_count=10\n",
        "    )\n",
        "\n",
        "    # 에포크별 평가 실행 (기존 데이터 체크 후 실행)\n",
        "    for epoch in range(0, 51, 10):\n",
        "        analysis.validate_model(epoch)\n",
        "\n",
        "    print('모든 검증 완료')\n",
        "    print(\"대국 결과가 CSV에 저장되었습니다.\")\n",
        "\n",
        "    # **최종 결과 출력 (CSV에서 불러와서 출력)**\n",
        "    print(\"\\n=== 최종 대국 결과 ===\")\n",
        "    cached_results = analysis.parse_results_from_csv()\n",
        "    for epoch, results in cached_results.items():\n",
        "        print(f'\\nEpoch {epoch} 결과:')\n",
        "        for opponent, values in results.items():\n",
        "            print(f'VS {opponent} -> '\n",
        "                  f'Win Rate: {values[\"win_rate\"]:.2%}, '\n",
        "                  f'Draw Rate: {values[\"draw_rate\"]:.2%}, '\n",
        "                  f'Lose Rate: {values[\"lose_rate\"]:.2%}, '\n",
        "                  f'Avg Reward: {values[\"average_reward\"]:.3f}, '\n",
        "                  f'Avg Duration: {values[\"average_duration\"]:.2f}s')\n",
        "\n",
        "    print(\"\\n모든 검증 과정이 완료되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYtt5OQMBaFQ",
        "outputId": "f4384292-932b-49eb-8d9f-906ef30a1d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가장 최신 체크포인트 파일: /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_17.pth\n",
            "Self-play 실행이 비활성화되었습니다.\n",
            "\n",
            "Epoch 0 - 모델 검증 시작\n",
            "\\nEvaluate VS_MCTS 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-179-ad0c0404f63e>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  params = torch.load(model_path, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 2/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 3/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 4/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 5/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 6/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Move 10: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 7/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 8/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 9/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 10/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Move 10: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MCTS: Avg Reward: 0.000 | Duration: 2.72s\n",
            "Wins: 5, Draws: 0, Losses: 5\n",
            "[DEBUG] Saving to CSV - Epoch 0, Opponent: MCTS, Win Rate: 50.00%, Draw Rate: 0.00%, Lose Rate: 50.00%\n",
            "Epoch 0 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 0 결과 저장 완료. 현재 CSV 데이터 개수: 1 행\n",
            "\n",
            "\n",
            "Epoch 10 - 모델 검증 시작\n",
            "\\nEvaluate VS_MCTS 1/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_MCTS 2/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 3/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 5/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 6/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_MCTS 7/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 8/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 9/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 10/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MCTS: Avg Reward: 0.000 | Duration: 2.77s\n",
            "Wins: 5, Draws: 0, Losses: 5\n",
            "[DEBUG] Saving to CSV - Epoch 10, Opponent: MCTS, Win Rate: 50.00%, Draw Rate: 0.00%, Lose Rate: 50.00%\n",
            "Epoch 10 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 10 결과 저장 완료. 현재 CSV 데이터 개수: 2 행\n",
            "\n",
            "\n",
            "Epoch 20 - 모델 검증 시작\n",
            "\\nEvaluate VS_MCTS 1/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 2/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Move 10: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 3/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 4/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 5/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 7/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Move 10: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCTS 9/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 10/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Move 10: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MCTS: Avg Reward: 0.100 | Duration: 2.64s\n",
            "Wins: 5, Draws: 1, Losses: 4\n",
            "[DEBUG] Saving to CSV - Epoch 20, Opponent: MCTS, Win Rate: 50.00%, Draw Rate: 10.00%, Lose Rate: 40.00%\n",
            "Epoch 20 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 20 결과 저장 완료. 현재 CSV 데이터 개수: 3 행\n",
            "\n",
            "\n",
            "Epoch 30 - 모델 검증 시작\n",
            "\\nEvaluate VS_MCTS 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 2/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 3/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 4/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 5/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 6/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_MCTS 7/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 9/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 10/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "VS_MCTS: Avg Reward: 0.400 | Duration: 2.56s\n",
            "Wins: 7, Draws: 0, Losses: 3\n",
            "[DEBUG] Saving to CSV - Epoch 30, Opponent: MCTS, Win Rate: 70.00%, Draw Rate: 0.00%, Lose Rate: 30.00%\n",
            "Epoch 30 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 30 결과 저장 완료. 현재 CSV 데이터 개수: 4 행\n",
            "\n",
            "\n",
            "Epoch 40 - 모델 검증 시작\n",
            "\\nEvaluate VS_MCTS 1/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 2/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 3/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 4/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Move 10: Action 3, Reward: 0\n",
            "[DEBUG] Move 11: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_MCTS 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 6/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 7/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 8/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 9/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 10/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "VS_MCTS: Avg Reward: 0.400 | Duration: 3.23s\n",
            "Wins: 7, Draws: 0, Losses: 3\n",
            "[DEBUG] Saving to CSV - Epoch 40, Opponent: MCTS, Win Rate: 70.00%, Draw Rate: 0.00%, Lose Rate: 30.00%\n",
            "Epoch 40 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 40 결과 저장 완료. 현재 CSV 데이터 개수: 5 행\n",
            "\n",
            "\n",
            "Epoch 50 - 모델 검증 시작\n",
            "\\nEvaluate VS_MCTS 1/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 2/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 3/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 4/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_MCTS 5/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 6/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCTS 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 8/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCTS 9/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCTS 10/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 2, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCTS: Avg Reward: 0.400 | Duration: 2.59s\n",
            "Wins: 6, Draws: 2, Losses: 2\n",
            "[DEBUG] Saving to CSV - Epoch 50, Opponent: MCTS, Win Rate: 60.00%, Draw Rate: 20.00%, Lose Rate: 20.00%\n",
            "Epoch 50 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 50 결과 저장 완료. 현재 CSV 데이터 개수: 6 행\n",
            "\n",
            "모든 검증 완료\n",
            "대국 결과가 CSV에 저장되었습니다.\n",
            "\n",
            "=== 최종 대국 결과 ===\n",
            "\n",
            "Epoch 0 결과:\n",
            "VS VS_MCTS -> Win Rate: 50.00%, Draw Rate: 0.00%, Lose Rate: 50.00%, Avg Reward: 0.000, Avg Duration: 2.72s\n",
            "\n",
            "Epoch 10 결과:\n",
            "VS VS_MCTS -> Win Rate: 50.00%, Draw Rate: 0.00%, Lose Rate: 50.00%, Avg Reward: 0.000, Avg Duration: 2.77s\n",
            "\n",
            "Epoch 20 결과:\n",
            "VS VS_MCTS -> Win Rate: 50.00%, Draw Rate: 10.00%, Lose Rate: 40.00%, Avg Reward: 0.100, Avg Duration: 2.64s\n",
            "\n",
            "Epoch 30 결과:\n",
            "VS VS_MCTS -> Win Rate: 70.00%, Draw Rate: 0.00%, Lose Rate: 30.00%, Avg Reward: 0.400, Avg Duration: 2.56s\n",
            "\n",
            "Epoch 40 결과:\n",
            "VS VS_MCTS -> Win Rate: 70.00%, Draw Rate: 0.00%, Lose Rate: 30.00%, Avg Reward: 0.400, Avg Duration: 3.23s\n",
            "\n",
            "Epoch 50 결과:\n",
            "VS VS_MCTS -> Win Rate: 60.00%, Draw Rate: 20.00%, Lose Rate: 20.00%, Avg Reward: 0.400, Avg Duration: 2.59s\n",
            "\n",
            "모든 검증 과정이 완료되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlphaZero vs 나머지 3개 에이전트"
      ],
      "metadata": {
        "id": "2mEbviMn4Zbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import os\n",
        "    import torch\n",
        "\n",
        "    # 필요한 상수 정의 (필요한 경우 기존 코드에서 가져옴)\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    N_RESIDUAL_BLOCK = 16\n",
        "    N_KERNEL = 128\n",
        "    STATE_SIZE = (3, 3)\n",
        "    N_ACTIONS = 9\n",
        "\n",
        "    # 결과 디렉토리 생성\n",
        "    results_dir = os.path.dirname('./results/validation_results.txt')\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # 디렉토리 내 모든 체크포인트 파일 검색\n",
        "    model_dir = '/content/drive/My Drive/3-2/강화'\n",
        "    checkpoint_files = [f for f in os.listdir(model_dir) if f.startswith('model_env_checkpoint_epoch_') and f.endswith('.pth')]\n",
        "\n",
        "    # 에포크 번호로 정렬하여 최신 파일 선택\n",
        "    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    latest_checkpoint = checkpoint_files[-1]\n",
        "\n",
        "    # 최신 체크포인트 파일 경로\n",
        "    model_path = os.path.join(model_dir, latest_checkpoint)\n",
        "    print(f\"가장 최신 체크포인트 파일: {model_path}\")\n",
        "\n",
        "    # AlphaZero 모델 초기화\n",
        "    alphazero_model = Network(N_RESIDUAL_BLOCK, N_KERNEL, STATE_SIZE, N_ACTIONS)\n",
        "\n",
        "    # 모델을 DataParallel로 래핑\n",
        "    alphazero_model = torch.nn.DataParallel(alphazero_model).to(DEVICE)\n",
        "\n",
        "    # 저장된 모델 가중치 로드\n",
        "    #현재 초기화된 모델에 맞게 동기화하려면, 현재 모델의 구조와 키를 확인해야함\n",
        "    params = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    # 현재 모델의 키 가져오기\n",
        "    model_keys = alphazero_model.module.state_dict().keys()\n",
        "\n",
        "    # 저장된 가중치의 키와 현재 모델 키를 동기화\n",
        "    if any(k.startswith(\"module.\") for k in model_keys) and not any(k.startswith(\"module.\") for k in params.keys()):\n",
        "        # 현재 모델 키에 \"module.\"이 있으나 저장된 키에 없을 경우\n",
        "        params = {f\"module.{k}\": v for k, v in params.items()}\n",
        "    elif not any(k.startswith(\"module.\") for k in model_keys) and any(k.startswith(\"module.\") for k in params.keys()):\n",
        "        # 현재 모델 키에 \"module.\"이 없으나 저장된 키에 있을 경우\n",
        "        params = {k.replace(\"module.\", \"\"): v for k, v in params.items()}\n",
        "\n",
        "    # 가중치 로드\n",
        "    alphazero_model.load_state_dict(params, strict=False)  # 키 불일치 허용\n",
        "\n",
        "    # AlphaZeroAgent 초기화 및 평가 설정\n",
        "    alphazero_agent = AlphaZeroAgent(\n",
        "        model_path=model_path,         # 학습된 모델 불러오기\n",
        "        eval_game_count=30,            # 평가 게임 수\n",
        "        pv_evaluate_count=100,         # MCTS 탐색 횟수 (대국 성능에 영향)\n",
        "        env=env,                       # 게임 환경 정보\n",
        "        sp_game_count=0,               # Self-play 게임 없음\n",
        "        learning_rate=0.01,            # 학습률 기본값 유지\n",
        "        batch_size=64,                 # 배치 크기 기본값 유지\n",
        "        epochs=30,                     # 학습 없음\n",
        "        learn_epoch=0                  # 학습 없음\n",
        "    )\n",
        "\n",
        "    alphazero_agent.selfplay(execute_self_play=False)  # self-play 비활성화\n",
        "\n",
        "    # TicTacToeEvaluator 초기화\n",
        "    evaluator = TicTacToeEvaluator(input_shape=(3, 3), hidden_units=128).to(DEVICE)\n",
        "\n",
        "    # 에이전트 딕셔너리 생성\n",
        "    agents = {\n",
        "        'AlphaZero': alphazero_agent,\n",
        "        'MinMax': MinMaxAgent(evaluator=evaluator, environment=env),\n",
        "        'MCS': MCSAgent(),\n",
        "        'AlphaBeta': AlphaBetaAgent()\n",
        "    }\n",
        "\n",
        "    # BestModelAnalysisWithPersistence 사용 (CSV 저장)\n",
        "    analysis = BestModelAnalysisWithPersistence(\n",
        "        agents=agents,\n",
        "        results_path='./results/validation_results.txt',\n",
        "        csv_path='./results/validation_results.csv',\n",
        "        ep_game_count=10\n",
        "    )\n",
        "\n",
        "    # 에포크별 평가 실행 (기존 데이터 체크 후 실행)\n",
        "    for epoch in range(0, 51, 10):\n",
        "        analysis.validate_model(epoch)\n",
        "\n",
        "    print('모든 검증 완료')\n",
        "    print(\"대국 결과가 CSV에 저장되었습니다.\")\n",
        "\n",
        "    # **최종 결과 출력 (CSV에서 불러와서 출력)**\n",
        "    print(\"\\n=== 최종 대국 결과 ===\")\n",
        "    cached_results = analysis.parse_results_from_csv()\n",
        "    for epoch, results in cached_results.items():\n",
        "        print(f'\\nEpoch {epoch} 결과:')\n",
        "        for opponent, values in results.items():\n",
        "            print(f'VS {opponent} -> '\n",
        "                  f'Win Rate: {values[\"win_rate\"]:.2%}, '\n",
        "                  f'Draw Rate: {values[\"draw_rate\"]:.2%}, '\n",
        "                  f'Lose Rate: {values[\"lose_rate\"]:.2%}, '\n",
        "                  f'Avg Reward: {values[\"average_reward\"]:.3f}, '\n",
        "                  f'Avg Duration: {values[\"average_duration\"]:.2f}s')\n",
        "\n",
        "    print(\"\\n모든 검증 과정이 완료되었습니다!\")"
      ],
      "metadata": {
        "id": "7Xm0EKCcHLue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77fb54b-57ae-483c-ebfc-f63553a66387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가장 최신 체크포인트 파일: /content/drive/My Drive/3-2/강화/model_env_checkpoint_epoch_17.pth\n",
            "Self-play 실행이 비활성화되었습니다.\n",
            "\n",
            "Epoch 0 - 모델 검증 시작\n",
            "\\nEvaluate VS_MinMax 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-182-903dc5b19e58>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  params = torch.load(model_path, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 3/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 5/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 7/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 9/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MinMax: Avg Reward: -0.100 | Duration: 2.76s\n",
            "Wins: 4, Draws: 1, Losses: 5\n",
            "\\nEvaluate VS_MCS 1/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 2/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCS 3/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 4/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 6/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 7/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 8/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 2, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 10/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCS: Avg Reward: -0.100 | Duration: 16.20s\n",
            "Wins: 0, Draws: 9, Losses: 1\n",
            "\\nEvaluate VS_AlphaBeta 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 3/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 7/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_AlphaBeta: Avg Reward: -0.100 | Duration: 2.64s\n",
            "Wins: 3, Draws: 3, Losses: 4\n",
            "[DEBUG] Saving to CSV - Epoch 0, Opponent: MinMax, Win Rate: 40.00%, Draw Rate: 10.00%, Lose Rate: 50.00%\n",
            "[DEBUG] Saving to CSV - Epoch 0, Opponent: MCS, Win Rate: 0.00%, Draw Rate: 90.00%, Lose Rate: 10.00%\n",
            "[DEBUG] Saving to CSV - Epoch 0, Opponent: AlphaBeta, Win Rate: 30.00%, Draw Rate: 30.00%, Lose Rate: 40.00%\n",
            "Epoch 0 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 0 결과 저장 완료. 현재 CSV 데이터 개수: 9 행\n",
            "\n",
            "\n",
            "Epoch 10 - 모델 검증 시작\n",
            "\\nEvaluate VS_MinMax 1/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 3/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 5/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MinMax: Avg Reward: 0.000 | Duration: 2.74s\n",
            "Wins: 4, Draws: 2, Losses: 4\n",
            "\\nEvaluate VS_MCS 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 2/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 3/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCS 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 6/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCS 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 9/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 10/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCS: Avg Reward: -0.200 | Duration: 15.84s\n",
            "Wins: 0, Draws: 8, Losses: 2\n",
            "\\nEvaluate VS_AlphaBeta 1/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 3/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 5/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 7/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_AlphaBeta: Avg Reward: -0.200 | Duration: 2.67s\n",
            "Wins: 1, Draws: 6, Losses: 3\n",
            "[DEBUG] Saving to CSV - Epoch 10, Opponent: MinMax, Win Rate: 40.00%, Draw Rate: 20.00%, Lose Rate: 40.00%\n",
            "[DEBUG] Saving to CSV - Epoch 10, Opponent: MCS, Win Rate: 0.00%, Draw Rate: 80.00%, Lose Rate: 20.00%\n",
            "[DEBUG] Saving to CSV - Epoch 10, Opponent: AlphaBeta, Win Rate: 10.00%, Draw Rate: 60.00%, Lose Rate: 30.00%\n",
            "Epoch 10 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 10 결과 저장 완료. 현재 CSV 데이터 개수: 12 행\n",
            "\n",
            "\n",
            "Epoch 20 - 모델 검증 시작\n",
            "\\nEvaluate VS_MinMax 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 3/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_MinMax 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 7/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 9/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MinMax: Avg Reward: 0.000 | Duration: 2.70s\n",
            "Wins: 4, Draws: 2, Losses: 4\n",
            "\\nEvaluate VS_MCS 1/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 3/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 2, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 4/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 5/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 6/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 8/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 9/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 10/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 2, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCS: Avg Reward: 0.000 | Duration: 16.40s\n",
            "Wins: 0, Draws: 10, Losses: 0\n",
            "\\nEvaluate VS_AlphaBeta 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 3/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 5/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 9/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_AlphaBeta: Avg Reward: 0.200 | Duration: 2.55s\n",
            "Wins: 3, Draws: 6, Losses: 1\n",
            "[DEBUG] Saving to CSV - Epoch 20, Opponent: MinMax, Win Rate: 40.00%, Draw Rate: 20.00%, Lose Rate: 40.00%\n",
            "[DEBUG] Saving to CSV - Epoch 20, Opponent: MCS, Win Rate: 0.00%, Draw Rate: 100.00%, Lose Rate: 0.00%\n",
            "[DEBUG] Saving to CSV - Epoch 20, Opponent: AlphaBeta, Win Rate: 30.00%, Draw Rate: 60.00%, Lose Rate: 10.00%\n",
            "Epoch 20 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 20 결과 저장 완료. 현재 CSV 데이터 개수: 15 행\n",
            "\n",
            "\n",
            "Epoch 30 - 모델 검증 시작\n",
            "\\nEvaluate VS_MinMax 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_MinMax 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 3/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 7/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 9/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MinMax: Avg Reward: 0.200 | Duration: 2.89s\n",
            "Wins: 4, Draws: 4, Losses: 2\n",
            "\\nEvaluate VS_MCS 1/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 2/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 3/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_MCS 4/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCS 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 6/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 8/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 9/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MCS 10/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCS: Avg Reward: -0.100 | Duration: 15.90s\n",
            "Wins: 1, Draws: 7, Losses: 2\n",
            "\\nEvaluate VS_AlphaBeta 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 3/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 9/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_AlphaBeta: Avg Reward: -0.100 | Duration: 2.53s\n",
            "Wins: 2, Draws: 5, Losses: 3\n",
            "[DEBUG] Saving to CSV - Epoch 30, Opponent: MinMax, Win Rate: 40.00%, Draw Rate: 40.00%, Lose Rate: 20.00%\n",
            "[DEBUG] Saving to CSV - Epoch 30, Opponent: MCS, Win Rate: 10.00%, Draw Rate: 70.00%, Lose Rate: 20.00%\n",
            "[DEBUG] Saving to CSV - Epoch 30, Opponent: AlphaBeta, Win Rate: 20.00%, Draw Rate: 50.00%, Lose Rate: 30.00%\n",
            "Epoch 30 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 30 결과 저장 완료. 현재 CSV 데이터 개수: 18 행\n",
            "\n",
            "\n",
            "Epoch 40 - 모델 검증 시작\n",
            "\\nEvaluate VS_MinMax 1/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 5, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 3/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 9/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MinMax: Avg Reward: -0.100 | Duration: 2.65s\n",
            "Wins: 3, Draws: 3, Losses: 4\n",
            "\\nEvaluate VS_MCS 1/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 0, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 2/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 3/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 4/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCS 5/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 6/10\n",
            "[DEBUG] Move 1: Action 6, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MCS 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 8/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 0, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 2, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 9/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 10/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCS: Avg Reward: -0.200 | Duration: 16.04s\n",
            "Wins: 0, Draws: 8, Losses: 2\n",
            "\\nEvaluate VS_AlphaBeta 1/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 3/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 3, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 5/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 7/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 3, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_AlphaBeta: Avg Reward: -0.500 | Duration: 2.44s\n",
            "Wins: 1, Draws: 3, Losses: 6\n",
            "[DEBUG] Saving to CSV - Epoch 40, Opponent: MinMax, Win Rate: 30.00%, Draw Rate: 30.00%, Lose Rate: 40.00%\n",
            "[DEBUG] Saving to CSV - Epoch 40, Opponent: MCS, Win Rate: 0.00%, Draw Rate: 80.00%, Lose Rate: 20.00%\n",
            "[DEBUG] Saving to CSV - Epoch 40, Opponent: AlphaBeta, Win Rate: 10.00%, Draw Rate: 30.00%, Lose Rate: 60.00%\n",
            "Epoch 40 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 40 결과 저장 완료. 현재 CSV 데이터 개수: 21 행\n",
            "\n",
            "\n",
            "Epoch 50 - 모델 검증 시작\n",
            "\\nEvaluate VS_MinMax 1/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 5, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 3/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 8, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_MinMax 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MinMax 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 6, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 2, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MinMax 9/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 8, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_MinMax 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "VS_MinMax: Avg Reward: 0.100 | Duration: 2.72s\n",
            "Wins: 3, Draws: 5, Losses: 2\n",
            "\\nEvaluate VS_MCS 1/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 8, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 2/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 3/10\n",
            "[DEBUG] Move 1: Action 7, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 1, Reward: 0\n",
            "[DEBUG] Move 6: Action 0, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 2, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 4/10\n",
            "[DEBUG] Move 1: Action 1, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 3, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 8, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 5/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 5, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 6/10\n",
            "[DEBUG] Move 1: Action 3, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 1, Reward: 0\n",
            "[DEBUG] Move 5: Action 7, Reward: 0\n",
            "[DEBUG] Move 6: Action 5, Reward: 0\n",
            "[DEBUG] Move 7: Action 4, Reward: 0\n",
            "[DEBUG] Move 8: Action 0, Reward: 0\n",
            "[DEBUG] Move 9: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=False\n",
            "[DEBUG] AlphaZero WIN (is_first_player=False, reward=1)\n",
            "\\nEvaluate VS_MCS 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 5, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 0, Reward: 0\n",
            "[DEBUG] Move 8: Action 1, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=True\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=True, reward=-1)\n",
            "\\nEvaluate VS_MCS 8/10\n",
            "[DEBUG] Move 1: Action 5, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 2, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 8, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_MCS 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 7, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 4, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 6, Reward: 0\n",
            "[DEBUG] Move 9: Action 0, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_MCS 10/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 6, Reward: 0\n",
            "[DEBUG] Move 3: Action 4, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 0, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 7, Reward: 0\n",
            "[DEBUG] Move 9: Action 1, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_MCS: Avg Reward: 0.000 | Duration: 16.28s\n",
            "Wins: 1, Draws: 8, Losses: 1\n",
            "\\nEvaluate VS_AlphaBeta 1/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 2/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 8, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 6, Reward: 0\n",
            "[DEBUG] Move 7: Action 3, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: -1, is_first_player=True\n",
            "[DEBUG] Adjusted reward after processing: -1, is_first_player=False\n",
            "[DEBUG] AlphaZero LOSE (is_first_player=False, reward=-1)\n",
            "\\nEvaluate VS_AlphaBeta 3/10\n",
            "[DEBUG] Move 1: Action 2, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 3, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 4, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 4/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 4, Reward: 0\n",
            "[DEBUG] Move 9: Action 3, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 5/10\n",
            "[DEBUG] Move 1: Action 4, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 7, Reward: 0\n",
            "[DEBUG] Move 5: Action 2, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 6, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 6/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 2, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 4, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 7/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 7, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 1, Reward: 0\n",
            "[DEBUG] Move 7: Action 2, Reward: -1\n",
            "[DEBUG] Final reward before return: -1, Adjusted: 1, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 1, is_first_player=True\n",
            "[DEBUG] AlphaZero WIN (is_first_player=True, reward=1)\n",
            "\\nEvaluate VS_AlphaBeta 8/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 4, Reward: 0\n",
            "[DEBUG] Move 3: Action 1, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 3, Reward: 0\n",
            "[DEBUG] Move 7: Action 5, Reward: 0\n",
            "[DEBUG] Move 8: Action 8, Reward: 0\n",
            "[DEBUG] Move 9: Action 7, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 9/10\n",
            "[DEBUG] Move 1: Action 8, Reward: 0\n",
            "[DEBUG] Move 2: Action 0, Reward: 0\n",
            "[DEBUG] Move 3: Action 5, Reward: 0\n",
            "[DEBUG] Move 4: Action 2, Reward: 0\n",
            "[DEBUG] Move 5: Action 6, Reward: 0\n",
            "[DEBUG] Move 6: Action 7, Reward: 0\n",
            "[DEBUG] Move 7: Action 1, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=True\n",
            "[DEBUG] DRAW (is_first_player=True, reward=0)\n",
            "\\nEvaluate VS_AlphaBeta 10/10\n",
            "[DEBUG] Move 1: Action 0, Reward: 0\n",
            "[DEBUG] Move 2: Action 1, Reward: 0\n",
            "[DEBUG] Move 3: Action 2, Reward: 0\n",
            "[DEBUG] Move 4: Action 6, Reward: 0\n",
            "[DEBUG] Move 5: Action 5, Reward: 0\n",
            "[DEBUG] Move 6: Action 8, Reward: 0\n",
            "[DEBUG] Move 7: Action 7, Reward: 0\n",
            "[DEBUG] Move 8: Action 3, Reward: 0\n",
            "[DEBUG] Move 9: Action 4, Reward: 0\n",
            "[DEBUG] Final reward before return: 0, Adjusted: 0, is_first_player=False\n",
            "[DEBUG] Adjusted reward after processing: 0, is_first_player=False\n",
            "[DEBUG] DRAW (is_first_player=False, reward=0)\n",
            "VS_AlphaBeta: Avg Reward: 0.200 | Duration: 2.58s\n",
            "Wins: 3, Draws: 6, Losses: 1\n",
            "[DEBUG] Saving to CSV - Epoch 50, Opponent: MinMax, Win Rate: 30.00%, Draw Rate: 50.00%, Lose Rate: 20.00%\n",
            "[DEBUG] Saving to CSV - Epoch 50, Opponent: MCS, Win Rate: 10.00%, Draw Rate: 80.00%, Lose Rate: 10.00%\n",
            "[DEBUG] Saving to CSV - Epoch 50, Opponent: AlphaBeta, Win Rate: 30.00%, Draw Rate: 60.00%, Lose Rate: 10.00%\n",
            "Epoch 50 결과가 CSV 파일에 저장되었습니다: ./results/validation_results.csv\n",
            "Epoch 50 결과 저장 완료. 현재 CSV 데이터 개수: 24 행\n",
            "\n",
            "모든 검증 완료\n",
            "대국 결과가 CSV에 저장되었습니다.\n",
            "\n",
            "=== 최종 대국 결과 ===\n",
            "\n",
            "Epoch 0 결과:\n",
            "VS VS_MCTS -> Win Rate: 50.00%, Draw Rate: 0.00%, Lose Rate: 50.00%, Avg Reward: 0.000, Avg Duration: 2.72s\n",
            "VS VS_MinMax -> Win Rate: 40.00%, Draw Rate: 10.00%, Lose Rate: 50.00%, Avg Reward: -0.100, Avg Duration: 2.76s\n",
            "VS VS_MCS -> Win Rate: 0.00%, Draw Rate: 90.00%, Lose Rate: 10.00%, Avg Reward: -0.100, Avg Duration: 16.20s\n",
            "VS VS_AlphaBeta -> Win Rate: 30.00%, Draw Rate: 30.00%, Lose Rate: 40.00%, Avg Reward: -0.100, Avg Duration: 2.64s\n",
            "\n",
            "Epoch 10 결과:\n",
            "VS VS_MCTS -> Win Rate: 50.00%, Draw Rate: 0.00%, Lose Rate: 50.00%, Avg Reward: 0.000, Avg Duration: 2.77s\n",
            "VS VS_MinMax -> Win Rate: 40.00%, Draw Rate: 20.00%, Lose Rate: 40.00%, Avg Reward: 0.000, Avg Duration: 2.74s\n",
            "VS VS_MCS -> Win Rate: 0.00%, Draw Rate: 80.00%, Lose Rate: 20.00%, Avg Reward: -0.200, Avg Duration: 15.84s\n",
            "VS VS_AlphaBeta -> Win Rate: 10.00%, Draw Rate: 60.00%, Lose Rate: 30.00%, Avg Reward: -0.200, Avg Duration: 2.67s\n",
            "\n",
            "Epoch 20 결과:\n",
            "VS VS_MCTS -> Win Rate: 50.00%, Draw Rate: 10.00%, Lose Rate: 40.00%, Avg Reward: 0.100, Avg Duration: 2.64s\n",
            "VS VS_MinMax -> Win Rate: 40.00%, Draw Rate: 20.00%, Lose Rate: 40.00%, Avg Reward: 0.000, Avg Duration: 2.70s\n",
            "VS VS_MCS -> Win Rate: 0.00%, Draw Rate: 100.00%, Lose Rate: 0.00%, Avg Reward: 0.000, Avg Duration: 16.40s\n",
            "VS VS_AlphaBeta -> Win Rate: 30.00%, Draw Rate: 60.00%, Lose Rate: 10.00%, Avg Reward: 0.200, Avg Duration: 2.55s\n",
            "\n",
            "Epoch 30 결과:\n",
            "VS VS_MCTS -> Win Rate: 70.00%, Draw Rate: 0.00%, Lose Rate: 30.00%, Avg Reward: 0.400, Avg Duration: 2.56s\n",
            "VS VS_MinMax -> Win Rate: 40.00%, Draw Rate: 40.00%, Lose Rate: 20.00%, Avg Reward: 0.200, Avg Duration: 2.89s\n",
            "VS VS_MCS -> Win Rate: 10.00%, Draw Rate: 70.00%, Lose Rate: 20.00%, Avg Reward: -0.100, Avg Duration: 15.90s\n",
            "VS VS_AlphaBeta -> Win Rate: 20.00%, Draw Rate: 50.00%, Lose Rate: 30.00%, Avg Reward: -0.100, Avg Duration: 2.53s\n",
            "\n",
            "Epoch 40 결과:\n",
            "VS VS_MCTS -> Win Rate: 70.00%, Draw Rate: 0.00%, Lose Rate: 30.00%, Avg Reward: 0.400, Avg Duration: 3.23s\n",
            "VS VS_MinMax -> Win Rate: 30.00%, Draw Rate: 30.00%, Lose Rate: 40.00%, Avg Reward: -0.100, Avg Duration: 2.65s\n",
            "VS VS_MCS -> Win Rate: 0.00%, Draw Rate: 80.00%, Lose Rate: 20.00%, Avg Reward: -0.200, Avg Duration: 16.04s\n",
            "VS VS_AlphaBeta -> Win Rate: 10.00%, Draw Rate: 30.00%, Lose Rate: 60.00%, Avg Reward: -0.500, Avg Duration: 2.44s\n",
            "\n",
            "Epoch 50 결과:\n",
            "VS VS_MCTS -> Win Rate: 60.00%, Draw Rate: 20.00%, Lose Rate: 20.00%, Avg Reward: 0.400, Avg Duration: 2.59s\n",
            "VS VS_MinMax -> Win Rate: 30.00%, Draw Rate: 50.00%, Lose Rate: 20.00%, Avg Reward: 0.100, Avg Duration: 2.72s\n",
            "VS VS_MCS -> Win Rate: 10.00%, Draw Rate: 80.00%, Lose Rate: 10.00%, Avg Reward: 0.000, Avg Duration: 16.28s\n",
            "VS VS_AlphaBeta -> Win Rate: 30.00%, Draw Rate: 60.00%, Lose Rate: 10.00%, Avg Reward: 0.200, Avg Duration: 2.58s\n",
            "\n",
            "모든 검증 과정이 완료되었습니다!\n"
          ]
        }
      ]
    }
  ]
}