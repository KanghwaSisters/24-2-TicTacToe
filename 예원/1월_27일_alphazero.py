# -*- coding: utf-8 -*-
"""1ì›”_27ì¼_AlphaZero.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f5fy8a4DwH7N6T1H3qLUYUfS7pXqYk0f

# Import
"""

from google.colab import drive
drive.mount('/content/drive')

# useful library
import pandas as pd
import numpy as np
import random
from math import sqrt
import matplotlib.pyplot as plt

# torch stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

"""# 00 Game Info"""

STATE_SIZE = (3,3)
N_ACTIONS = STATE_SIZE[0]*STATE_SIZE[1]
STATE_DIM = 3 # first player ì •ë³´ ë„£ìŒ
BOARD_SHAPE = (STATE_DIM, 3, 3)



"""# 01 HYPER PARAMS
#### PV_EVALUATE_COUNT: MCTS ì„±ëŠ¥ì„ í–¥ìƒ
#### EVAL_COUNT: í‰ê°€ì˜ ì‹ ë¢°ë„ í–¥ìƒ
"""

# ì‹ ê²½ë§ êµ¬ì¡° ê´€ë ¨
N_KERNEL = 64 # í•©ì„±ê³± ë ˆì´ì–´(ConvLayer)ì˜ í•„í„° ìˆ˜ / í´ìˆ˜ë¡ ë” ë§ì€ íŒ¨í„´ì„ í•™ìŠµ
N_RESIDUAL_BLOCK = 16 # Residual Block ê°œìˆ˜ / í´ìˆ˜ë¡ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ

# í•™ìŠµ ê´€ë ¨
BATCH_SIZE = 128 # í•œ ë²ˆì˜ í•™ìŠµì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜
EPOCHS = 50 # ì „ì²´ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ -> ëŠ˜ë ¤ë³´ì!
LEARNING_RATE = 0.001 # í•™ìŠµë¥  (ë‰´ëŸ° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì†ë„)
LEARN_EPOCH = 100 # í•™ìŠµë¥  ê°ì†Œ ì£¼ê¸°

# Self-play ê´€ë ¨ -> í•œë²ˆ selfplay ëŒë¦° í›„ í•™ìŠµ
SP_GAME_COUNT = 20  # self-playì—ì„œ ìƒì„±í•  ê²Œì„ ìˆ˜
SP_TEMPERATURE = 1.5  # self-playì—ì„œ íƒìƒ‰ ë‹¤ì–‘ì„±ì„ ì¡°ì ˆí•˜ëŠ” ì˜¨ë„ íŒŒë¼ë¯¸í„°

# í‰ê°€ ê´€ë ¨
EVAL_COUNT = 100  # í‰ê°€í•  ë•Œ MCTS íƒìƒ‰ì„ ëª‡ ë²ˆ ìˆ˜í–‰í• ì§€ ê²°ì •
PV_EVALUATE_COUNT = 800 # MCTSê°€ ì–¼ë§ˆë‚˜ ê¹Šì´ íƒìƒ‰í• ì§€ë¥¼ ê²°ì • -> 400ìœ¼ë¡œ ìˆ˜ì •
EVAL_GAME_COUNT = 20  # í‰ê°€ 1íšŒ ë‹¹ ê²Œì„ ìˆ˜(ì˜¤ë¦¬ì§€ë„: 400)
EVAL_TEMPERATURE = 1.0  # ì˜¨ë„ íŒŒë¼ë¯¸í„°

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

"""# Env"""

class Environment: #í‹±íƒí†  ê²Œì„ í™˜ê²½ ì •ì˜ í´ë˜ìŠ¤ / ê²Œì„ ê·œì¹™ì„ ì½”ë“œë¡œ êµ¬í˜„í•œ ê²ƒ
    def __init__(self):
        self.n = STATE_SIZE[0] #ë³´ë“œ í–‰ ë˜ëŠ” ì—´ í¬ê¸°
        self.num_actions = self.n ** 2
        self.action_space = np.arange(self.num_actions)
        self.reward_dict = {'win': 1, 'lose': -1, 'draw': 0} #ê²°ê³¼ì— ë”°ë¥¸ ë³´ìƒ ì •ì˜
        #ìŠ¹ë¦¬ 1ì  / íŒ¨ë°° -1ì  / ë¬´ìŠ¹ë¶€ 0ì 


    def step(self, present_state, action_idx): #í˜„ì¬ ìƒíƒœì—ì„œ ì£¼ì–´ì§„ í–‰ë™ì— ë”°ë¼ ê²Œì„ ì§„í–‰ / í–‰ë™ ê²°ê³¼ë¥¼ ê³„ì‚°
        """
        present_stateì— ëŒ€í•´ action_idxì˜ í–‰ë™ì— ë”°ë¼ ê²Œì„ì„ í•œ í„´ ì§„í–‰ì‹œí‚¤ê³ 
        next_state, is_done, is_loseë¥¼ ë°˜í™˜í•œë‹¤.
        """
        #action_idx: í”Œë ˆì´ì–´ê°€ ì„ íƒí•œ í–‰ë™ì˜ ì¸ë±ìŠ¤ / ë³´ë“œ ì¹¸
        #í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™í•˜ëŠ” í–‰ë™ì„ ìˆ˜í–‰í•˜ì—¬ ë‹¤ìŒ ìƒíƒœë¥¼ ê³„ì‚°
        next_state = present_state.next(action_idx)
        is_done, is_lose = next_state.check_done()
        #next_stateì—ì„œ ê²Œì„ì´ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        #í˜„ì¬ í”Œë ˆì´ì–´ê°€ íŒ¨ë°°í–ˆëŠ”ì§€ í™•ì¸

        return next_state, is_done, is_lose


    def get_reward(self, final_state): #ê²Œì„ ì¢…ë£Œ í›„ ë³´ìƒ ê³„ì‚° / ìŠ¹íŒ¨ ë° ë¬´ìŠ¹ë¶€ íŒì •
        """
        ê²Œì„ì´ ì¢…ë£Œëœ stateì— ëŒ€í•´ ê° í”Œë ˆì´ì–´ì˜ rewardë¥¼ ë°˜í™˜í•œë‹¤.
        final_state: ê²Œì„ì´ ì¢…ë£Œëœ state
        note: final_stateê°€ is_loseë¼ë©´, í•´ë‹¹ stateì—ì„œ í–‰ë™í•  ì°¨ë¡€ì˜€ë˜ í”Œë ˆì´ì–´ê°€ íŒ¨ë°°í•œ ê²ƒ.
        """
        '''
        ì¥ì˜ˆì› ìˆ˜ì •!
        '''
        is_done, is_lose = final_state.check_done()  # ê²Œì„ ì¢…ë£Œ ë° íŒ¨ë°° ì—¬ë¶€ í™•ì¸

        # ìŠ¹ë¦¬, íŒ¨ë°°, ë¬´ìŠ¹ë¶€ ì •í™•í•˜ê²Œ íŒë³„
        if not is_done:
            return 0  # ê²Œì„ì´ ì•„ì§ ëë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ë³´ìƒì„ ì£¼ì§€ ì•ŠìŒ

        # âœ… ë¨¼ì €, ë¬´ìŠ¹ë¶€ì¸ì§€ í™•ì¸
        if not is_lose and final_state.total_pieces_count() == self.num_actions:
            return self.reward_dict['draw']  # ë¬´ìŠ¹ë¶€ ì‹œ 0

        # âœ… ìŠ¹íŒ¨ ë³´ìƒì„ ë°˜ì˜í•˜ë˜, ì„ ê³µ/í›„ê³µì´ ë™ì¼í•œ ê¸°ì¤€ì„ ê°–ë„ë¡ ì„¤ì •
        reward = self.reward_dict['lose'] if is_lose else self.reward_dict['win']

        # âœ… í˜„ì¬ stateì˜ í”Œë ˆì´ì–´ê°€ ì„ ê³µì¸ì§€ í›„ê³µì¸ì§€ì— ë”°ë¼ ë³´ìƒ ë°˜ì „
        return reward if final_state.check_first_player() else -reward


    def render(self, state): #ê²Œì„ ìƒíƒœ ì¶œë ¥
        '''
        ì…ë ¥ë°›ì€ stateë¥¼ ë¬¸ìì—´ë¡œ ì¶œë ¥í•œë‹¤.
        X: first_player, O: second_player
        '''
        is_first_player = state.check_first_player() #í˜„ì¬ ì°¨ë¡€ê°€ ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì¸ì§€ í™•ì¸
        board = state.state - state.enemy_state if is_first_player else state.enemy_state - state.state
        board = board.reshape(3,3) #í˜„ì¬ ë³´ë“œ ìƒíƒœë¥¼ ì •ë¦¬

        board_list = [['X' if cell == 1 else 'O' if cell == -1 else '.' for cell in row] for row in board]
        #ëŒ ìƒíƒœë¥¼ ë¬¸ìë¡œ ë³€í™˜ -> ì½ê¸° ì‰½ê²Œ í‘œì‹œ
        #X: ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ëŒ / O: ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ëŒ / .: ë¹ˆì¹¸
        formatted_board = "\n".join([" ".join(row) for row in board_list])
        #ë³´ê¸° ì¢‹ê²Œ ì¤„ë°”ê¿ˆí•˜ì—¬ ì¶œë ¥
        return formatted_board  #print() ëŒ€ì‹  ë¬¸ìì—´ ë°˜í™˜

import copy

class State(Environment):
    def __init__(self, state=None, enemy_state=None, move_count=0):
        super().__init__()
        self.state = np.array(state if state is not None else np.zeros(STATE_SIZE, dtype=int))
        self.enemy_state = np.array(enemy_state if enemy_state is not None else np.zeros(STATE_SIZE, dtype=int))
        self.move_count = move_count  # âœ… move_countë¥¼ ì¸ìë¡œ ë°›ì•„ ëª…í™•í•˜ê²Œ ì„¤ì •


    def total_pieces_count(self):
        '''
        ì´ stateì˜ ì „ì²´ ëŒì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤.
        '''
        total_state = self.state + self.enemy_state
        return np.sum(total_state)


    def get_legal_actions(self):
        '''
        ì´ stateì—ì„œ ê°€ëŠ¥í•œ actionì„
        one-hot encoding í˜•ì‹ì˜ arrayë¡œ ë°˜í™˜í•œë‹¤.
        '''
        total_state = (self.state + self.enemy_state).reshape(-1)
        legal_actions = np.array([total_state[x] == 0 for x in self.action_space], dtype = int)
        return legal_actions


    def check_done(self):
        '''
        ì´ stateì˜ done, lose ì—¬ë¶€ë¥¼ ë°˜í™˜í•œë‹¤.
        note: ìƒëŒ€ê°€ í–‰ë™í•œ í›„, ìì‹ ì˜ í–‰ë™ì„ í•˜ê¸° ì „ ì´ stateë¥¼ í™•ì¸í•œë‹¤.
        ë”°ë¼ì„œ ì´ì „ stateì—ì„œ ìƒëŒ€ì˜ í–‰ë™ìœ¼ë¡œ ìƒëŒ€ê°€ ì´ê¸´ ê²½ìš°ëŠ” ì´ stateì˜ í”Œë ˆì´ì–´ê°€ ì§„ ê²½ìš°ì´ë‹¤.
        '''
        is_done, is_lose = False, False

        # Check lose
        lose_condition = np.concatenate([
            self.enemy_state.sum(axis=0),
            self.enemy_state.sum(axis=1),
            [np.trace(self.enemy_state)],  # ì™¼ìª½ ìœ„ â†’ ì˜¤ë¥¸ìª½ ì•„ë˜ ëŒ€ê°ì„  ê²€ì‚¬
            [np.trace(np.fliplr(self.enemy_state))]  # ì˜¤ë¥¸ìª½ ìœ„ â†’ ì™¼ìª½ ì•„ë˜ ëŒ€ê°ì„  ê²€ì‚¬
        ])

        if np.any(lose_condition == self.n):  # í™•ì‹¤íˆ 3ì´ ìˆëŠ”ì§€ ì²´í¬
            is_done, is_lose = True, True

        # Check draw (ìŠ¹ë¦¬ ì¡°ê±´ì´ ì—†ì„ ê²½ìš°ë§Œ ë¬´ìŠ¹ë¶€ í™•ì¸)
        if not is_lose and self.total_pieces_count() == self.n ** 2:
            is_done, is_lose = True, False

        # ë””ë²„ê¹… ì¶œë ¥
        #print(f"\n[DEBUG] check_done() í˜¸ì¶œë¨")
        #print(f"[DEBUG] Current Board:\n{self.state - self.enemy_state}")  # í˜„ì¬ ë³´ë“œ ìƒíƒœ ì¶œë ¥
        #print(f"[DEBUG] Lose condition: {lose_condition}")  # lose_condition ê°’ ì¶œë ¥
        #print(f"[DEBUG] is_done: {is_done}, is_lose: {is_lose}")  # ì¢…ë£Œ ì—¬ë¶€ ì¶œë ¥

        return is_done, is_lose


    def next(self, action_idx):
        '''
        ì£¼ì–´ì§„ actionì— ë”°ë¼ ë‹¤ìŒ stateë¥¼ ìƒì„±í•œë‹¤.
        note: ë‹¤ìŒ stateëŠ” ìƒëŒ€ì˜ ì°¨ë¡€ì´ë¯€ë¡œ state ìˆœì„œë¥¼ ë°”ê¾¼ë‹¤.
        '''
        x, y =np.divmod(action_idx, self.n)
        state = self.state.copy()
        state[x, y] = 1
        enemy_state = self.enemy_state.copy()

        # âœ… move_countë¥¼ ì •í™•íˆ 1 ì¦ê°€
        new_move_count = self.move_count + 1

        next_state = State(enemy_state, state, new_move_count)
        return next_state


    def check_first_player(self):
        # State()ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ìƒì„±í•˜ë©´ move_count = 0ì´ë¯€ë¡œ check_first_player()ëŠ” í•­ìƒ True
        return self.move_count % 2 == 0


    def get_random_action(self):
        '''
        ì´ stateì—ì„œ ê°€ëŠ¥í•œ action ì¤‘ ëœë¤ìœ¼ë¡œ actionì„ ë°˜í™˜í•œë‹¤.
        '''
        legal_actions = self.get_legal_actions()
        legal_action_idxs = np.where(legal_actions != 0)[0]
        action = np.random.choice(legal_action_idxs)
        return action


    def __str__(self):
        return Environment().render(self)  # stateë¥¼ ì¸ìë¡œ ëª…í™•íˆ ì „ë‹¬

"""# 03 Network

## Dual Network

### ConvLayer (ë³´ë“œ ê³µê°„ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìŠ¹ë¦¬ / ë°©ì–´ ì¡°ê±´ì„ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´ì„ ìë™ìœ¼ë¡œ ê°ì§€)
"""

class ConvLayer(nn.Module):
    def __init__(self, state_dim, n_kernel, kernel_size=3, padding=1, dropout_rate=0.0):
        #ì…ë ¥ ì±„ë„ ìˆ˜
        #ì¶œë ¥ ì±„ë„ ìˆ˜
        #í•„í„° í¬ê¸° (3X3)
        #íŒ¨ë”©: ì…ë ¥ê³¼ ì¶œë ¥ í¬ê¸° ë™ì¼í•˜ê²Œ ìœ ì§€
        #ë°”ì´ì–´ìŠ¤ í•­ ìƒëµ (ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš© ì‹œ ë°”ì´ì–´ìŠ¤ í•­ì„ ìƒëµí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì )
        #ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        super().__init__()
        self.conv = nn.Conv2d(3, n_kernel, kernel_size=kernel_size, padding=padding, bias=False)
        self.bn = nn.BatchNorm2d(n_kernel) #ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©í•  ì±„ë„ ìˆ˜
        self.relu = nn.ReLU(inplace=True)  # in-place ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€
        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None

    def forward(self, x):
        x = self.conv(x) #ì…ë ¥ ë°ì´í„°ì— í•©ì„±ê³± ì—°ì‚°ì„ ì ìš©í•˜ì—¬ íŠ¹ì§•ë§µ ìƒì„±
        x = self.bn(x) #ë°°ì¹˜ ì •ê·œí™” ì ìš©
        x = self.relu(x) #ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ì ìš©
        if self.dropout:  # ë“œë¡­ì•„ì›ƒ ì ìš© (í•„ìš”ì‹œ)
            x = self.dropout(x)
        return x

"""### Residual Block
##### 1. ë°°ì¹˜ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„±)ì™€ ReLU í™œì„±í™” í•¨ìˆ˜ (ë¹„ì„ í˜•ì„±)ë¥¼ ì‚¬ìš© -> ì¼ë°˜ CNN ë™ì¼
##### 2. ì”ì°¨ ì—°ê²°ì„ ì¶”ê°€í•˜ì—¬ ê¸°ì¡´ CNN êµ¬ì¡°ë¥¼ í™•ì¥ -> ì¶œë ¥ê°’ì´ ì•„ë‹Œ ì”ì°¨ë¥¼ í•™ìŠµ
##### 3. ê¹Šì€ ì‹ ê²½ë§ì—ì„œë„ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™” -> ì¼ë°˜ CNN ë¬¸ì œì  ì™„í™”
"""

class ResidualBlock(nn.Module):
    def __init__(self, n_kernel, kernel_size=3, padding=1, dropout_rate=0.0):
        super().__init__()
        self.conv1 = nn.Conv2d(n_kernel, n_kernel, kernel_size=kernel_size, padding=padding, bias=False)
        #í•©ì„±ê³± ì—°ì‚° ìˆ˜í–‰
        #residual blockì€ ì…ë ¥ê³¼ ì¶œë ¥ ì±„ë„ì´ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ì œì•½ì„ ë”°ë¥¸ë‹¤~
        self.bn1 = nn.BatchNorm2d(n_kernel) #ë°°ì¹˜ ì •ê·œí™” ì ìš©
        self.conv2 = nn.Conv2d(n_kernel, n_kernel, kernel_size=kernel_size, padding=padding, bias=False)
        self.bn2 = nn.BatchNorm2d(n_kernel)
        self.relu = nn.ReLU(inplace=True)  # í™œì„±í™” í•¨ìˆ˜ ê°ì²´í™”
        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None


    def forward(self, x):
        residual = x #ì…ë ¥ì„ ì”ì°¨ë¡œ ì €ì¥

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x) # double check
        if self.dropout:  # ë“œë¡­ì•„ì›ƒ ì ìš©
            x = self.dropout(x)
        x = self.conv2(x)
        x = self.bn2(x)

        x += residual
        #ëª¨ë¸ì´ ì”ì°¨ë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ì—¬ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™”
        x = self.relu(x)

        return x

"""### Network -> Alphazero ìŠ¤íƒ€ì¼ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸
#### ì •ì±…(Policy)ê³¼ ê°’(Value)ì„ ë™ì‹œì— ì¶œë ¥í•˜ëŠ” ì‹ ê²½ë§ì„ ì‚¬ìš©!
##### 1. ì •ì±…ë§: í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ë“¤ì— ëŒ€í•´ í™•ë¥  ë¶„í¬ë¥¼ ë°˜í™˜
##### 2. ê°€ì¹˜ë§: v=1 ë‚´ê°€ ë°˜ë“œì‹œ ìŠ¹ë¦¬í•  ìƒíƒœ / v=-1 ìƒëŒ€ë°©ì´ ë°˜ë“œì‹œ ìŠ¹ë¦¬í•  ìƒíƒœ
"""

class Network(nn.Module):
    def __init__(self, n_residual_block, n_kernel, input_shape, n_actions, dropout_rate=0.0):
        super().__init__()
        self.n_residual_block = n_residual_block #residual blockë¥¼ ìŒ“ì•„ ë„¤íŠ¸ì›Œí¬ ê¹Šì´ë¥¼ ì¡°ì •
        self.n_kernel = n_kernel #í•©ì„±ê³± ë ˆì´ì–´ì˜ í•„í„° ìˆ˜
        self.input_shape = input_shape #ì…ë ¥ ë°ì´í„°ì˜ í¬ê¸° ex)í‹±íƒí†  (3X3X3)
        self.n_actions = n_actions #ê¸¸ì´ ê³„ì‚° -> ê°€ëŠ¥í•œ í–‰ë™ì˜ ì´ ê°œìˆ˜ë¥¼ ì €ì¥ ex)í‹±íƒí†  9ê°œ
        self.n_fc_nodes = n_kernel  # Match to the number of kernels

        # nn
        self.conv_layer = ConvLayer(self.input_shape[0], self.n_kernel) # input_shape[0] = 3
        self.residual_blocks = nn.Sequential() #ëª¨ë“ˆì„ ì°¨ë¡€ë¡œ ì‹¤í–‰
        for i in range(self.n_residual_block):
            self.residual_blocks.add_module(f"residual_block_{i+1}", ResidualBlock(self.n_kernel))
        #ê° residual blockì— ì´ë¦„ì„ ë¶€ì—¬í•˜ì—¬ ì¶”ê°€
        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        #ê³µê°„ ì°¨ì›ì„ ì¶•ì†Œí•´ ê° ì±„ë„ë³„ë¡œ í•˜ë‚˜ì˜ í‰ê· ê°’ì„ ë°˜í™˜
        # heads and softmax ftn for policy
        self.policy_head = nn.Sequential(
            nn.Linear(self.n_fc_nodes, self.n_actions),
            nn.Softmax(dim=-1),
        )
        #fully connected layerë¥¼ í†µí•´ í–‰ë™ë³„ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚° / ì´ í–‰ë™ì„ ì„ íƒí–ˆì„ ë•Œ ìœ ë¦¬í•  ê°€ëŠ¥ì„±
        #self.n_fc_nodes: ê° ì¶œë ¥ ë…¸ë“œ ê°’ (íŠ¹ì • í–‰ë™ ì„ íƒ í™•ë¥ ) / self.n_actions: ë³´ë“œ ì¹¸ ìˆ˜ (9)
        #p = [0.1, 0.3, 0.2, ..., 0.4]
        #policy headì˜ ì¶œë ¥ê°’ì„ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
        self.value_head = nn.Sequential(
            nn.Linear(self.n_fc_nodes, 1),
            nn.Tanh(),
            )
        #fully connected layerë¥¼ í†µí•´ í˜„ì¬ ìƒíƒœì˜ ê°€ì¹˜ë¥¼ í‰ê°€ / í˜„ì¬ ìƒíƒœì˜ ìŠ¹ë¦¬ ê°€ëŠ¥ì„±
        #v = 0.8
        #value headì˜ ì¶œë ¥ê°’ì„ [-1,1]ë¡œ ì œí•œ

        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None


    def forward(self, x):
        x = self.conv_layer(x)
        x = self.residual_blocks(x)
        x = self.global_pooling(x) #conv -> residual blocks -> global poolingì„ ê±°ì³ 1Dë¡œ ë³€í™˜
        x = x.view(x.size(0), -1)  # Flatten

        if self.dropout:
            x = self.dropout(x)  # Optional Dropout

        p = self.policy_head(x)  # í–‰ë™ë³„ í™•ë¥  ë¶„í¬
        v = self.value_head(x)  # í˜„ì¬ ìƒíƒœì˜ ê°€ì¹˜ í‰ê°€

        return p, v

"""## Test Code"""

N_RESIDUAL_BLOCK = 16
N_KERNEL = 64
N_ACTIONS = 9
INPUT_SHAPE = (3, 3, 3)  # (ì±„ë„, ë†’ì´, ë„ˆë¹„)

model = Network(N_RESIDUAL_BLOCK, N_KERNEL, INPUT_SHAPE, N_ACTIONS, dropout_rate=0.1) #ë°°ì¹˜ í¬ê¸°, ì±„ë„ ìˆ˜, ë†’ì´, ë„ˆë¹„
#ì•ŒíŒŒì œë¡œì˜ ì •ì±… ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬

# example
x = torch.randn((2, STATE_DIM, 3, 3), requires_grad=True)

# policy and value
p, v = model(x) #ì…ë ¥ í…ì„œë¥¼ Network ëª¨ë¸ì— í†µê³¼ì‹œì¼œ ì •ì±… pì™€ ê°€ì¹˜ vë¥¼ ê³„ì‚°
p, v = p.detach().numpy(), v.detach().numpy()

print(f"ì •ì±…ì˜ ì¶œë ¥ í¬ê¸°: {p.shape}")  # ì˜ˆìƒ ì¶œë ¥: (2, 9)
print(f"ê°€ì¹˜ì˜ ì¶œë ¥ í¬ê¸°: {v.shape}")  # ì˜ˆìƒ ì¶œë ¥: (2, 1)
print("ì •ì±… ì¶œë ¥:\n", p)
print("ê°€ì¹˜ ì¶œë ¥:\n", v)

"""# 04 MCTS

### Node (MCTSì—ì„œ í•˜ë‚˜ì˜ ìƒíƒœë¥¼ í‘œí˜„í•˜ëŠ” ë…¸ë“œ)
#### PUCT ì•Œê³ ë¦¬ì¦˜: Upper Confidence Bound for Trees
1. ê²Œì„ì˜ í˜„ì¬ ìƒíƒœ state
2. í•´ë‹¹ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ child_nodes,
3. ê° í–‰ë™ì˜ í‰ê°€ ì •ë³´ n,w,pë¡œ êµ¬ì„±ëœë‹¤!
"""

class Node:
    def __init__(self, state, p):
        self.state = state #í˜„ì¬ ìƒíƒœ
        self.p = p # prior prob / residual ë„¤íŠ¸ì›Œí¬ì—ì„œ ì˜ˆì¸¡ëœ í–‰ë™ í™•ë¥ 
        self.n = 0 # n_visit / ë…¸ë“œ ë°©ë¬¸ íšŸìˆ˜
        self.w = 0 # cum weight / ë…¸ë“œ í‰ê°€ ê°€ì¹˜ë¥¼ ëˆ„ì í•œ ê°’
        self.child_nodes = [] #í˜„ì¬ ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì„ í†µí•´ ìƒì„±ëœ ìì‹ ë…¸ë“œ


    def evaluate_value(self, model, C_PUCT): #model ì´ìš©í•˜ì—¬ í˜„ì¬ ë…¸ë“œ ê°€ì¹˜ í‰ê°€ / ìì‹ ë…¸ë“œë¥¼ í™•ì¥ or ì„ íƒ
        #model -> ì •ì±… ë° ê°€ì¹˜ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ëª¨ë¸
        #return -> í˜„ì¬ ë…¸ë“œì˜ í‰ê°€ ê°€ì¹˜
        is_done, is_lose = self.state.check_done()

        if is_done: #ê²Œì„ì´ ëë‚œ ìƒíƒœì¸ì§€ í™•ì¸ / ëë‚œ ìƒíƒœë¼ë©´ ê°€ì¹˜ ê³„ì‚°
            # judge current value / Use the reward system from the common environment
            value = self.state.get_reward(self.state)
            self.n += 1 #ë…¸ë“œ ì—…ë°ì´íŠ¸
            self.w += value
            #print(f"[DEBUG] MCTS - Terminal State Reached. Value: {value}")
            return value

        # When child node does not exist ìì‹ ë…¸ë“œê°€ ì—†ëŠ” ê²½ìš°
        if len(self.child_nodes) == 0:
            policy, value = predict(model, self.state) # by using nn
            #residual ë„¤íŠ¸ì›Œí¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì •ì±… ë° ê°€ì¹˜ ì˜ˆì¸¡

            # update í‰ê°€ ê²°ê³¼ë¥¼ ë¶€ëª¨ ë…¸ë“œë¡œ ì—­ì „íŒŒí•˜ì—¬ ì—…ë°ì´íŠ¸
            self.n += 1 #íƒìƒ‰ í›„ ë°©ë¬¸ íšŸìˆ˜ ì—…ë°ì´íŠ¸
            self.w += value #íƒìƒ‰ í›„ ëˆ„ì  ê°€ì¹˜ë¥¼ ì—…ë°ì´íŠ¸
            #print(f"[DEBUG] MCTS - Expanding Node. Policy: {policy}, Value: {value}")

            # expand í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ì„ ê¸°ë°˜ìœ¼ë¡œ ìì‹ ë…¸ë“œ í™•ì¥
            legal_actions = np.where(self.state.get_legal_actions() != 0)[0]
            policy = np.maximum(policy, 1e-3)  # ì •ì±… ê°’ì´ 0ì´ ë˜ì§€ ì•Šë„ë¡ ìµœì†Œê°’ ì„¤ì •
            policy /= np.sum(policy)  # ì •ê·œí™”

            for action, p in zip(legal_actions, policy):
                self.child_nodes.append(Node(self.state.next(action), p))
                #ê°€ëŠ¥í•œ í–‰ë™ê³¼, ì´ì— ëŒ€ì‘í•˜ëŠ” í™•ë¥ ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ìì‹ ë…¸ë“œ ìƒì„±

            return value

        # When child node exist ìì‹ ë…¸ë“œê°€ ìˆëŠ” ê²½ìš°
        else:
            value = - self.select_next_child_node(C_PUCT).evaluate_value(model, C_PUCT)
            #PUCT ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ìµœì ì˜ ìì‹ ë…¸ë“œ ì„ íƒ / ë°˜í™˜ëœ ê°’ì„ ë°˜ì „

            # update
            self.n += 1
            self.w += value

            return value


    def select_next_child_node(self, C_PUCT): #PUCT ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ìì‹ ë…¸ë“œ ì„ íƒ
        total_visit = sum(get_n_child(self.child_nodes)) #ëª¨ë“  ìì‹ ë…¸ë“œì˜ ì´ ë°©ë¬¸ íšŸìˆ˜
        values = []
        for node in self.child_nodes: #ê° ìì‹ ë…¸ë“œì— ëŒ€í•´~
            q_value = -(node.w / node.n) if node.n != 0 else 0 #í˜„ì¬ ë…¸ë“œì˜ í‰ê·  ê°€ì¹˜
            #ë°©ë¬¸í•˜ì§€ ì•Šì€ ë…¸ë“œëŠ” 0
            node_value = q_value + C_PUCT * node.p * sqrt(total_visit) / (1 + node.n) # PUCT ì•Œê³ ë¦¬ì¦˜
            #íƒí—˜ ìš©ì–´: ë°©ë¬¸ íšŸìˆ˜ê°€ ì ê³ , ì‚¬ì „ í™•ë¥  pê°€ ë†’ì€ ìì‹ ë…¸ë“œë¥¼ ë” ë§ì´ íƒí—˜
            values.append(node_value) #PUCT ì ìˆ˜: q_value + node_valueë¥¼ ê³„ì‚°

        return self.child_nodes[np.argmax(values)]
        #ê°€ì¥ ë†’ì€ PUCT ì ìˆ˜ë¥¼ ê°€ì§„ ìì‹ ë…¸ë“œë¥¼ ì„ íƒí•˜ì—¬ ë°˜í™˜

"""### def for MCTS

### get_n_child
"""

def get_n_child(child_nodes): #ìì‹ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë…¸ë“œì˜ ë°©ë¬¸ íšŸìˆ˜ë¥¼ ì¶”ì¶œ

    child_n = [] #ê° ë…¸ë“œì˜ ë°©ë¬¸ íšŸìˆ˜ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸

    for node in child_nodes:
        child_n.append(node.n)

    return child_n

"""### predict"""

def predict(model, state): #ì£¼ì–´ì§„ ìƒíƒœì—ì„œ ì •ì±… í™•ë¥ ê³¼ ê°€ì¹˜ë¥¼ ê³„ì‚°
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = next(model.parameters()).device
    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    input_array = np.array([state.state, state.enemy_state, state.state - state.enemy_state], dtype=np.float32)
    input_tensor = torch.tensor(input_array).unsqueeze(0).to(device)  # (1, 3, 3, 3)

    model.eval() #ëª¨ë¸ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    with torch.no_grad():
        policy, value = model(input_tensor)
        policy = policy.cpu().numpy().reshape(-1)
        value = value.cpu().numpy().item()

    # Value ì •ê·œí™” (ìŠ¹íŒ¨ë¥¼ ë”ìš± ëª…í™•í•˜ê²Œ êµ¬ë¶„)
    value = np.tanh(value * 25)  # ê¸°ì¡´ 8ì—ì„œ 15ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ìŠ¹íŒ¨ ì°¨ì´ë¥¼ ë” ê·¹ëŒ€í™”

    # Debugging policy values
    #print(f"[DEBUG] Predict() - Raw Policy: {policy}, Value: {value}")

    # âœ… ì •ì±… ê°’ ì •ê·œí™” ë° NaN ë°©ì§€
    if np.isnan(policy).any() or np.isinf(policy).any() or np.sum(policy) == 0:
        print(f"[WARNING] NaN detected in predict(). Resetting to uniform policy.")
        policy = np.ones_like(policy) / len(policy)

    # take legal policy
    legal_actions = np.where(state.get_legal_actions() != 0)[0]

    # âœ… ì •ì±… í¬ê¸° ì •í•©ì„± í™•ì¸ ë° ë³´ì •
    if policy.shape[0] != N_ACTIONS:
        print(f"[WARNING] Policy shape mismatch in predict(). Expected {N_ACTIONS}, got {policy.shape[0]}. Adjusting...")
        policy = np.ones(N_ACTIONS) / N_ACTIONS # ê¸°ë³¸ ê· ì¼ ë¶„í¬ ì„¤ì •

    policy = policy[legal_actions] if len(legal_actions) > 0 else np.ones_like(policy) / len(policy)

    # âœ… ìµœì¢… í™•ë¥  ì •ê·œí™”
    policy /= np.sum(policy)

    #ëª¨ë¸ì—ì„œ ë°˜í™˜ëœ ì •ì±… í™•ë¥  ì¤‘ í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ì˜ í™•ë¥ ë§Œ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸

    # Debugging normalized policy
    #print(f"[DEBUG] Predict() - Legal Policy: {legal_policy}")

    return policy, value

"""## MCTS (ìµœì ì˜ í–‰ë™ì„ ì„ íƒ)
#### ê° ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì—¬ ë‹¤ìŒ ì•¡ì…˜ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°
#### ê°’ì„ ë†’ì´ë©´ MCTSê°€ ë” ê¹Šì´ íƒìƒ‰í•˜ë¯€ë¡œ ë” ì •í™•í•œ ì •ì±…/ê°€ì¹˜ í‰ê°€ë¥¼ ì œê³µ
"""

class MCTS:
    def __init__(self, initial_pv_evaluate_count=800, max_pv_evaluate_count=1500, growth_rate=1.02, initial_c_puct=2.0, min_c_puct=1.5, decay_rate=0.99):
        self.policy = None #MCTSë¥¼ í†µí•´ ê³„ì‚°ëœ í–‰ë™ í™•ë¥  ë¶„í¬ë¥¼ ì €ì¥
        self.pv_evaluate_count = initial_pv_evaluate_count #MCTSê°€ í˜„ì¬ ìƒíƒœë¥¼ íƒìƒ‰í•˜ëŠ” íšŸìˆ˜ë¥¼ ì •ì˜
        self.MAX_PV_EVALUATE_COUNT = max_pv_evaluate_count  # ìµœëŒ€ íƒìƒ‰ íšŸìˆ˜
        self.GROWTH_RATE = growth_rate  # ì¦ê°€ìœ¨
        self.C_PUCT = initial_c_puct  # ì´ˆê¸° C_PUCT ê°’
        self.MIN_C_PUCT = min_c_puct  # ìµœì†Œ C_PUCT ê°’
        self.DECAY_RATE = decay_rate  # ê°ì†Œìœ¨


    def update_c_puct(self):
        """ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ C_PUCT ê°’ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ """
        self.C_PUCT = max(self.MIN_C_PUCT, self.C_PUCT * self.DECAY_RATE)
        print(f"[DEBUG] C_PUCT updated to {self.C_PUCT:.3f}")


    def update_pv_evaluate_count(self):
        """ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ MCTS ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ """
        self.pv_evaluate_count = min(self.MAX_PV_EVALUATE_COUNT, int(self.pv_evaluate_count * self.GROWTH_RATE))
        print(f"[DEBUG] Updated pv_evaluate_count: {self.pv_evaluate_count}")


    def get_policy(self, state, model, temp): #í˜„ì¬ ìƒíƒœì—ì„œ ìµœì ì˜ í–‰ë™ í™•ë¥  ë¶„í¬ ê³„ì‚°
        original_temp = temp  # ê¸°ì¡´ temp ê°’ ì €ì¥

        if state.move_count == 0:
            temp = max(2.5, original_temp)  # Move 0ì—ì„œëŠ” ì˜¨ë„ë¥¼ ë†’ì—¬ ë” ê· ë“±í•œ ë¶„í¬ ìœ ì§€

        root_node = Node(state, p=1.0)

        # âœ… í›„ê³µ(Player 2)ì˜ íƒìƒ‰ì„ ë³´ë‹¤ ê¹Šê²Œ ìˆ˜í–‰
        adjust_c_puct = self.C_PUCT * (1.3 if not state.check_first_player() else 1.0)
        evaluate_count = self.pv_evaluate_count + (100 if not state.check_first_player() else 50)

        for _ in range(evaluate_count):
            root_node.evaluate_value(model, adjust_c_puct)

        childs_n = get_n_child(root_node.child_nodes)
        legal_actions = np.where(state.get_legal_actions() != 0)[0]

        # âœ… ì •ì±… ê°’ shape ì •í•©ì„± ì²´í¬ ë° ë³´ì •
        if len(childs_n) != len(legal_actions):
            print(f"[WARNING] Mismatch detected in get_policy(): legal_actions={len(legal_actions)}, childs_n={len(childs_n)}.")

            # Fix: `childs_n` í¬ê¸°ë¥¼ `legal_actions` í¬ê¸°ë¡œ ì¡°ì •
            if len(legal_actions) > 0:
                childs_n = np.ones(len(legal_actions))  # ê· ì¼í•œ í™•ë¥ ë¡œ ë³´ì •
            else:
                childs_n = np.ones(env.num_actions)  # ê¸°ë³¸ í¬ê¸°(9)ë¡œ ì„¤ì •

        policy = self.boltzmann_dist(childs_n, temp)

        # âœ… NaN ê°’ ë°©ì§€
        if np.isnan(policy).any() or np.isinf(policy).any() or np.sum(policy) == 0:
            print(f"[WARNING] Invalid policy detected in get_policy(). Resetting to uniform distribution.")
            policy = np.ones(len(legal_actions)) / len(legal_actions)
        else:
            policy /= np.sum(policy)  # ì •ê·œí™”

        temp = original_temp  # Move 0 í›„ì—ëŠ” ì›ë˜ temp ê°’ìœ¼ë¡œ ë³µêµ¬

        return policy


    def get_actions_of(self, model, temp): #MCTS í†µí•´ ê³„ì‚°ëœ ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒ
        def get_action(state):
            self.policy = self.get_policy(state, model, temp)
            #get_policyë¥¼ í˜¸ì¶œí•˜ì—¬ í˜„ì¬ ìƒíƒœì—ì„œ íƒìƒ‰ëœ í–‰ë™ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•˜ê³  ì €ì¥
            legal_actions = np.where(state.get_legal_actions() != 0)[0]
            action = np.random.choice(legal_actions, p=self.policy)
            #ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ / ì„ íƒ í™•ë¥ ì€ self.policyì— ë”°ë¼ ê²°ì •
            #ex) ê°€ëŠ¥í•œ í–‰ë™ [0,1,2] / í–‰ë™ í™•ë¥  [0.2, 0.5, 0.3]
            return action
        return get_action


    def boltzmann_dist(self, x_lst, temp):
        x_lst = np.array(x_lst, dtype=np.float32) + 1e-8  # ì‘ì€ ê°’ ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€

        # âœ… NaN ë°©ì§€: x_lst ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ í´ë¦¬í•‘ ì ìš©
        x_lst = np.clip(x_lst, -20, 20)  # log scaleì—ì„œ ì•ˆì •ì ì¸ ê°’ ìœ ì§€

        x_lst = np.exp(x_lst / temp)  # ì§€ìˆ˜ í•¨ìˆ˜ ì ìš©

        # âœ… ì •ì±… ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡ ë³´ì •)
        x_lst /= np.sum(x_lst) if np.sum(x_lst) > 0 else len(x_lst)

        # âœ… NaN í™•ì¸ ë° ëŒ€ì²´
        if np.isnan(x_lst).any() or np.isinf(x_lst).any():
            print(f"[WARNING] NaN detected in boltzmann_dist, replacing with uniform distribution.")
            x_lst = np.ones_like(x_lst) / len(x_lst)

        return x_lst.tolist()

"""# 05 SelfPlay *í™˜ê²½ ì ìš©ì— ë”°ë¼ ìˆ˜ì •í•œ ì½”ë“œ

## SelfPlay (ë‘ ì—ì´ì „íŠ¸ê°€ ëŒ€ì „í•˜ì—¬ ë°ì´í„° ìƒì„±)
### MCTSì™€ ì •ì±…-ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©
#### ëœë¤ í–‰ë™ ë¹„ìœ¨ì„ ì¶”ê°€í•˜ì—¬ ì´ˆë°˜ ë°ì´í„° ë‹¤ì–‘ì„±ì„ ì¦ê°€!
"""

# Self-play ë°ì´í„° í¬ê¸° ì œí•œ (ìµœì‹  Nê°œì˜ ê²Œì„ë§Œ ìœ ì§€)
MAX_HISTORY_SIZE = 5000  # ìµœê·¼ 5000ê°œì˜ ê²Œì„ ê¸°ë¡ë§Œ ìœ ì§€

class SelfPlay:
    def __init__(self, model, temp, sp_game_count, pv_evaluate_count, temp_discount=0.999995):
        # model
        self.model = model #ì •ì±…-ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸

        # params
        self.sp_game_count = sp_game_count #self-play ê²Œì„ íšŸìˆ˜

        # about temps
        self.temp = temp #ì´ˆê¸° ë³¼ì¸ ë§Œ ì˜¨ë„ íŒŒë¼ë¯¸í„°
        self.temp_discount = 0.999995 #ì˜¨ë„ ê°ì†Œìœ¨

        self.history = [] # selfPlay's yield / self-play ê²°ê³¼ë¥¼ ì €ì¥ (ê²Œì„ ìƒíƒœ, ì •ì±…, ê°€ì¹˜)

        self.pv_evaluate_count = pv_evaluate_count
        self.mcts = MCTS(self.pv_evaluate_count) #MCTSì— ì „ë‹¬

        self.env = Environment()

        self.FIRST_PLAYER_BIAS = 0.6  # Player 1ì´ ì„ ê³µí•  í™•ë¥ ì„ 60%ë¡œ ì„¤ì •


    def get_first_player_value(self, ended_state): #ê²Œì„ ì¢…ë£Œ ìƒíƒœì—ì„œ ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ìŠ¹íŒ¨ë¥¼ í‰ê°€
        # Use the common environment's reward system
        reward = self.env.get_reward(ended_state)
        return reward if ended_state.check_first_player() else -reward


    def _single_play(self, i): #í•˜ë‚˜ì˜ self-play ê²Œì„ì„ ìˆ˜í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±
        history = [] #ê²Œì„ ì¤‘ ì €ì¥ëœ ë°ì´í„°
        # Player 1ì´ ì„ ê³µí•  í™•ë¥ ì„ FIRST_PLAYER_BIASë¡œ ì¡°ì •
        first_player = random.random() < self.FIRST_PLAYER_BIAS
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        #state = State() if first_player else State(state=np.zeros((self.env.n, self.env.n)),
                                               #enemy_state=np.zeros((self.env.n, self.env.n)))

        # âœ… State ê°ì²´ ìƒì„± ì‹œ move_count ëª…í™•íˆ ì„¤ì •
        if first_player:
            state = State(move_count=0)  # ì„ ê³µ (Player 1)
        else:
            state = State(state=np.zeros(STATE_SIZE, dtype=int),
                          enemy_state=np.zeros(STATE_SIZE, dtype=int),
                          move_count=2)  # í›„ê³µ (Player 2), move_count=2ë¡œ ì„¤ì •

        legal_actions = np.where(state.get_legal_actions() != 0)[0] # í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ë“¤ì„ ê°€ì ¸ì˜´

        # ì²« ë²ˆì§¸ ìˆ˜ë¥¼ MCTS ê¸°ë°˜ìœ¼ë¡œ ì„ íƒ
        if len(legal_actions) > 0:
            policy = self.mcts.get_policy(state, self.model, self.temp)  # MCTSë¡œ ì •ì±… ê³„ì‚°

            # âœ… ì •ì±… ê°’ì—ì„œ NaN í™•ì¸ ë° ì •ê·œí™”
            if np.isnan(policy).any() or np.sum(policy) == 0:
                print(f"[WARNING] NaN detected in policy at Game {i + 1}. Replacing with uniform distribution.")
                policy = np.ones_like(legal_actions) / len(legal_actions)

            action = np.random.choice(legal_actions, p=policy)  # MCTS ì •ì±… ê¸°ë°˜ ì„ íƒ
            state = state.next(action)
            # ì„ ê³µì— ë”°ë¼ move_countë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •
            #state.move_count = 0 if first_player else 1 # ì²« ìˆ˜ë¥¼ ë‘” í›„ move_countë¥¼ ì¡°ì •í•˜ì—¬ ë¶ˆì¼ì¹˜ ë°©ì§€

            print(f"Game {i + 1}: {'Player 1' if first_player else 'Player 2'} starts, first move at {action}")

        # ë””ë²„ê¹… ë©”ì‹œì§€ ì¶”ê°€
        print(f"[DEBUG] Game {i + 1}: first_player={first_player}")# check_first_player()={state.check_first_player()}")

        move_count = 0

        # ê²Œì„ ì§„í–‰ (MCTSë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë¥¼ ì„ íƒ)
        while not state.check_done()[0]: #ì—¬ê¸°ì„œëŠ” state.is_done()ì´ trueì¼ ë•Œ ë£¨í”„ ì¢…ë£Œ
            move_count += 1
            policy = self.mcts.get_policy(state, self.model, self.temp)

            # âœ… ì •ì±… ê°’ì—ì„œ NaN í™•ì¸ ë° ì •ê·œí™”
            if np.isnan(policy).any() or np.sum(policy) == 0:
                print(f"[WARNING] NaN detected in policy during move {move_count} of Game {i + 1}.")
                policy = np.ones_like(legal_actions) / len(legal_actions)

            legal_actions = np.where(state.get_legal_actions() != 0)[0]

            # Check if legal_actions is empty before random move selection
            if len(legal_actions) == 0:
                print(f"Game {i + 1}: No legal actions available, ending game.")
                break  # or handle it differently

            final_policy = np.zeros([self.env.num_actions]) # init value / í–‰ë™ í™•ë¥  ì´ˆê¸°í™”
            final_policy[legal_actions] = policy
            #í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ì— ëŒ€í•´ì„œë§Œ í™•ë¥ ì„ ì±„ìš°ê¸°
            #ì „ì²´ í–‰ë™ ê³µê°„ì— ëŒ€í•œ ì •ì±…ì´ ì™„ì„±!
            #ë¶ˆê°€ëŠ¥í•œ í–‰ë™ì˜ í™•ë¥ ì€ 0ìœ¼ë¡œ ìœ ì§€, ê°€ëŠ¥í•œ í–‰ë™ì˜ í™•ë¥ ì€ MCTS ê²°ê³¼ì— ê¸°ë°˜~
            history.append([state, final_policy, 0]) #ë°ì´í„° ì €ì¥ (ë³´ë“œ ìƒíƒœ, ì •ì±…, ê°€ì¹˜)
            #state_n.board: ê²Œì„ì˜ në²ˆì§¸ ìƒíƒœ
            #policy_n: í•´ë‹¹ ìƒíƒœì—ì„œì˜ ì •ì±…(í™•ë¥  ë¶„í¬)
            #None: ê°€ì¹˜(ì•„ì§ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ)

            action = np.random.choice(legal_actions, p=policy)
            state = state.next(action)

        # ê²Œì„ ì¢…ë£Œ í›„ ë³´ìƒ ê³„ì‚°
        final_reward = self.env.get_reward(state)
        player1_reward = final_reward if first_player else -final_reward
        player2_reward = -player1_reward

        # ê²Œì„ ê²°ê³¼ ì¶œë ¥
        game_result = "Draw" if final_reward == 0 else "Player 1 Wins" if player1_reward > 0 else "Player 2 Wins"
        print(f"Game {i + 1} Result: {game_result} (P1: {player1_reward}, P2: {player2_reward})")

        # êµ¬ë¶„ì„  ì¶”ê°€ (ê²Œì„ ì¢…ë£Œ í›„)
        print("-" * 20)  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ 

        # íˆìŠ¤í† ë¦¬ì˜ ê°€ì¹˜ ì—…ë°ì´íŠ¸ (ê° í„´ë§ˆë‹¤ ë°˜ì „)
        value = player1_reward
        for j in range(len(history)): #ê°€ì¹˜ë¥¼ ê²Œì„ íˆìŠ¤í† ë¦¬ì— ì—…ë°ì´íŠ¸
            history[j][-1] = value #ê° ìƒíƒœ history[i]ì˜ ë§ˆì§€ë§‰ ìš”ì†Œì— ê°€ì¹˜ë¥¼ ê¸°ë¡
            #state.board, final_policy, None -> state_i.board, policy_i, value
            value = -value #ê° í„´ë§ˆë‹¤ ê°€ì¹˜ë¥¼ ë°˜ì „í•˜ì—¬ ì €ì¥
            #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ê°€ì¹˜ì™€ ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ê°€ì¹˜ëŠ” ë°˜ëŒ€ì´ë‹¤!
            #ì¦‰ í•œ í”Œë ˆì´ì–´ê°€ ìœ ë¦¬í•œ ìƒíƒœì¼ìˆ˜ë¡ ìƒëŒ€ë°©ì€ ë¶ˆë¦¬í•œ ìƒíƒœê°€ ëœë‹¤~

        # discount temp / ì˜¨ë„ ê°ì†Œ
        if i >= 30:
            self.temp = max(0.75, self.temp * self.temp_discount)
        #ì´ˆê¸° 30í„´ê¹Œì§€ëŠ” ì˜¨ë„ë¥¼ ìœ ì§€, ì´í›„ì—ëŠ” temp_discountë¥¼ ì ìš©í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ

        return history #ì¢…ë£Œ ìƒíƒœì˜ ê°€ì¹˜(ì ìˆ˜)ë¥¼ íˆìŠ¤í† ë¦¬ì— ê¸°ë¡


    def _self_play(self): #ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ self-playë¥¼ ë°˜ë³µ ìˆ˜í–‰
        # sp_game_count ë²ˆ ë§Œí¼ single play ì‹œí–‰
        for i in range(self.sp_game_count): #ì§€ì •ëœ ê²Œì„ íšŸìˆ˜ë§Œí¼ ë°˜ë³µ
            single_history = self._single_play(i) #í•œ ë²ˆì˜ ê²Œì„ ì‹¤í–‰
            self.history.extend(single_history) #ê²Œì„ ê²°ê³¼ë¥¼ self.history ì „ì²´ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ
            if len(self.history) > MAX_HISTORY_SIZE:
                self.history = self.history[-MAX_HISTORY_SIZE:]  # ìµœê·¼ ê¸°ë¡ë§Œ ìœ ì§€

            if (i+1) % (self.sp_game_count // 10) == 0: #ê²Œì„ ì§„í–‰ ìƒíƒœë¥¼ 10% ë‹¨ìœ„ë¡œ ì¶œë ¥
                print(f"self play {i+1} / {self.sp_game_count}")


    def __call__(self, execute_self_play=False):
        """
        SelfPlay ê°ì²´ë¥¼ í˜¸ì¶œ ì‹œ self-play ì‹¤í–‰ ì—¬ë¶€ë¥¼ ì œì–´.
        - execute_self_play: Trueì´ë©´ self-play ì‹¤í–‰, Falseì´ë©´ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ.
        """
        if execute_self_play:
            print("Self-play ì‹¤í–‰ ì¤‘...")
            self._self_play()
        else:
            print("Self-play ì‹¤í–‰ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


    def reset_history(self):
        """
        Self-play ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.history = []
        print("Self-play ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


    def update_model(self, model):  # ëª¨ë¸ ì—…ë°ì´íŠ¸ ë©”ì„œë“œ
        self.model.load_state_dict(model.module.state_dict())  # ì£¼ì–´ì§„ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ë³µì‚¬



"""# 06 TrainNetwork

## TrainNetwork
### ì•ŒíŒŒì œë¡œ ì•Œê³ ë¦¬ì¦˜ì˜ ì •ì±…-ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµ
#### self-playë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì±…-ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì—…ë°ì´íŠ¸
1. ìƒ˜í”Œë§ëœ ë°ì´í„°ë¥¼ ì¤€ë¹„
2. ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° (ì •ì±… ë° ê°€ì¹˜ ì†ì‹¤)
3. ì—­ì „íŒŒë¥¼ í†µí•´ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
"""

class TrainNetwork:
    def __init__(self, model, batch_size, learning_rate, learn_epoch, save_loss=True):
        # define
        self.losses = [] # elements : ( p_loss, v_loss, total_loss )
        self.batch_size = batch_size

        # model & device
        self.model = model
        self.device = next(model.parameters()).device #ë„¤íŠ¸ì›Œí¬ê°€ ì‹¤í–‰ë  ë””ë°”ì´ìŠ¤ (GPU/CPU)

        # learning rate
        self.learning_rate = learning_rate
        self.learn_decay = 0.1
        self.learn_epoch = learn_epoch #í•™ìŠµë¥  ê°ì†Œ ì£¼ê¸°

        # define loss ftn / ì†ì‹¤ í•¨ìˆ˜
        self.mse_loss = F.mse_loss #ê°€ì¹˜ ì†ì‹¤ ê³„ì‚°
        #ë„¤íŠ¸ì›Œí¬ê°€ ì˜ˆì¸¡í•œ ê°€ì¹˜ì™€ ì‹¤ì œ ê°€ì¹˜ ê°„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”
        #ê°€ì¹˜ëŠ” ì—°ì†í˜• ë°ì´í„°ë¡œ [-1,1] ë²”ìœ„ì˜ ì‹¤ìˆ˜ (íšŒê·€ ë¬¸ì œ)
        self.cross_entropy_loss = nn.CrossEntropyLoss() #ì •ì±… ì†ì‹¤ ê³„ì‚°
        #ë„¤íŠ¸ì›Œí¬ê°€ ì˜ˆì¸¡í•œ ì •ì±…ê³¼ ì‹¤ì œ ì •ì±… ê°„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”
        #ì •ì±…ì€ í™•ë¥  ë¶„í¬ë¡œ í´ë˜ìŠ¤ í™•ë¥  ê°’, ì¦‰ í™•ë¥ ì˜ í•©ì´ 1 (ë¶„ë¥˜ ë¬¸ì œ)

        # Optimizer & Scheduler
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, eps=1e-4, weight_decay=1e-4) # L2 ì •ê·œí™” í¬í•¨
        #ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=self.learn_epoch, gamma = self.learn_decay)
        #í•™ìŠµë¥ ì„ learn_epoch ì£¼ê¸°ë¡œ ê°ì†Œì‹œì¼œ í•™ìŠµ

        self.save_loss = save_loss  # ì†ì‹¤ì„ ì €ì¥í• ì§€ ì—¬ë¶€
        self.loss_file = "train_losses.csv"

        # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ì¶”ê°€
        if self.save_loss and not os.path.exists(self.loss_file):
            with open(self.loss_file, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["epoch", "p_loss", "v_loss", "total_loss"])


    def _train(self, history, epoch): #self-playì—ì„œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµ
        # ìƒ˜í”Œë§
        # historyì— ë°°ì¹˜ ìƒ˜í”Œë§ì„ ìœ„í•œ ì¶©ë¶„í•œ ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
        num_samples = len(history) # historyì˜ ì‹¤ì œ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        batch_size = min(self.batch_size, num_samples)  # historyê°€ ë” ì§§ìœ¼ë©´ ë” ì‘ì€ ë°°ì¹˜ë¥¼ ì‚¬ìš©
        sample = random.choices(history, k=batch_size) #historyì—ì„œ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§

        states, target_policies, target_values = zip(*sample)
        #ìƒ˜í”Œ ë°ì´í„°ëŠ” ìƒíƒœ, ëª©í‘œ ì •ì±…, ëª©í‘œ ê°€ì¹˜ë¡œ ë¶„ë¦¬

        # `reward` ê°’ì´ ë¹„ì •ìƒì ìœ¼ë¡œ -1ë¡œë§Œ ì±„ì›Œì¡ŒëŠ”ì§€ í™•ì¸
        # print(f"[DEBUG] Target values before normalization: {target_values}")

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        expanded_states = np.array([
            np.array([s.state, s.enemy_state, s.state - s.enemy_state], dtype=np.float32)
            for s in states
        ])  # (BATCH, 3, 3, 3)

        #ìƒ˜í”Œ ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜
        states = torch.tensor(expanded_states, dtype=torch.float32)  # (BATCH, STATE_DIM, 3, 3)
        target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).view(batch_size, -1)  # (BATCH, 9)
        target_values = torch.tensor(np.array(target_values), dtype=torch.float32).view(batch_size, -1)  # (BATCH, 1)

        target_values = (target_values - target_values.mean()) / (target_values.std() + 1e-6)  # ì •ê·œí™”

        # print(f"[DEBUG] Target values after normalization: {target_values}")

        # deviceì— ì˜¬ë¦¬ê¸° / ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ (GPU/CPU)ì— ì „ì†¡
        states, target_policies, target_values = states.to(self.device), target_policies.to(self.device), target_values.to(self.device)

        # í•™ìŠµ ì‹¤í–‰
        self.model.train()
        policy, value = self.model(states) #ì •ì±…-ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ ì‹¤í–‰

        # ì†ì‹¤ ê³„ì‚°
        p_loss = self.cross_entropy_loss(policy, target_policies) #ì •ì±… ì†ì‹¤
        v_loss = 0.25 * self.mse_loss(value, target_values) #ê°€ì¹˜ ì†ì‹¤
        total_loss = p_loss + 0.5 * v_loss #ì´ ì†ì‹¤ / ê¸°ì¡´ë³´ë‹¤ Value Loss ê¸°ì—¬ë„ë¥¼ ë‚®ì¶¤

        # Logging losses / ì†ì‹¤ ê¸°ë¡
        self.losses.append((p_loss.item(), v_loss.item(), total_loss.item()))

        # 10 ì—í¬í¬ë§ˆë‹¤ CSV íŒŒì¼ì— ì €ì¥
        if epoch % 10 == 0 and self.save_loss:
            with open(self.loss_file, "a", newline="") as f:
                writer = csv.writer(f)
                writer.writerows([(epoch, p, v, t) for p, v, t in self.losses])
            self.losses.clear()  # ì €ì¥ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œí•˜ì—¬ ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš© ë°©ì§€

        # Backpropagation / ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step() #ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        self.scheduler.step() #í•™ìŠµë¥  ê°ì†Œ


    def __call__(self, history, epoch):
        self._train(history, epoch) #í˜¸ì¶œ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ _train ë§¤ì„œë“œ ì‹¤í–‰
        #train_network._train(history) ì§ì ‘ í˜¸ì¶œ -> train_network(history) í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œ


    def update_model(self, model): #self-playì— ì‚¬ìš©ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ / ìµœì‹  í•™ìŠµëœ ëª¨ë¸ë¡œ êµì²´
        self.model.load_state_dict(model.module.state_dict())
        #í˜„ì¬ í´ë˜ìŠ¤ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ì™¸ë¶€ì—ì„œ ì œê³µëœ ìƒˆë¡œìš´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¡œ ì—…ë°ì´íŠ¸

"""# 07 EvalNetwork
### í•™ìŠµëœ ìµœê·¼ ëª¨ë¸ì„ í˜„ì¬ ìµœê³  ëª¨ë¸ê³¼ ë¹„êµ í‰ê°€í•˜ëŠ” ì—­í• 
"""

import os

save_path = '/content/drive/My Drive/3-2/ê°•í™”'
os.makedirs(save_path, exist_ok=True)  # í´ë” ìƒì„±

class EvalNetwork: #ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ë‹´ë‹¹
    def __init__(self, best_model, eval_game_count, eval_temperature, eval_count, env):
        # eval info
        self.eval_game_count = eval_game_count #í‰ê°€ ì‹œ ì‹¤í–‰í•  ê²Œì„ ìˆ˜
        self.eval_temperature = eval_temperature #ë³¼ì¸ ë§Œ ë¶„í¬ ì˜¨ë„ íŒŒë¼ë¯¸í„°

        # models
        self.best_model = best_model #í˜„ì¬ ìµœê³  ëª¨ë¸
        self.recent_model = None #ë¹„êµí•  ìµœì‹  ëª¨ë¸ / ì´ˆê¸°í™” ì‹œì—ëŠ” None
        #í‰ê°€ í›„, ìµœì‹  ëª¨ë¸ì´ ë” ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ ìµœê³  ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸~

        # MCTS
        self.mcts = MCTS(eval_count) #í‰ê°€ì— ì‚¬ìš©í•  MCTS íƒìƒ‰ ê°œì²´

        # update / win
        self.updated = False #ìµœê·¼ ëª¨ë¸ì´ ìµœê³  ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        self.win_rate = 0.0 #ìµœê·¼ ëª¨ë¸ì˜ í‰ê·  ìŠ¹ë¥ 

        self.env = env  # AlphaZeroAgentì—ì„œ ì „ë‹¬ë°›ë„ë¡ ìˆ˜ì •

    def _first_player_point(self, ended_state):
        reward = self.env.get_reward(ended_state)
        # ì²« ë²ˆì§¸ í”Œë ˆì´ì–´ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜, ë‘ ë²ˆì§¸ í”Œë ˆì´ì–´ì¼ ê²½ìš° ë°˜ì „
        return reward if ended_state.check_first_player() else -reward


    def _evaluate_network(self, recent_model): #ìµœì‹  ëª¨ë¸ì„ í˜„ì¬ ìµœê³  ëª¨ë¸ê³¼ í‰ê°€
        self.updated = False #ì´ˆê¸°í™”
        self.recent_model = recent_model

        # MCTSë¥¼ ì´ìš©
        next_actions_recent = self.mcts.get_actions_of(self.recent_model, self.eval_temperature)
        #ìµœì‹  ëª¨ë¸ì˜ í–‰ë™ ìƒì„± í•¨ìˆ˜ / ê° ëª¨ë¸ì˜ í–‰ë™ í™•ë¥  ìƒì„±
        next_actions_best = self.mcts.get_actions_of(self.best_model, self.eval_temperature)
        #ìµœê³  ëª¨ë¸ì˜ í–‰ë™ ìƒì„± í•¨ìˆ˜
        next_actions = (next_actions_recent, next_actions_best)

        total_point = 0

        for i in range(self.eval_game_count): #ì§€ì •ëœ ê²Œì„ ìˆ˜ë§Œí¼ í‰ê°€ ì§„í–‰
            if i % 2 == 0: #ì§ìˆ˜ ê²Œì„: ìµœì‹  ëª¨ë¸ì´ ì„ ê³µ
                total_point += self._single_play(next_actions)
            else: #í™€ìˆ˜ ê²Œì„: ìµœì‹  ëª¨ë¸ì´ í›„ê³µ
                total_point += 1 - self._single_play(list(reversed(next_actions)))

        average_point = total_point / self.eval_game_count #ê° ê²Œì„ ê²°ê³¼ë¥¼ ëˆ„ì í•˜ì—¬ í‰ê·  ì ìˆ˜ ê³„ì‚°
        print('Average Point of Latest Model', average_point)

        if average_point >= 0.5:
            self._update_best_model() #í‰ê·  ì ìˆ˜ê°€ 0.5 ì´ìƒì´ë©´ ìµœê³  ëª¨ë¸ ì—…ë°ì´íŠ¸


    def _single_play(self, next_actions): #ë‹¨ì¼ ê²Œì„ì„ ì‹¤í–‰í•˜ì—¬ ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ì ìˆ˜ ë°˜í™˜
        state = State() #ì´ˆê¸° ìƒíƒœ ìƒì„±
        while not state.check_done()[0]:
            next_action = next_actions[0] if state.check_first_player() else next_actions[1]
            #í–‰ë™ ìƒì„± í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í–‰ë™ ì„ íƒ
            #next_actions[0]: next_actions_recent
            #next_actions[1]: next_actions_best
            action = next_action(state)
            state = state.next(action) #ì„ íƒëœ í–‰ë™ì— ë”°ë¼ ë‹¤ìŒ ìƒíƒœë¡œ ì´ë™
        return self._first_player_point(state) #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ì ìˆ˜ ë°˜í™˜


    def _update_best_model(self): #ìµœì‹  ëª¨ë¸ì„ í˜„ì¬ ìµœê³  ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
        #DataParallelë¡œ ê°ì‹¸ì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ìƒíƒœì— ë”°ë¼ ì ì ˆíˆ state_dictë¥¼ ë¡œë“œ
        if isinstance(self.recent_model, torch.nn.DataParallel):
            recent_model_state_dict = self.recent_model.module.state_dict()  # DataParallel ì œê±°
        else:
            recent_model_state_dict = self.recent_model.state_dict()

        if isinstance(self.best_model, torch.nn.DataParallel):
            self.best_model.module.load_state_dict(recent_model_state_dict)  # DataParallel ì œê±° í›„ ë¡œë“œ
        else:
            self.best_model.load_state_dict(recent_model_state_dict)

        self.updated = True
        print("Best Model is Updated.")


    #í˜„ì¬ ìµœê³  ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥
    def save_model(self, file_name="best_model_env_weights.pth"):
        save_file_path = os.path.join(save_path, file_name)
        if isinstance(self.best_model, torch.nn.DataParallel):
            torch.save(self.best_model.module.state_dict(), save_file_path)
        else:
            torch.save(self.best_model.state_dict(), save_file_path)
        print(f"Model saved to {save_file_path}")


    def __call__(self, recent_model):
        self._evaluate_network(recent_model)

"""# 08. AlphaZeroAgent"""

import time

# Updated main code with optimizations / device ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class AlphaZeroAgent:
    def __init__(self, model_path, sp_game_count, eval_game_count, learning_rate, batch_size, epochs, learn_epoch, pv_evaluate_count, env):
        """
        AlphaZeroAgent ì´ˆê¸°í™” ë©”ì„œë“œ
        """
        # ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model = Network(N_RESIDUAL_BLOCK, N_KERNEL, STATE_SIZE, N_ACTIONS)
        self.model = nn.DataParallel(self.model).to(device) #Multi-GPU í™œìš©

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        if model_path:
            params = torch.load(model_path, map_location=device)  # map_location ì¶”ê°€
            self.model.load_state_dict(params, strict=False)

        # SelfPlay, TrainNetwork, EvalNetwork ì´ˆê¸°í™”
        self.selfplay = SelfPlay(self.model, SP_TEMPERATURE, sp_game_count, pv_evaluate_count)
        self.train_network = TrainNetwork(self.model, batch_size, learning_rate, LEARN_EPOCH)
        self.eval_network = EvalNetwork(self.model, eval_game_count, EVAL_TEMPERATURE, EVAL_COUNT, env)

        # í•™ìŠµ ê´€ë ¨ ì„¤ì •
        self.epochs = epochs
        self.sp_game_count = sp_game_count


    def get_action(self, state):
        """
        MCTSë¥¼ ì´ìš©í•´ í˜„ì¬ ìƒíƒœì—ì„œ ìµœì ì˜ í–‰ë™ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not hasattr(self, 'selfplay') or not hasattr(self.selfplay, 'mcts'):
            raise AttributeError("MCTS ê°ì²´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # MCTSë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë™ í™•ë¥ ì„ ê³„ì‚°
        action_probs = self.selfplay.mcts.get_policy(state, self.model, SP_TEMPERATURE)
        # í™•ë¥ ì— ë”°ë¼ í–‰ë™ì„ ì„ íƒ
        legal_actions = np.where(state.get_legal_actions() != 0)[0]
        return np.random.choice(legal_actions, p=action_probs)


    def train(self):
        for i in range(self.epochs):
            print(f'\n[Epoch {i + 1}/{self.epochs}] --------------------------------')


            for j in range(self.sp_game_count):  # self-play í•œ íŒ â†’ í•™ìŠµ í•œ íŒ ë°˜ë³µ
                print(f'\n[Game {j + 1}/{self.sp_game_count} in Epoch {i + 1}]')


                # Self-play ìˆ˜í–‰ (í•œ íŒë§Œ ì§„í–‰)
                start_time = time.time()
                with torch.no_grad():
                    self.selfplay._single_play(j)  # í•œ íŒë§Œ ìˆ˜í–‰
                print(f"Single Self-play ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")

                # Training ìˆ˜í–‰ (ë°”ë¡œ í•™ìŠµ)
                start_time = time.time()
                self.train_network(self.selfplay.history, i+1)  # ì†ì‹¤ ë°˜í™˜
                print(f"Training ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")

                # Loss ì¶œë ¥
                if len(self.train_network.losses) > 0:
                    latest_p_loss, latest_v_loss, latest_total_loss = self.train_network.losses[-1]
                    print(f"ğŸ”¹ Game {j + 1}: Policy Loss = {latest_p_loss:.4f}, Value Loss = {latest_v_loss:.4f}, Total Loss = {latest_total_loss:.4f}")

                # Model evaluation (í•œ epoch ë‹¨ìœ„ë¡œ í‰ê°€)
                start_time = time.time()
                self.eval_network(self.train_network.model)
                print(f"Evaluation ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")

                # C_PUCT ë° MCTS ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜ ì—…ë°ì´íŠ¸ ì ìš©
                self.selfplay.mcts.update_c_puct()
                self.selfplay.mcts.update_pv_evaluate_count()

                # ìµœì‹  ëª¨ë¸ì´ ìµœê³  ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ ì—…ë°ì´íŠ¸
                if self.eval_network.updated:
                    print(f"Saving best model at epoch {i + 1}...")
                    self.eval_network.save_model(f"model_env_checkpoint_epoch_{i + 1}.pth")  # ì—í¬í¬ë³„ íŒŒì¼ ì €ì¥
                    self.eval_network.save_model()  # ê¸°ë³¸ íŒŒì¼ ì´ë¦„(best_model_env_weights.pth)ìœ¼ë¡œ ì €ì¥
                    print("Model saved.")

                    print("ìµœì‹  ëª¨ë¸ì´ ìµœê³  ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    self.eval_network.updated = False  # ì—…ë°ì´íŠ¸ ìƒíƒœ ì´ˆê¸°í™”

        print("Training completed!")

"""# 09. Main
#### self-play, ëª¨ë¸ í•™ìŠµ, í‰ê°€, ìµœê³  ëª¨ë¸ ì—…ë°ì´íŠ¸ -> ëª¨ë¸ ì„±ëŠ¥ ì ì§„ì ìœ¼ë¡œ ê°œì„ 
"""

import csv

if __name__ == '__main__':
    import torch

    # í•„ìš”í•œ ìƒìˆ˜ ì •ì˜
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    N_KERNEL = 64
    N_RESIDUAL_BLOCK = 16

    BATCH_SIZE = 128
    EPOCHS = 50  # í•™ìŠµ ì—í¬í¬
    LEARNING_RATE = 0.001
    LEARN_EPOCH = 100

    SP_GAME_COUNT = 20
    SP_TEMPERATURE = 1.5 # ë³¼ì¸ ë§Œ ë¶„í¬ì˜ ì˜¨ë„ íŒŒë¼ë¯¸í„°

    EVAL_COUNT = 100
    PV_EVALUATE_COUNT = 800
    EVAL_GAME_COUNT = 20
    EVAL_TEMPERATURE = 1.0

    # ì´ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìƒˆ ëª¨ë¸ë¡œ ì´ˆê¸°í™”
    model_path = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ê´€ë ¨ ì¸ìˆ˜ë¥¼ ì œê±°

    # Environment ê°ì²´ ìƒì„±
    env = Environment()

    # AlphaZeroAgent ì´ˆê¸°í™”
    alphazero_agent = AlphaZeroAgent(
        model_path=None,  # ì´ì „ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        sp_game_count=SP_GAME_COUNT,
        eval_game_count=EVAL_GAME_COUNT,
        learning_rate=LEARNING_RATE,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        learn_epoch=LEARN_EPOCH,
        pv_evaluate_count=PV_EVALUATE_COUNT,  # MCTS íƒìƒ‰ ê¹Šì´ ì¦ê°€
        env=env
    )

    # Self-play ì‹¤í–‰ì„ ëª…ì‹œì ìœ¼ë¡œ ì œì–´
    alphazero_agent.selfplay(execute_self_play=True)  # Self-play ë¹„í™œì„±í™”

    # í•™ìŠµ ì‹¤í–‰
    alphazero_agent.train()

"""## Test Code
### ì•ŒíŒŒì œë¡œ ëª¨ë¸ê³¼ ì‚¬ëŒì´ í‹±íƒí†  ê²Œì„ì„ í”Œë ˆì´í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µ
#### ëª¨ë¸ -> MCTSë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë™ì„ ì„ íƒ
#### ì‚¬ëŒ -> í‚¤ë³´ë“œ ì…ë ¥ìœ¼ë¡œ í–‰ë™ì„ ì„ íƒ
"""





'''
class ModelvsHuman: #ì•ŒíŒŒì œë¡œì™€ ì‚¬ëŒì´ ëŒ€ê²°í•˜ëŠ” ì½”ë“œ
    def __init__(self, model):
        self.model = model #ì•ŒíŒŒì œë¡œ ëª¨ë¸ ì €ì¥
        self.mcts = MCTS(EVAL_COUNT) #MCTS ê°ì²´ ì´ˆê¸°í™”í•˜ì—¬ ëª¨ë¸ ê¸°ë°˜ì˜ í–‰ë™ ìƒì„±
        self.next_actions = self.mcts.get_actions_of(model, EVAL_TEMPERATURE)
        #MCTSê°€ ìƒì„±í•œ í–‰ë™ í™•ë¥  ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë™ì„ ì„ íƒ


    def render(self, state): #ê²Œì„ í˜„ì¬ ìƒíƒœë¥¼ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
        is_first_player = state.check_first_player()
        board = (
            state.state - state.enemy_state
            if is_first_player
            else state.enemy_state - state.state
        )
        #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì™€ ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ë³´ë“œë¥¼ í•©ì‚°í•˜ì—¬ í˜„ì¬ ë³´ë“œ ìƒíƒœë¥¼ í‘œí˜„
        #ê° í”Œë ˆì´ì–´ì˜ ëŒì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ ê°’ì„ 1 (ì²«ë²ˆì§¸ í”Œë ˆì´ì–´)ì™€ -1 (ë‘ë²ˆì§¸ í”Œë ˆì´ì–´)ë¡œ ë³€í™˜
        #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ ì°¨ë¡€ì¼ ë•Œ: state.board[0] + (-1 * state.board[1])
        #board = [[ 1, -1,  0],
        #         [ 0,  1,  0],
        #         [ 0,  0, -1]]
        #ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ ì°¨ë¡€ì¼ ë•Œ: state.board[1] + (-1 * state.board[0])
        #board = [[-1,  1,  0],
        #         [ 0, -1,  0],
        #         [ 0,  0,  1]]
        def pattern(x): #ìˆ«ì ìƒíƒœ (1,0,-1)ì„ 'O','ê³µë°±','X'ë¡œ ë³€í™˜
            if x == 1:
                return 'O' #í”Œë ˆì´ì–´ ë§ 1 (ì²«ë²ˆì§¸ í”Œë ˆì´ì–´) -> 'O'
            elif x == 0:
                return ' ' #í”Œë ˆì´ì–´ ë§ 0 -> 'ê³µë°±'
            else:
                return 'X' #í”Œë ˆì´ì–´ ë§ -1 (ë‘ë²ˆì§¸ í”Œë ˆì´ì–´) -> 'X'

        board_list = list(map(pattern, board.reshape(-1)))
        # Fix: Convert sub-lists to strings before joining
        board_string = '\n'.join([' | '.join(board_list[i:i + 3]) for i in range(0, 9, 3)])
        print(board_string)


    def vs_human(self, with_policy=True): #ì•ŒíŒŒì œë¡œ ëª¨ë¸ê³¼ ì¸ê°„ ê°„ì˜ ê²Œì„ ì§„í–‰
        """
        ì‚¬ëŒê³¼ AlphaZeroê°€ ëŒ€êµ­í•˜ë©°, ì‚¬ëŒì´ í›„ê³µì„ ì„ íƒí•  ê²½ìš° ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ë³€ê²½.
        """
        # ì„ ê³µ / í›„ê³µ ì„ íƒ
        player_choice = input("ì„ ê³µ(O)ìœ¼ë¡œ í”Œë ˆì´í•˜ë ¤ë©´ '1', í›„ê³µ(X)ìœ¼ë¡œ í”Œë ˆì´í•˜ë ¤ë©´ '2'ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        while player_choice not in ["1", "2"]:
            player_choice = input("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. '1'(ì„ ê³µ) ë˜ëŠ” '2'(í›„ê³µ)ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

        human_first = player_choice == "1"  # ì‚¬ëŒì´ ì„ ê³µì¸ì§€ ì—¬ë¶€
        state = State()
        self.render(state)

        while not state.check_done()[0]:
           if state.check_first_player() == human_first:  # ì‚¬ëŒì´ ë‘˜ ì°¨ë¡€
                 legal_actions = np.where(state.get_legal_actions() != 0)[0]
                 my_action = int(input("Choose Your Action (0-8): "))
                 while my_action not in legal_actions:
                     print(f"Invalid action. Choose from {legal_actions}")
                     my_action = int(input("Choose Your Action (0-8): "))
                 state = state.next(my_action)
                 print(f"Human ({'O' if human_first else 'X'}) chose: {my_action}")
           else:  # ì•ŒíŒŒì œë¡œê°€ ë‘˜ ì°¨ë¡€
                 action = self.next_actions(state)
                 state = state.next(action)
                 print(f"AlphaZero ({'X' if human_first else 'O'}) chose: {action}")

           self.render(state)  # í˜„ì¬ ë³´ë“œ ìƒíƒœ ì¶œë ¥

        print("Game Over!")


    def __call__(self, with_policy=True): #ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì•ŒíŒŒì œë¡œ ì •ì±…ì„ ì‹œê°í™”í•˜ê²Œ ì‹œí‚´
        self.vs_human(with_policy)
'''

'''
class ModelvsHuman:
    def __init__(self, model):
        self.model = model
        self.mcts = MCTS(EVAL_COUNT)
        self.next_actions = self.mcts.get_actions_of(model, EVAL_TEMPERATURE)

    def render(self, state):
        board = state.state - state.enemy_state if state.check_first_player() else state.enemy_state - state.state
        board_list = [['O' if cell == 1 else 'X' if cell == -1 else ' ' for cell in row] for row in board]
        formatted_board = "\n".join([" | ".join(row) for row in board_list])
        print(formatted_board)

    def vs_human(self):
        """
        AlphaZeroì™€ ì‚¬ëŒì´ ë²ˆê°ˆì•„ê°€ë©° ì •í™•íˆ êµëŒ€í•˜ë©° ë‘ë„ë¡ ìˆ˜ì •ëœ ì½”ë“œ.
        """
        # ì„ ê³µ/í›„ê³µ ì„ íƒ
        player_choice = input("ì„ ê³µ(O)ìœ¼ë¡œ í”Œë ˆì´í•˜ë ¤ë©´ '1', í›„ê³µ(X)ìœ¼ë¡œ í”Œë ˆì´í•˜ë ¤ë©´ '2'ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        while player_choice not in ["1", "2"]:
            player_choice = input("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. '1'(ì„ ê³µ) ë˜ëŠ” '2'(í›„ê³µ)ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

        human_first = player_choice == "1"
        state = State()
        self.render(state)

        while not state.check_done()[0]:  # ê²Œì„ì´ ì¢…ë£Œë˜ì§€ ì•Šì•˜ì„ ë•Œ
            if state.check_first_player() == human_first:  # ì‚¬ëŒì´ ë‘˜ ì°¨ë¡€
                legal_actions = np.where(state.get_legal_actions() != 0)[0]
                my_action = input(f"Choose Your Action {tuple(legal_actions)}: ").strip()

                # ìœ íš¨í•œ ì…ë ¥ê°’ë§Œ í—ˆìš©
                while not my_action.isdigit() or int(my_action) not in legal_actions:
                    my_action = input(f"ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. {tuple(legal_actions)} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”: ").strip()

                my_action = int(my_action)
                state = state.next(my_action)
                print(f"Human ({'O' if human_first else 'X'}) chose: {my_action}")

            else:  # AlphaZero ì°¨ë¡€
                action = self.next_actions(state)
                state = state.next(action)
                print(f"AlphaZero ({'X' if human_first else 'O'}) chose: {action}")

            self.render(state)  # í˜„ì¬ ë³´ë“œ ìƒíƒœ ì¶œë ¥

        print("Game Over!")  # ê²Œì„ ì¢…ë£Œ ë©”ì‹œì§€ ì¶œë ¥

    def __call__(self):
        self.vs_human()
'''

class ModelvsHuman:
    def __init__(self, model):
        self.model = model
        self.mcts = MCTS(EVAL_COUNT)
        self.get_action = self.mcts.get_actions_of(model, EVAL_TEMPERATURE)

    def render(self, state):
        """í˜„ì¬ ê²Œì„ ìƒíƒœë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        board = state.state - state.enemy_state if state.check_first_player() else state.enemy_state - state.state
        board_list = [['O' if cell == 1 else 'X' if cell == -1 else ' ' for cell in row] for row in board]
        formatted_board = "\n".join([" | ".join(row) for row in board_list])
        print(formatted_board)
        print("-" * 11)  # êµ¬ë¶„ì„  ì¶”ê°€

    def vs_human(self):
        """AlphaZeroì™€ ì‚¬ëŒì´ ì •í™•íˆ ë²ˆê°ˆì•„ê°€ë©° í”Œë ˆì´í•˜ë„ë¡ ìˆ˜ì •"""
        # âœ… ì„ ê³µ/í›„ê³µ ì„ íƒ
        player_choice = input("ì„ ê³µ(O)ìœ¼ë¡œ í”Œë ˆì´í•˜ë ¤ë©´ '1', í›„ê³µ(X)ìœ¼ë¡œ í”Œë ˆì´í•˜ë ¤ë©´ '2'ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        while player_choice not in ["1", "2"]:
            player_choice = input("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. '1'(ì„ ê³µ) ë˜ëŠ” '2'(í›„ê³µ)ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

        human_first = player_choice == "1"
        state = State()
        self.render(state)

        while not state.check_done()[0]:  # ê²Œì„ì´ ì¢…ë£Œë˜ì§€ ì•Šì•˜ì„ ë•Œ
            if state.check_first_player() == human_first:  # ì‚¬ëŒì´ ë‘˜ ì°¨ë¡€
                legal_actions = np.where(state.get_legal_actions() != 0)[0]
                my_action = input(f"Choose Your Action {tuple(legal_actions)}: ").strip()

                # âœ… ìœ íš¨í•œ ì…ë ¥ê°’ë§Œ í—ˆìš©
                while not my_action.isdigit() or int(my_action) not in legal_actions:
                    my_action = input(f"ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. {tuple(legal_actions)} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”: ").strip()

                my_action = int(my_action)
                state = state.next(my_action)
                print(f"Human ({'O' if human_first else 'X'}) chose: {my_action}")

            else:  # âœ… AlphaZero ì°¨ë¡€
                action = self.get_action(state)
                state = state.next(action)
                print(f"AlphaZero ({'X' if human_first else 'O'}) chose: {action}")

            # âœ… í„´ì´ ë°”ë€” ë•Œë§ˆë‹¤ ë³´ë“œ ìƒíƒœë¥¼ í•œ ë²ˆë§Œ ì¶œë ¥
            self.render(state)

        print("Game Over!")  # ê²Œì„ ì¢…ë£Œ ë©”ì‹œì§€ ì¶œë ¥

    def __call__(self):
        self.vs_human()

#ì •ì±… ì‹œê°í™”ì™€ í˜„ì¬ ê²Œì„ ìƒíƒœë¥¼ ê·¸ë˜í”„ í˜•íƒœë¡œ í‘œí˜„
def visualize_history(history): #ì •ì±… ê°’ì˜ í¬ê¸°ë¥¼ ê°•ì¡°í•˜ê³ , ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í–‰ë™ì„ í‘œì‹œ

    state, policy, _ = history #self-play ê³¼ì •ì—ì„œ ê¸°ë¡ëœ í•˜ë‚˜ì˜ ìƒíƒœ ê¸°ë¡
    policy = np.array(policy).reshape(3,3) #ì•Œí˜ì œë¡œê°€ ìƒì„±í•œ í™•ë¥  ë¶„í¬ -> ì´ë¥¼ 3X3ìœ¼ë¡œ ë³€í™˜!

    # Obtain is_first_player from the state object
    is_first_player = state.check_first_player() # stateì—ì„œ is_first_player ì •ë³´ë¥¼ ì–»ì–´ì˜µë‹ˆë‹¤.


    def make_coord(state): #ì¢Œí‘œ ìƒì„± í•¨ìˆ˜ / ëŒì´ ë†“ì¸ ìœ„ì¹˜ë¥¼ ì¢Œí‘œë¡œ ë³€í™˜í•˜ëŠ” ì—­í• 
        mask = state.astype(bool).reshape(-1)  # ë¶ˆë¦¬ì–¸ ë§ˆìŠ¤í¬ ìƒì„±
        indices = list(np.arange(len(mask))[mask])  # Trueì¸ ì¸ë±ìŠ¤ ì¶”ì¶œ
        #stateì—ì„œ ëŒì´ ë†“ì¸ ìœ„ì¹˜ë¥¼ Trueë¡œ ë³€í™˜ í›„, í•´ë‹¹ ì¢Œí‘œë¥¼ ê°€ì ¸ì˜´

        # ë°°ì—´ì˜ ì—´ í¬ê¸° (stateëŠ” 2D ë°°ì—´ë¡œ ê°€ì •)
        width = state.shape[1]

        # divmodë¡œ ëª«ê³¼ ë‚˜ë¨¸ì§€ë¥¼ ê³„ì‚°
        if len(indices) != 0: #ëŒì´ ë†“ì¸ ìœ„ì¹˜ê°€ ìˆì„ ê²½ìš°
            quotients, remainders = zip(*[divmod(index, width) for index in indices])
            #divmod: 1D ì¸ë±ìŠ¤ë¥¼ 2D ì¢Œí‘œ (row, col)ë¡œ ë³€í™˜
            coords = [quotients, remainders]

        else: #ëŒì´ ë†“ì¸ ìœ„ì¹˜ê°€ ì•„ë‹ ê²½ìš°
            coords = None

        return coords


    # Plotting í˜„ì¬ ìƒíƒœ ì‹œê°í™”
    fig, ax = plt.subplots()
    cax = ax.matshow(policy, cmap='Greens', alpha=0.7)
    #policy ë°°ì—´ì„ ìƒ‰ìƒìœ¼ë¡œ í‘œí˜„ / ì •ì±… í™•ë¥  ê°’ì˜ í¬ê¸°ë¥¼ ì´ˆë¡ìƒ‰ ìŒì˜ìœ¼ë¡œ í‘œí˜„ / ìƒ‰ìƒ íˆ¬ëª…ë„


    # í”Œë ˆì´ì–´ì˜ ì°¨ë¡€ ì¸ì‹
    if is_first_player:  #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ ì°¨ë¡€ (ì¦‰, ì•ŒíŒŒì œë¡œ)
        #ê° í”Œë ˆì´ì–´ì˜ ëŒ ì¢Œí‘œ ê³„ì‚°
        o_coords = make_coord(state.state) #state[0]: ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ëŒ ì¢Œí‘œ
        x_coords = make_coord(state.enemy_state) #state[1]: ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ëŒ ì¢Œí‘œ

        #ëŒì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
        if o_coords is not None: #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ (O)ì˜ ëŒì´ ë³´ë“œì— ìˆëŠ” ê²½ìš°
            plt.scatter(o_coords[1], o_coords[0], marker="o", s=2000, c='black')  #í° ì› (ê²€ì •ìƒ‰)
            plt.scatter(o_coords[1], o_coords[0], marker="o", s=1500, c='#F2F2F2') #ì‘ì€ ì› (í°ìƒ‰ ë‚´ë¶€)
            #í”Œë ˆì´ì–´ì˜ ëŒì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
            #ëŒì´ ì—†ì„ ë•Œ ë¶ˆí•„ìš”í•œ ì‹œê°í™” ì‘ì—…ì„ ë°©ì§€

        if x_coords is not None: #ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ (X)ì˜ ëŒì´ ë³´ë“œì— ìˆëŠ” ê²½ìš°
            plt.scatter(x_coords[1], x_coords[0], marker="x", s=2000, c='black', linewidths=3.0)


    else: #ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ ì°¨ë¡€ (ì¦‰, ì¸ê°„)
        o_coords = make_coord(state.enemy_state) #state[1]: ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ëŒ ì¢Œí‘œ
        x_coords = make_coord(state.state) #state[0]: ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ì˜ ëŒ ì¢Œí‘œ

        #ëŒì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
        if o_coords is not None: #ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ (O)ì˜ ëŒì´ ë³´ë“œì— ìˆëŠ” ê²½ìš°
            plt.scatter(o_coords[1], o_coords[0], marker="o", s=2000, c='black')  # x, y ì¢Œí‘œë¡œ ë§¤í•‘
            plt.scatter(o_coords[1], o_coords[0], marker="o", s=1500, c='#F2F2F2')

        if x_coords is not None:  #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ (X)ì˜ ëŒì´ ë³´ë“œì— ìˆëŠ” ê²½ìš°
            plt.scatter(x_coords[1], x_coords[0], marker="x", s=2000, c='black', linewidths=3.0)


    max_coord = divmod(np.argmax(policy), policy.shape[1]) #ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í–‰ë™ ê°•ì¡°
    #np.argmax(policy): ê°€ì¥ ë†’ìœ¼ í™•ë¥ ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ
    #divmod: 1D ì¸ë±ìŠ¤ë¥¼ 2D ì¢Œí‘œ (row, col)ë¡œ ë³€í™˜


    if is_first_player: #ì²«ë²ˆì§¸ í”Œë ˆì´ì–´ ì°¨ë¡€
        plt.scatter(max_coord[1], max_coord[0], marker="o", s=2000, c='red', linewidths=3.0) # #FF19A3
        #ì •ì±…ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í–‰ë™ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ê°•ì¡° -> ì•ŒíŒŒì œë¡œ ì°¨ë¡€ì¼ ë•Œ

    else: #ë‘ë²ˆì§¸ í”Œë ˆì´ì–´ ì°¨ë¡€
        plt.scatter(max_coord[1], max_coord[0], marker="x", s=2000, c='red', linewidths=3.0) # #FF19A3
        #ì •ì±…ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í–‰ë™ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ê°•ì¡° -> ì¸ê°„ ì°¨ë¡€ì¼ ë•Œ


    # Add text annotations (optional) / ì •ì±… ê°’ ì£¼ì„ ì¶”ê°€
    for (i, j), value in np.ndenumerate(policy): #ì •ì±… ë°°ì—´ì˜ ê° ìœ„ì¹˜ì™€ ê°’ì„ ë°˜í™˜
        if value != 0.0:
            ax.text(j, i, f"{value:.2f}", ha='center', va='center', fontsize='15',
                    fontweight='bold' if value == np.max(policy) else "normal",
                    color="white" if value == np.max(policy) else "black")


    # Colorbar and titles / ìƒ‰ìƒ ë°”ì™€ íƒ€ì´í‹€ ì¶”ê°€
    plt.colorbar(cax, ax=ax)
    ax.set_title("Current State, Policy with Highlighted Highest Action")
    ax.axis("off")
    plt.show()

#1 ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
model = Network(N_RESIDUAL_BLOCK, N_KERNEL, STATE_SIZE, N_ACTIONS)
model = nn.DataParallel(model).to(device)  # DataParallelë¡œ ë³‘ë ¬í™”

# self_play ê°ì²´ ë…ë¦½ì ìœ¼ë¡œ ì´ˆê¸°í™”
self_play = SelfPlay(model, SP_TEMPERATURE, SP_GAME_COUNT, EVAL_COUNT)

params = torch.load("/content/drive/MyDrive/3-2/á„€á…¡á†¼á„’á…ª/best_model_env_weights.pth", map_location=torch.device('cpu'), weights_only=False)

# Check if the checkpoint was saved using DataParallel by checking if keys have the 'module.' prefix
#The error occurs when the model is initialized directly with the saved weights without accounting for the 'module.' prefix if the model was saved using DataParallel.
#To fix this, you should modify the loading logic to add the 'module.' prefix to the keys of the saved checkpoint
if any(k.startswith('module.') for k in params.keys()):
    # If saved with DataParallel, load directly
    model.load_state_dict(params)
else:
    # If not saved with DataParallel, use the modified state_dict with prefix
    state_dict = {f"module.{k}": v for k, v in params.items()}  # Add 'module.' prefix
    model.load_state_dict(state_dict)

print("ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ!")

"""ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ«ìë¥¼ ì…ë ¥í•˜ë©´ í˜„ì¬ í‹±íƒí†  ê²Œì„ì˜ ë³´ë“œ ìƒíƒœê°€ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.

- í”Œë ˆì´ì–´ 1 (ì•ŒíŒŒì œë¡œ ì—ì´ì „íŠ¸)ì˜ ë§ì€ Oì´ê³ , í”Œë ˆì´ì–´ 2 (ì¸ê°„)ì˜ ë§ì€ Xë¡œ í‘œí˜„ëœë‹¤!

- í–‰ë™ í™•ë¥ ì— ë”°ë¼ ì•ŒíŒŒì œë¡œê°€ ì„ íƒí•œ ì¹¸ì´ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ê°•ì¡°ë˜ê¸°ë„ í•œë‹¤.
"""

#2. ëª¨ë¸ ëŒ€ ì¸ê°„ ëŒ€ê²°
# vs_human = ModelvsHuman(eval_network.best_model)

vs_human = ModelvsHuman(model) #ê°ì²´ë¥¼ ì´ˆê¸°í™”
vs_human() #__call__ ë§¤ì„œë“œë¥¼ í†µí•´ ê²Œì„ì„ ì‹œì‘



#2. self-play ê¸°ë¡ ì‹œê°í™”
for idx in range(len(self_play.history)):
  visualize_history(self_play.history[idx])
#self_play.history: self-playì—ì„œ ìƒì„±ëœ ìƒíƒœ, ì •ì±…, ê°€ì¹˜ë¥¼ ê¸°ë¡í•œ ë¦¬ìŠ¤íŠ¸
#self_play.historyì˜ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì´í„°ë ˆì´í„° ìƒì„±
#ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë¡ ë°˜ë³µ:



"""---------------

## ì†ì‹¤ (loss) ë³€í™” ë¶„ì„
"""

import matplotlib.pyplot as plt

def plot_losses(loss_file="my_train_losses.csv"):
    if not os.path.exists(loss_file):
        print(f"{loss_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    df = pd.read_csv(loss_file)

    plt.figure(figsize=(10, 5))
    plt.plot(df["epoch"], df["p_loss"], label="Policy Loss", color="blue")
    plt.plot(df["epoch"], df["v_loss"], label="Value Loss", color="red")
    plt.plot(df["epoch"], df["total_loss"], label="Total Loss", color="green")

    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training Loss Over Epochs")
    plt.legend()
    plt.grid()
    plt.show()

# ì†ì‹¤ ê·¸ë˜í”„ ì¶œë ¥
plot_losses()